[
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "glue-sst2",
        "instruction": "Create a sentence with the given sentiment."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-mrpc",
        "instruction": "Determine whether two sentences are equivalent or not. Answers must be one of not_equivalent, equivalent."
    },
    {
        "input_fields": [
            "sentence2"
        ],
        "output_field": [
            "sentence1"
        ],
        "task_name": "glue-mrpc",
        "instruction": "Rewrite the first sentence to make it equivalent to the second sentence."
    },
    {
        "input_fields": [
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "glue-mrpc",
        "instruction": "Rewrite the second sentence to make it equivalent to the first sentence."
    },
    {
        "input_fields": [
            "question1",
            "question2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-qqp",
        "instruction": "Determine whether two questions are duplicates or not. Answers must be one of duplicate, not_duplicate."
    },
    {
        "input_fields": [
            "question1"
        ],
        "output_field": [
            "question2"
        ],
        "task_name": "glue-qqp",
        "instruction": "Given a question, generate a similar question."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-stsb",
        "instruction": "Given two sentences, please predict the similarity score between them."
    },
    {
        "input_fields": [
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "glue-stsb",
        "instruction": "Given a sentence, please generate a similar sentence with a different wording."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-mnli",
        "instruction": "Given a premise and a hypothesis, determine whether the hypothesis is entailment, contradiction, or neutral to the premise. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "label",
            "hypothesis"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "glue-mnli",
        "instruction": "Create a premise that entails, contradicts, or is neutral to the hypothesis."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "glue-mnli",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailment, contradiction, or neutral to the premise."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-mnli_mismatched",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of entailment, neutral, contradiction."
    },
    {
        "input_fields": [
            "label",
            "hypothesis"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "glue-mnli_mismatched",
        "instruction": "This task asks models to write a premise that entails, contradicts to, or is neutral to the hypothesis."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-mnli_matched",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "glue-mnli_matched",
        "instruction": "This task asks models to generate a hypothesis given a premise and a label."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "glue-mnli_matched",
        "instruction": "This task asks models to generate a premise given a hypothesis and a label."
    },
    {
        "input_fields": [
            "question",
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-qnli",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "glue-qnli",
        "instruction": "Create a sentence that is entailed by the question."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "glue-qnli",
        "instruction": "Create a question that entails the sentence."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-rte",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "label",
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "glue-rte",
        "instruction": "This task asks models to write a sentence that entails, contradicts to, or is neutral to the given sentence."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-wnli",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "label",
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "glue-wnli",
        "instruction": "This task asks models to write a sentence that entails, contradicts to, or is neutral to the given sentence."
    },
    {
        "input_fields": [
            "sentence_good"
        ],
        "output_field": [
            "sentence_bad"
        ],
        "task_name": "blimp-anaphor_gender_agreement",
        "instruction": "Given a sentence with a correct anaphor agreement, please generate a sentence with an incorrect anaphor agreement."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-anaphor_gender_agreement",
        "instruction": "Given a sentence with an incorrect anaphor agreement, please generate a sentence with a correct anaphor agreement."
    },
    {
        "input_fields": [
            "sentence_good"
        ],
        "output_field": [
            "sentence_bad"
        ],
        "task_name": "blimp-anaphor_number_agreement",
        "instruction": "Given a sentence with a correct anaphor agreement, generate a sentence with an incorrect anaphor agreement."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-anaphor_number_agreement",
        "instruction": "Given a sentence with an incorrect anaphor agreement, generate a sentence with a correct anaphor agreement."
    },
    {
        "input_fields": [
            "sentence_good"
        ],
        "output_field": [
            "sentence_bad"
        ],
        "task_name": "blimp-causative",
        "instruction": "Given a sentence with a correct causative verb, can you replace it with an incorrect one?"
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-causative",
        "instruction": "Given a sentence with an incorrect causative verb, can you replace it with a correct one?"
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-determiner_noun_agreement_with_adj_irregular_1",
        "instruction": "Given a sentence with a grammatical error, please correct the error."
    },
    {
        "input_fields": [
            "sentence_good"
        ],
        "output_field": [
            "sentence_bad"
        ],
        "task_name": "blimp-determiner_noun_agreement_with_adj_irregular_2",
        "instruction": "Given a sentence with a good determiner-noun agreement, please generate a sentence with a bad determiner-noun agreement."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-determiner_noun_agreement_with_adj_irregular_2",
        "instruction": "Given a sentence with a bad determiner-noun agreement, please generate a sentence with a good determiner-noun agreement."
    },
    {
        "input_fields": [
            "sentence_good"
        ],
        "output_field": [
            "sentence_bad"
        ],
        "task_name": "blimp-determiner_noun_agreement_with_adjective_1",
        "instruction": "Given a sentence with a good determiner-noun agreement, please generate a sentence with a bad determiner-noun agreement."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-determiner_noun_agreement_with_adjective_1",
        "instruction": "Given a sentence with a bad determiner-noun agreement, please generate a sentence with a good determiner-noun agreement."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-distractor_agreement_relative_clause",
        "instruction": "Given a sentence with a grammatical error, provide a corrected version of the sentence."
    },
    {
        "input_fields": [
            "sentence_bad",
            "linguistics_term"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-drop_argument",
        "instruction": "Given a sentence with a missing argument, please fill in the blank with the correct argument."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-existential_there_object_raising",
        "instruction": "Given a sentence with bad existential there object raising, rewrite it to be a good example."
    },
    {
        "input_fields": [
            "sentence_good"
        ],
        "output_field": [
            "sentence_bad"
        ],
        "task_name": "blimp-existential_there_object_raising",
        "instruction": "Given a sentence with good existential there object raising, rewrite it to be a bad example."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-irregular_past_participle_adjectives",
        "instruction": "Given a sentence with an irregular past participle adjective, please provide the correct form of the adjective."
    },
    {
        "input_fields": [
            "sentence_bad",
            "field",
            "linguistics_term"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-left_branch_island_echo_question",
        "instruction": "Given a sentence with an island effect, please rewrite the sentence to remove the island effect."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-left_branch_island_simple_question",
        "instruction": "Given a sentence with an island effect, please rewrite the sentence to remove the island effect."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-npi_present_1",
        "instruction": "Given a sentence with an NPI (Negative Polarity Item), provide a sentence that licenses the NPI."
    },
    {
        "input_fields": [
            "sentence_bad",
            "linguistics_term"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-passive_2",
        "instruction": "Given a sentence with a specific linguistic term, generate a new sentence that is grammatically correct and has the same meaning."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-principle_A_case_1",
        "instruction": "Given a sentence with a syntax/semantics error, correct the sentence."
    },
    {
        "input_fields": [
            "sentence_bad",
            "linguistics_term"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-principle_A_domain_1",
        "instruction": "Given a sentence with a bad version, generate a good version based on the principle of binding."
    },
    {
        "input_fields": [
            "sentence_good",
            "linguistics_term"
        ],
        "output_field": [
            "sentence_bad"
        ],
        "task_name": "blimp-principle_A_domain_1",
        "instruction": "Given a sentence with a good version, generate a bad version based on the principle of binding."
    },
    {
        "input_fields": [
            "sentence_good"
        ],
        "output_field": [
            "sentence_bad"
        ],
        "task_name": "blimp-regular_plural_subject_verb_agreement_2",
        "instruction": "Given a sentence with a regular plural subject, please provide the correct verb form."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-regular_plural_subject_verb_agreement_2",
        "instruction": "Given a sentence with a regular plural subject and an incorrect verb form, please provide the correct verb form."
    },
    {
        "input_fields": [
            "sentence_good"
        ],
        "output_field": [
            "sentence_bad"
        ],
        "task_name": "blimp-sentential_subject_island",
        "instruction": "Given a sentence with a subject, please rewrite the sentence by moving the subject to a position where it is blocked by an island."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-sentential_subject_island",
        "instruction": "Given a sentence with a subject blocked by an island, please rewrite the sentence by moving the subject to a position where it is not blocked by an island."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-superlative_quantifiers_2",
        "instruction": "Given a sentence with a superlative quantifier, correct the sentence if it is bad."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_dbert_generate_question",
        "instruction": "Please generate a question based on the given passage."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_dbert_generate_question",
        "instruction": "Please generate a paraphrase of the given passage."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_dbert_tell_what_it_is",
        "instruction": "Given the text, please answer the question \"what concept is mentioned last?\""
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_dbert_tell_what_it_is",
        "instruction": "Given the text, please answer the question \"what concept is mentioned first?\""
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_dbidaf_answer_the_following_q",
        "instruction": "Given the following passage, please answer the question: In what year was Slack's comparison of three groups that conducted biological research at Yale published?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_dbidaf_answer_the_following_q",
        "instruction": "Given the following passage, please answer the question: Who was the leader of the group that was not a classic research school?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_dbidaf_generate_question",
        "instruction": "Please answer the question based on the given passage."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_dbidaf_tell_what_it_is",
        "instruction": "Identify the name of the group that was not led by someone with departmental or institutional position or power."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_droberta_generate_question",
        "instruction": "Please generate a question based on the given passage."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_droberta_generate_question",
        "instruction": "Please generate a passage based on the given question."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_droberta_tell_what_it_is",
        "instruction": "Given the input text, please answer the question \"What jewelry like accessories did he wear?\""
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_droberta_tell_what_it_is",
        "instruction": "Given the input text, please answer the question \"What accessory could have fallen from the night sky?\""
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-ag_news_classify_question_first",
        "instruction": "Create a news article and provide answer choices."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-ag_news_classify_with_choices_question_first",
        "instruction": "Create a news article and its category."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Challenge_heres_a_problem",
        "instruction": "Given a problem and answer choices, select the correct answer. Answers must be one of A, D, B, C, 2."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Challenge_i_am_hesitating",
        "instruction": "Given the question and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Challenge_multiple_choice",
        "instruction": "Create a multiple choice question with answer choices based on a prompt."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Challenge_pick_false_options",
        "instruction": "Pick and copy all the correct options for the following question."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Challenge_pick_false_options",
        "instruction": "Create a question based on the options provided."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Challenge_pick_the_most_correct_option",
        "instruction": "Given the question and answer choices, select the most correct option. Answers must be one of A, D, B, C, 2."
    },
    {
        "input_fields": [
            "answer_choices",
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Challenge_pick_the_most_correct_option",
        "instruction": "Create a question based on the answer choices and the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Challenge_qa_options",
        "instruction": "Given a question and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_i_am_hesitating",
        "instruction": "Given the question and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_i_am_hesitating",
        "instruction": "Create a question based on the answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_multiple_choice",
        "instruction": "Given a multiple choice question and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_multiple_choice",
        "instruction": "Create a multiple choice question with answer choices based on a prompt."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_pick_the_most_correct_option",
        "instruction": "Given a question and answer choices, select the most correct option. Answers must be one of A, D, B, 3, C, 1."
    },
    {
        "input_fields": [
            "answer_choices",
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_pick_the_most_correct_option",
        "instruction": "Create a question based on the answer choices and the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_qa_options",
        "instruction": "Given a question and answer choices, select the most likely answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_qa_options",
        "instruction": "Given a question and answer choices, select the least likely answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_qa_options",
        "instruction": "Given a question and answer choices, rank the answer choices from most to least likely."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-amazon_polarity_Is_this_product_review_positive",
        "instruction": "Write a product review that is positive."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-amazon_polarity_Is_this_review",
        "instruction": "Determine the sentiment of the review based on the given inputs. Answers must be one of Positive, Negative."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-amazon_polarity_Is_this_review_negative",
        "instruction": "Determine whether the product review is negative or not based on the given inputs. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-amazon_polarity_convey_negative_or_positive_sentiment",
        "instruction": "Determine the sentiment of the product review. Answers must be one of Positive, Negative."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-amazon_polarity_convey_negative_or_positive_sentiment",
        "instruction": "Can you write a review for the product?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-amazon_polarity_flattering_or_not",
        "instruction": "Given a product review, determine whether it is flattering or unflattering. Answers must be one of flattering, unflattering."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-amazon_polarity_flattering_or_not",
        "instruction": "Create a product review that is either flattering or unflattering."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-amazon_polarity_negative_or_positive_tone",
        "instruction": "Write a product review with a positive sentiment."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-amazon_polarity_negative_or_positive_tone",
        "instruction": "Write a product review with a negative sentiment."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-amazon_polarity_user_satisfied",
        "instruction": "Given a review left by a customer on a product, determine whether the customer is satisfied or dissatisfied. Answers must be one of dissatisfied, satisfied."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-amazon_polarity_user_satisfied",
        "instruction": "Create a review for a product that would make the customer satisfied."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-amazon_polarity_would_you_buy",
        "instruction": "Given a product review, predict whether the review would increase or decrease the chances of buying the product. Answers must be one of decrease, increase."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_GPT_3_style_r1_score_eval",
        "instruction": "Given the input and target templates, please select the correct label for the question. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_GPT_3_style_r1_score_eval",
        "instruction": "Given the input and target templates, please predict the correct label for the question. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_GPT_3_style_r2",
        "instruction": "Answer the question based on the given input. The answer is either True, False, or Neither. Answers must be one of True, Neither, False."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_GPT_3_style_r2",
        "instruction": "Create a question based on the given input."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_GPT_3_style_r2_score_eval",
        "instruction": "Given the input and target templates, please predict the correct label (True, False, or Neither) for the given input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_GPT_3_style_r2_score_eval",
        "instruction": "Given the input and target templates, please generate the target label (True, False, or Neither) for the given input. Answers must be one of True, Neither, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_GPT_3_style_r3_score_eval",
        "instruction": "Given an article and a question, predict whether the answer to the question is True, False, or Neither based on the article. Answers must be one of True, Neither, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_GPT_3_style_r3_score_eval",
        "instruction": "Given an article and a question, predict whether the answer to the question is True or False based on the article. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-anli_MNLI_crowdsource_r1",
        "instruction": "Create a statement and a question based on the given information."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_MNLI_crowdsource_r1_score_eval",
        "instruction": "Given a statement and a question, determine whether the statement is definitely correct, incorrect, or inconclusive based on the information provided. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_MNLI_crowdsource_r1_score_eval",
        "instruction": "Given a statement and a question, provide a label for the statement based on the information provided. Answers must be one of Correct, Incorrect, Inconclusive."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_MNLI_crowdsource_r2",
        "instruction": "Given the inputs and answer choices, determine the correct answer. Answers must be one of Correct, Incorrect, Inconclusive."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_MNLI_crowdsource_r2_score_eval",
        "instruction": "Given the description of a movie, determine whether a statement about the lead role is correct, incorrect, or inconclusive. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_MNLI_crowdsource_r2_score_eval",
        "instruction": "Given the description of a movie and a statement about the lead role, determine whether the statement is correct, incorrect, or inconclusive. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_MNLI_crowdsource_r3_score_eval",
        "instruction": "Given the inputs_pretokenized and targets_pretokenized, determine whether is_correct is True, False, or Inconclusive. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_MNLI_crowdsource_r3_score_eval",
        "instruction": "Given the inputs_pretokenized and is_correct, write a targets_pretokenized that is either Correct, Incorrect, or Inconclusive. Answers must be one of Correct, Incorrect, Inconclusive."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_always_sometimes_never_r1_score_eval",
        "instruction": "Given an input statement and a hypothesis, determine if the hypothesis is always, sometimes, or never true based on the input statement. Answers must be one of Never, Always, Sometimes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_always_sometimes_never_r1_score_eval",
        "instruction": "Given an input statement and a hypothesis, determine if the hypothesis is true or false based on the input statement. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_always_sometimes_never_r2",
        "instruction": "Given the input and target templates, please select the correct answer choice. Answers must be one of Always, Never, Sometimes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_always_sometimes_never_r2_score_eval",
        "instruction": "Given a statement and its truth value, determine whether the target is always, sometimes, or never true. Answers must be one of Never, Always, Sometimes."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_always_sometimes_never_r3",
        "instruction": "Create a statement that can be used to determine whether the given article is always, sometimes, or never true."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_always_sometimes_never_r3_score_eval",
        "instruction": "Given an article and a statement, determine if the statement is always, sometimes, or never true based on the article. Answers must be one of Never, Always, Sometimes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_always_sometimes_never_r3_score_eval",
        "instruction": "Given an article and a statement, determine if the statement is true or false based on the article. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_based_on_the_previous_passage_r1_score_eval",
        "instruction": "Based on the previous passage, answer whether the statement is true, false, or uncertain. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_based_on_the_previous_passage_r1_score_eval",
        "instruction": "Based on the previous passage, predict the correctness of the statement. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_based_on_the_previous_passage_r2",
        "instruction": "Based on the previous passage, answer the question with \"Yes\", \"No\", or \"Maybe\". Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_based_on_the_previous_passage_r2",
        "instruction": "Create a sentence that can be used as the input for the previous task."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_based_on_the_previous_passage_r2_score_eval",
        "instruction": "Based on the previous passage, please select the correct answer among \"Yes\", \"No\", and \"Maybe\" to the given question. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_based_on_the_previous_passage_r2_score_eval",
        "instruction": "Based on the previous passage, please answer the given question with \"True\" or \"False\". Answers must be one of True, False."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_based_on_the_previous_passage_r2_score_eval",
        "instruction": "Can you write a premise that entails, contradicts to, or is neutral to the hypothesis?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_based_on_the_previous_passage_r3_score_eval",
        "instruction": "Based on the previous passage, determine whether the statement is true, false, or maybe. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_based_on_the_previous_passage_r3_score_eval",
        "instruction": "Based on the previous passage, provide the corresponding answer (\"yes\", \"no\", or \"maybe\") to the question. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_can_we_infer_r1_score_eval",
        "instruction": "Given an input and a hypothesis, can we infer a certain statement? The answer is either yes, no, or maybe. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_can_we_infer_r1_score_eval",
        "instruction": "Given an input and a hypothesis, is the answer correct or not? Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_can_we_infer_r2",
        "instruction": "Given the inputs and answer choices, select the correct answer. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_can_we_infer_r2_score_eval",
        "instruction": "Given a sentence and a question, can we infer the answer from the sentence? The answer can be \"yes\", \"no\", or \"maybe\". Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_can_we_infer_r2_score_eval",
        "instruction": "Given a sentence and a question, can we infer the answer from the sentence? The answer can be \"True\" or \"False\". Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_can_we_infer_r3_score_eval",
        "instruction": "Given the input and target templates, determine whether the target is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_can_we_infer_r3_score_eval",
        "instruction": "Given the input and target templates, predict the target. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_claim_true_false_inconclusive_r1_score_eval",
        "instruction": "Determine whether the claim is true, false, or inconclusive based on the given information. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_claim_true_false_inconclusive_r1_score_eval",
        "instruction": "Provide the target label for the given claim. Answers must be one of True, Inconclusive, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_claim_true_false_inconclusive_r2",
        "instruction": "Determine whether the claim is true, false, or inconclusive based on the given information. Answers must be one of True, Inconclusive, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_claim_true_false_inconclusive_r2_score_eval",
        "instruction": "Given an input template and a target template, determine whether the claim is true, false, or inconclusive. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_claim_true_false_inconclusive_r2_score_eval",
        "instruction": "Given an input template and a target template, predict the target template. Answers must be one of True, Inconclusive, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_consider_always_sometimes_never_r1_score_eval",
        "instruction": "Given a statement and a question, determine if the statement is always, sometimes, or never correct. Answers must be one of Never, Always, Sometimes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_consider_always_sometimes_never_r1_score_eval",
        "instruction": "Given a statement and a question, determine if the statement is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_consider_always_sometimes_never_r2",
        "instruction": "Given the inputs, choose the correct answer choice. Answers must be one of Always, Never, Sometimes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_consider_always_sometimes_never_r2_score_eval",
        "instruction": "Given the text and a statement, determine if the statement is always, sometimes, or never correct. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_consider_always_sometimes_never_r3_score_eval",
        "instruction": "Given the article and the target, determine whether the statement \"The article was written on December 18th\" is always, sometimes, or never correct. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_consider_always_sometimes_never_r3_score_eval",
        "instruction": "Given the article and the statement \"The article was written on December 18th\", write a target that indicates whether the statement is always, sometimes, or never correct. Answers must be one of Never, Always, Sometimes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_does_it_follow_that_r1_score_eval",
        "instruction": "Given an input template and a target template, this task asks models to predict whether the target follows from the input. The label choices are yes, maybe, and no. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "answer_choices",
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_does_it_follow_that_r2",
        "instruction": "Create a statement based on the given answer choices and prompt. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_does_it_follow_that_r2_score_eval",
        "instruction": "Given a statement and a question, determine if the statement follows the question. The answer should be either Yes, No, or Maybe. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_does_it_follow_that_r3_score_eval",
        "instruction": "Given the inputs_pretokenized and targets_pretokenized, determine whether is_correct is True, False, or Maybe. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_does_it_follow_that_r3_score_eval",
        "instruction": "Given the inputs_pretokenized and is_correct, write a targets_pretokenized that is either Yes, No, or Maybe. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_does_this_imply_r1",
        "instruction": "Can you generate a statement that implies the given question?"
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_does_this_imply_r1_score_eval",
        "instruction": "Given the input and target templates, determine whether the target is \"Yes\", \"No\", or \"Maybe\". Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_does_this_imply_r2",
        "instruction": "Given a sentence and a question, please select the correct answer from the given choices. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_does_this_imply_r2",
        "instruction": "Can you write a sentence that implies a given statement?"
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_does_this_imply_r2_score_eval",
        "instruction": "Given a statement and a question, determine if the statement implies the answer to the question. The answer can be \"Yes\", \"No\", or \"Maybe\". Answers must be one of True, False."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_does_this_imply_r3",
        "instruction": "Given an article and a target answer, write a question that would have that answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_does_this_imply_r3_score_eval",
        "instruction": "Given the inputs_pretokenized and targets_pretokenized, determine whether is_correct is True, False, or Maybe. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_does_this_imply_r3_score_eval",
        "instruction": "Given the inputs_pretokenized and is_correct, write a targets_pretokenized that is either Yes, No, or Maybe. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_possible_impossible_r1_score_eval",
        "instruction": "Given an input statement, determine whether the statement is guaranteed, possible, or impossible based on the information provided. Answers must be one of Impossible, Guaranteed, Possible."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_possible_impossible_r1_score_eval",
        "instruction": "Given an input statement and its corresponding truth value, determine whether the statement is correct or incorrect. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_possible_impossible_r2",
        "instruction": "Given a statement and a context, determine whether the statement is guaranteed, possible, or impossible based on the context. Answers must be one of Impossible, Guaranteed, Possible."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_possible_impossible_r2",
        "instruction": "Create a context and a statement, and ask whether the statement is guaranteed, possible, or impossible based on the context."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_possible_impossible_r2_score_eval",
        "instruction": "Given an input and a target, determine whether the target is guaranteed, possible, or impossible based on the input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_possible_impossible_r3_score_eval",
        "instruction": "Given an article and a statement, determine whether the statement is guaranteed, possible, or impossible based on the article. Answers must be one of Impossible, Guaranteed, Possible."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_possible_impossible_r3_score_eval",
        "instruction": "Given an article and a statement, determine whether the statement is true or false based on the article. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_true_r1_score_eval",
        "instruction": "Given an input template and a target template, determine whether the target is guaranteed true or not based on the input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_true_r1_score_eval",
        "instruction": "Given an input template and a target template, generate the target options. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_true_r2",
        "instruction": "Given the inputs, choose the correct answer among the answer choices. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_true_r2_score_eval",
        "instruction": "Given an input template and a target template, determine whether the target template is guaranteed true or not based on the input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_true_r2_score_eval",
        "instruction": "Given an input template and a target template, predict the target template. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_true_r3_score_eval",
        "instruction": "Given the input and target templates, determine whether the target is guaranteed true, false, or maybe based on the input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_true_r3_score_eval",
        "instruction": "Given the input and target templates, predict the target. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_justified_in_saying_r1_score_eval",
        "instruction": "Given an input and a hypothesis, determine if the statement is justified or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_justified_in_saying_r1_score_eval",
        "instruction": "Given an input and a hypothesis, predict the answer to the question \"Are we justified in saying that the trolleybus system has over 2 urban routes?\" Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_justified_in_saying_r2",
        "instruction": "Given the inputs, choose the correct answer among the answer choices. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_justified_in_saying_r2_score_eval",
        "instruction": "Given an input and a target, determine whether the input is justified in saying the target. The answer should be \"Yes\", \"No\", or \"Maybe\". Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_justified_in_saying_r3_score_eval",
        "instruction": "Given the inputs_pretokenized and targets_pretokenized, determine whether is_correct is True, False, or Maybe. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_justified_in_saying_r3_score_eval",
        "instruction": "Given the inputs_pretokenized and is_correct, write a targets_pretokenized that is either Yes, No, or Maybe. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_must_be_true_r1_score_eval",
        "instruction": "Given the input and target templates, determine the correct label choice. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_must_be_true_r1_score_eval",
        "instruction": "Given the input and target templates, generate the correct target choice. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_must_be_true_r2",
        "instruction": "Given the input and target templates, please select the correct answer choice. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_must_be_true_r2_score_eval",
        "instruction": "Given the input and target templates, determine whether the target is \"Yes\", \"No\", or \"Maybe\". Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_must_be_true_r2_score_eval",
        "instruction": "Given the input and target templates, determine whether the target is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_must_be_true_r3_score_eval",
        "instruction": "Given the input and target templates, determine whether the target is \"Yes\", \"No\", or \"Maybe\". Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_must_be_true_r3_score_eval",
        "instruction": "Given the input and target templates, determine whether the target is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_should_assume_r1_score_eval",
        "instruction": "Given the input and target templates, please select the correct label choice. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_should_assume_r1_score_eval",
        "instruction": "Given the input and target templates, please determine whether the is_correct field is True or False. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_should_assume_r2",
        "instruction": "Given the inputs, choose the correct answer among the answer choices. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_should_assume_r2_score_eval",
        "instruction": "Given an input template and a target template, please select the correct target from the given options. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_should_assume_r2_score_eval",
        "instruction": "Given an input template and a target template, please predict the target from the given options. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_should_assume_r3_score_eval",
        "instruction": "Given the inputs_pretokenized and targets_pretokenized, determine whether is_correct is True, False, or Maybe. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_should_assume_r3_score_eval",
        "instruction": "Given the inputs_pretokenized and is_correct, write a targets_pretokenized that is either Yes, No, or Maybe. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_take_the_following_as_truth_r1_score_eval",
        "instruction": "Determine whether the statement is true, false, or inconclusive based on the given information. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_take_the_following_as_truth_r1_score_eval",
        "instruction": "Provide the target value (true, false, or inconclusive) based on the given information. Answers must be one of True, Inconclusive, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_take_the_following_as_truth_r2",
        "instruction": "Determine whether the statement is true, false, or inconclusive based on the given information. Answers must be one of True, Inconclusive, False."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_take_the_following_as_truth_r2",
        "instruction": "Create a statement that is true, false, or inconclusive based on the given information."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_take_the_following_as_truth_r2_score_eval",
        "instruction": "Given a statement and a movie description, determine whether the statement is true, false, or inconclusive. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_take_the_following_as_truth_r2_score_eval",
        "instruction": "Given a statement and a movie description, generate a target label for the statement. Answers must be one of True, Inconclusive, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_take_the_following_as_truth_r3_score_eval",
        "instruction": "Determine whether the statement is true, false, or inconclusive based on the given article. Answers must be one of True, Inconclusive, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_generate_story",
        "instruction": "Given a story plot, generate a news article that summarizes the plot."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_generate_story",
        "instruction": "Given a news article, generate a story plot that summarizes the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_news_stock",
        "instruction": "Given the article, predict the potential impact on the stock market."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_news_stock",
        "instruction": "Given the article, predict the number of gold medals won by Usain Bolt in the world championships."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_news_summary",
        "instruction": "Given the inputs, generate a summary of the article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_news_summary",
        "instruction": "Given the targets, generate the inputs of the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_sum_in_brief",
        "instruction": "Given the inputs, generate a summary of the article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_sum_in_brief",
        "instruction": "Given the targets, generate the inputs of the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_tldr_summary",
        "instruction": "Given an article, generate a TLDR (Too Long Didn't Read) summary."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_tldr_summary",
        "instruction": "Given an article, predict the label of the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_tldr_summary",
        "instruction": "Given an article, generate a question based on the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_write_an_outline",
        "instruction": "Given the article, can you write a few points about it?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_write_an_outline",
        "instruction": "Can you summarize the article in one sentence?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_write_an_outline",
        "instruction": "Can you write a question based on the article?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-common_gen_Example_prompt",
        "instruction": "Given a few abstract concepts, generate a coherent sentence."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-common_gen_Example_prompt",
        "instruction": "Given a sentence, generate a few abstract concepts."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-common_gen_Given_concepts_type_2",
        "instruction": "Ignoring the order of the concepts, generate a sentence with all the concepts provided."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-common_gen_Given_concepts_type_2",
        "instruction": "Ignoring the order of the concepts, generate a sentence with all the concepts provided using a different sentence structure."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-common_gen_Given_concepts_type_2",
        "instruction": "Ignoring the order of the concepts, generate a sentence with all the concepts provided using a different verb tense."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-common_gen_Put_together",
        "instruction": "Generate a sentence based on the given concepts."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-common_gen_Put_together",
        "instruction": "Rearrange the given sentence to form a new sentence with the same meaning."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-common_gen_sentence_to_concepts",
        "instruction": "Given a sentence, generate a list of key concepts."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-common_gen_sentence_to_concepts",
        "instruction": "Given a list of key concepts, generate a sentence."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-common_gen_topics_from_the_sentence",
        "instruction": "Generate a sentence based on the given topics."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-common_gen_topics_from_the_sentence",
        "instruction": "Identify the topic(s) in the given sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_description_question_option_id",
        "instruction": "Given a question and options, select the option that is in line with common sense. Answers must be one of A, C, D, B, E."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_description_question_option_text",
        "instruction": "Choose the correct option based on common sense to answer the question."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_description_question_option_text",
        "instruction": "Create a question based on the given information."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_generate_explanation_given_text",
        "instruction": "Given a question and options, select the correct option."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_i_think",
        "instruction": "Given a question and possible answers, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_question_description_option_id",
        "instruction": "Answer the question based on the given input and options. Answers must be one of A, C, D, B, E."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_question_description_option_text",
        "instruction": "Given the input and answer choices, select the most suitable option to answer the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_question_option_description_id",
        "instruction": "Given a question and answer choices, select the correct answer. Answers must be one of A, C, D, B, E."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_question_option_description_text",
        "instruction": "Given the input and answer choices, select the best answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_rationale",
        "instruction": "Given the question and choices, select the correct answer and provide the rationale for the choice."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_rationale",
        "instruction": "Given the rationale and choices, select the correct answer to the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_context_description_question_answer_id",
        "instruction": "Choose the best option to answer the following question based on the given context. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_context_description_question_answer_text",
        "instruction": "Choose the best option to answer the following question based on the given context."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_context_description_question_text",
        "instruction": "Given the context and answer choices, select the correct answer to the question."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-cosmos_qa_context_description_question_text",
        "instruction": "Create a context and answer choices provided the target."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_context_question_description_answer_id",
        "instruction": "Given the inputs and answer choices, select the best answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_context_question_description_answer_text",
        "instruction": "Given the inputs and answer choices, select the best answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_description_context_question_answer_id",
        "instruction": "Choose the best option to answer the question based on the context. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_description_context_question_answer_text",
        "instruction": "Choose the best option to answer the question based on the context."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_description_context_question_answer_text",
        "instruction": "Create a context and question provided the answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_description_context_question_text",
        "instruction": "Given the context and answer choices, please select the correct answer to the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_no_prompt_id",
        "instruction": "Given the inputs and answer choices, select the correct answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_no_prompt_text",
        "instruction": "Given the inputs and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-duorc_SelfRC_title_generation",
        "instruction": "Given a movie plot, suggest a title for the movie."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-duorc_SelfRC_title_generation",
        "instruction": "Given a movie title, generate a plot for the movie."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_TLDR",
        "instruction": "Given the input text, generate a TL;DR summary."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_TLDR",
        "instruction": "Given the TL;DR summary, generate the input text."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_first_sentence_title",
        "instruction": "Given the first sentence of an article, generate a title for the article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_first_sentence_title",
        "instruction": "Given a title of an article, generate the first sentence of the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_generate_summary_for_this",
        "instruction": "Generate a summary for the given article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_in_a_nutshell",
        "instruction": "Given an input sentence, generate a summary of the sentence in a nutshell."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_in_a_nutshell",
        "instruction": "Given a summary of a sentence in a nutshell, generate the original sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_make_a_title",
        "instruction": "Create a summary for the given article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_make_a_title",
        "instruction": "Given a title, create an article that matches the title."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_make_a_title",
        "instruction": "Given an article, create a new title that summarizes the article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_reverse_writing",
        "instruction": "Given the target sequence, generate the input sequence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_reverse_writing",
        "instruction": "Given the input sequence, generate the target sequence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_write_a_title_for_this_sentence",
        "instruction": "Write a title for the given sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_write_an_article",
        "instruction": "Write an article with the given title."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_write_an_article",
        "instruction": "Given an article, write a title for it."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_write_its_sentence",
        "instruction": "Given the sentence, write a summary or title for it."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_equivalent",
        "instruction": "Determine whether the two sentences are equivalent or not. Answers must be one of not equivalent, equivalent."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_equivalent",
        "instruction": "Provide a sentence that is equivalent to the given sentence. Answers must be one of not equivalent, equivalent."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_equivalent",
        "instruction": "Provide a sentence that is not equivalent to the given sentence. Answers must be one of not equivalent, equivalent."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_generate_paraphrase",
        "instruction": "Create a sentence that is a paraphrase of the given sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_generate_sentence",
        "instruction": "Generate a sentence that means the same thing as the input sentence."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_generate_sentence",
        "instruction": "Create an input sentence given the target sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_paraphrase",
        "instruction": "Paraphrase the given sentence. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_replace",
        "instruction": "Create a replacement sentence that means the same thing as the original sentence. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_same_thing",
        "instruction": "Determine whether the two sentences mean the same thing. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_same_thing",
        "instruction": "Determine whether the two sentences mean different things. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_want_to_know",
        "instruction": "Determine whether the two sentences mean the same thing. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_qqp_answer",
        "instruction": "Given an input and a target, determine whether the target can be answered by the input. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_qqp_duplicate",
        "instruction": "Determine whether two questions are duplicates or not. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_qqp_duplicate_or_not",
        "instruction": "Determine whether the two questions are duplicates or not. Answers must be one of not duplicates, duplicates."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_qqp_duplicate_or_not",
        "instruction": "Provide a question that is a duplicate or not duplicate of the given question."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_qqp_quora",
        "instruction": "Given two questions, determine whether they are asking the same thing. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_qqp_quora",
        "instruction": "Given a question, determine whether it is asking about controlling horniness. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_qqp_same_thing",
        "instruction": "Determine if two questions are asking the same thing. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Appropriate_continuation_Yes_or_No",
        "instruction": "Given a description and a continuation, determine if the continuation is appropriate or not. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Open_ended_completion",
        "instruction": "Complete the sentence based on the given context."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Open_ended_completion",
        "instruction": "Choose the correct option based on the given context."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Open_ended_start",
        "instruction": "Given the target sentence, please complete the sentence starting with the given phrase."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Open_ended_start",
        "instruction": "Given the starting phrase, please write a target sentence."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Predict_ending_with_hint",
        "instruction": "Create a sentence that ends with the provided target."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Randomized_prompts_template",
        "instruction": "Choose the correct ending for the given sentence from the list of options."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Randomized_prompts_template",
        "instruction": "Create a sentence that ends with the given target."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Randomized_prompts_template_score_eval",
        "instruction": "Given a sentence and a list of possible endings, select the ending that makes the most sense for the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Topic_of_the_context",
        "instruction": "Given an input paragraph, identify the topic of the paragraph."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Topic_of_the_context",
        "instruction": "Given a topic, write a paragraph that describes it."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Topic_without_the_ending_answer",
        "instruction": "Given a sentence, please write a possible ending answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Topic_without_the_ending_answer",
        "instruction": "Given a sentence, please write a possible topic."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_complete_first_then",
        "instruction": "Complete the description with an appropriate ending."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_complete_first_then",
        "instruction": "Given a stack of baking pans in a large kitchen, complete the description with an appropriate ending."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_complete_first_then_score_eval",
        "instruction": "Complete the description with an appropriate ending."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_how_ends",
        "instruction": "Given the input and answer choices, select the correct ending that likely completes the description. Answers must be one of Ending 1, Ending 4, Ending 2, Ending 3."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_how_ends",
        "instruction": "Create a description that ends with the given target."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_if_begins_how_continues",
        "instruction": "Given a description of a situation and a few possible endings, please select the correct ending. Answers must be one of Ending 1, Ending 4, Ending 2, Ending 3."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_if_begins_how_continues",
        "instruction": "Create a description of a situation and a few possible endings."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-imdb_Movie_Expressed_Sentiment",
        "instruction": "Can you write a review for the movie?"
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-imdb_Movie_Expressed_Sentiment_2",
        "instruction": "Can you generate a movie review with the given sentiment?"
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-imdb_Reviewer_Enjoyment",
        "instruction": "Given the targets, write a review of the movie."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-imdb_Text_Expressed_Sentiment",
        "instruction": "Create a text that expresses a sentiment (positive or negative) about a given topic."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-kilt_tasks_hotpotqa_combining_facts",
        "instruction": "Given a prompt that combines facts, answer a question based on the prompt."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-kilt_tasks_hotpotqa_complex_question",
        "instruction": "Answer the question based on the input. The answer is a segment of text from the input."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-kilt_tasks_hotpotqa_formulate",
        "instruction": "Answer the question based on the input. The answer should be a segment of text from the corresponding input."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-kilt_tasks_hotpotqa_formulate",
        "instruction": "Create a question based on the input."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-kilt_tasks_hotpotqa_straighforward_qa",
        "instruction": "Given an input question, please provide the answer."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-kilt_tasks_hotpotqa_straighforward_qa",
        "instruction": "Can you write a question based on the answer?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-kilt_tasks_hotpotqa_straighforward_qa",
        "instruction": "Can you identify the city where the hotel company has its head office?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-multi_news_distill",
        "instruction": "Given the inputs, generate a summary of the article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-multi_news_distill",
        "instruction": "Given the targets, generate a summary of the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-multi_news_expand_reverse_task_",
        "instruction": "Write a news article based on the given summary."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-multi_news_expand_reverse_task_",
        "instruction": "Can you write a summary for the given news article?"
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-multi_news_expand_reverse_task_",
        "instruction": "Can you identify the label of the given news article?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-multi_news_summarize",
        "instruction": "Given the inputs, write a summary of the articles."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-multi_news_summarize",
        "instruction": "Given the targets, write a summary of the articles."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-multi_news_summary_scenario",
        "instruction": "Given the inputs, generate a more concise summary of the article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-multi_news_summary_scenario",
        "instruction": "Given the inputs, generate a question that can be answered by the targets."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-multi_news_synthesize",
        "instruction": "Given the inputs, generate the targets."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-multi_news_synthesize",
        "instruction": "Given the targets, generate the inputs."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_choices",
        "instruction": "Given the answer choices and the input, please select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_choose_an_answer_with_options",
        "instruction": "Choose the correct answer from the given options based on the input."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_only_options",
        "instruction": "Given the answer choices and the input, please select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_pick_answer_with_options",
        "instruction": "Pick the right answer from the list based on the given input."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_pick_answer_with_options",
        "instruction": "Create a new input prompt with the given answer choices and target."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_pick_using_id",
        "instruction": "Given the answer choices and the input prompt, select the correct answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-openbookqa_main_pick_using_id",
        "instruction": "Create a new prompt with answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_which_correct",
        "instruction": "Given the input and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_which_correct_inverse",
        "instruction": "Given the answer choices and the prompt, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_Concatenation",
        "instruction": "Determine whether the two sentences are paraphrases of each other. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_Concatenation",
        "instruction": "Determine whether the given sentence is a paraphrase of the other sentence. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_Concatenation_no_label",
        "instruction": "Determine whether the two sentences are paraphrases of each other. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_Concatenation_no_label",
        "instruction": "Provide a sentence that is a paraphrase of the given sentence. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_Meaning",
        "instruction": "Determine whether two sentences express the same meaning. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_Meaning_no_label",
        "instruction": "Determine whether two sentences express the same meaning. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_PAWS_ANLI_GPT3",
        "instruction": "Answer the question based on the given input and choose True or False. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_PAWS_ANLI_GPT3_no_label",
        "instruction": "Determine whether the given statement is true or false based on the information provided. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_Rewrite",
        "instruction": "Given two sentences, determine if they can be rewritten to each other. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_Rewrite_no_label",
        "instruction": "Determine if the two sentences are already in the same format. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_context_question",
        "instruction": "Create a sentence that is a paraphrase of the input sentence. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_paraphrase_task",
        "instruction": "Identify the original sentence given the paraphrased sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_task_description_no_label",
        "instruction": "Determine if the following two sentences paraphrase each other or not. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_task_description_no_label",
        "instruction": "Provide the correct order of the following sentences. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_task_description_no_label",
        "instruction": "Can you provide a sentence that paraphrases the given sentence? Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Correct_the_solution_if_false_from_sol_1",
        "instruction": "Correct the sentence so that it makes sense."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Correct_the_solution_if_false_from_sol_1",
        "instruction": "Correct the sentence if it does not make sense, otherwise copy it."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Correct_the_solution_if_false_from_sol_2",
        "instruction": "Given a sentence, correct it if it does not make sense. If it makes sense, just return it as the answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Correct_the_solution_if_false_from_sol_2",
        "instruction": "Create a sentence that is similar in meaning to the input sentence, but with a different word choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Correct_the_solution_if_false_from_sol_2",
        "instruction": "Given a sentence, identify the part of the sentence that does not make sense and provide a corrected version of that part."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Does_this_solution_make_sense_sol1",
        "instruction": "Determine whether the given phrase makes sense or not. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "answer_choices",
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Does_this_solution_make_sense_sol1",
        "instruction": "Provide a new phrase that makes sense or does not make sense based on the given answer choices. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Does_this_solution_make_sense_sol2",
        "instruction": "Determine whether the phrase makes sense or not. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Does_this_solution_make_sense_sol2",
        "instruction": "Provide a new target for the given phrase. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_choose_the_most_appropriate_solution",
        "instruction": "Create a goal and 2 solutions, and ask the model to choose the most appropriate solution."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_finish_sentence_with_correct_choice",
        "instruction": "Given a sentence with a blank and two options, determine which option is the best to fill in the blank."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_no_prompt_needed",
        "instruction": "Given an input and a target, please generate a question that can be answered by the target."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_pick_correct_choice_index",
        "instruction": "Given a sentence and two choices, please select the index of the correct choice for ending the sentence. Answers must be one of 2, 1."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_pick_correct_choice_index",
        "instruction": "Create a sentence with two choices, and ask the model to select the index of the correct choice for ending the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_what_is_the_correct_ending",
        "instruction": "Given the input and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_combined_facts_1",
        "instruction": "Given the answer choices and the question, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_1",
        "instruction": "Given the inputs and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_2",
        "instruction": "Answer the question based on the given facts and options."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_2",
        "instruction": "Given the facts and options, write a question that can be answered with the given target."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_3",
        "instruction": "Given the two facts, please select the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_4",
        "instruction": "Choose the best answer to the given question based on the answer choices provided."
    },
    {
        "input_fields": [
            "answer_choices",
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_4",
        "instruction": "Provide a question based on the answer choices and the information provided."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_5",
        "instruction": "Given the quiz and hints, please select the best answer to the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_description_question_answer_id",
        "instruction": "Given the inputs, please select the correct answer choice. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_description_question_answer_text",
        "instruction": "Given the inputs, please select the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_description_question_text",
        "instruction": "Given the inputs, select the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_answer_description_id",
        "instruction": "Given the inputs, please select the correct answer choice. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_answer_description_id",
        "instruction": "Create a description based on the inputs. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_answer_description_id",
        "instruction": "Can you write an input based on the target?"
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_answer_description_text",
        "instruction": "Given the inputs, please select the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_answer_description_text",
        "instruction": "Given the inputs, please write a description of the text."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_answer_description_text",
        "instruction": "Given the inputs, please write a question based on the text."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_description_answer_id",
        "instruction": "Given the inputs, please select the correct answer choice. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_description_answer_text",
        "instruction": "Given the inputs, please select the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_description_text",
        "instruction": "Given the inputs, select the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_description_context_question_answer_id",
        "instruction": "Choose the correct option to answer the question based on the given context. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_description_context_question_answer_id",
        "instruction": "Create a question based on the given context. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_description_context_question_answer_id",
        "instruction": "Can you identify the correct context based on the given question and answer choices?"
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_description_context_question_answer_text",
        "instruction": "Choose the correct option to answer the question based on the given context."
    },
    {
        "input_fields": [
            "answer_choices",
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_description_context_question_answer_text",
        "instruction": "Can you write a context given the answer choices and the question?"
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_description_context_question_text",
        "instruction": "Please select the correct answer based on the context."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_description_context_question_text",
        "instruction": "Create a context and question provided the answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_no_prompt_id",
        "instruction": "Given the input and answer choices, select the correct answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_no_prompt_id",
        "instruction": "Given the input and answer choices, select the incorrect answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_no_prompt_text",
        "instruction": "Given the inputs, select the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quarel_choose_between",
        "instruction": "Choose the correct answer based on the given question and answer choices."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-quarel_choose_between",
        "instruction": "Create a question based on the given answer choices and the context."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quarel_do_not_use",
        "instruction": "Choose the correct answer to the question."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-quarel_do_not_use",
        "instruction": "Create a question based on the given answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quarel_heres_a_story",
        "instruction": "Choose the correct answer to the question based on the given story."
    },
    {
        "input_fields": [
            "answer_choices",
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quarel_heres_a_story",
        "instruction": "Create a story given the answer choices and the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quarel_logic_test",
        "instruction": "Answer the question based on the given logic test."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quarel_logic_test",
        "instruction": "Create a logic test given the answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quarel_testing_students",
        "instruction": "Given a logic test, please choose the correct answer between the given choices."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quarel_testing_students",
        "instruction": "Create a logic test provided the answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_answer_question_based_on",
        "instruction": "Answer the question based on the following text."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_answer_question_below",
        "instruction": "Answer the question based on the given context."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_answer_question_below",
        "instruction": "Can you write a question based on the given context?"
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_given_the_fact_answer_the_q",
        "instruction": "Given the fact and answer choices, answer the question."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-quartz_given_the_fact_answer_the_q",
        "instruction": "Create a fact and answer choices, and ask a question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_having_read_above_passage",
        "instruction": "Choose the right answer to the following question based on the given passage."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_having_read_above_passage",
        "instruction": "Create a passage and a question with answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_paragraph_question_plain_concat",
        "instruction": "Given a sentence with a blank, choose the correct word to fill in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_read_passage_below_choose",
        "instruction": "Choose the right answer to the following question."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_use_info_from_paragraph_question",
        "instruction": "Answer the question based on the paragraph. The answer is one of the answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_use_info_from_paragraph_question",
        "instruction": "Determine the missing word in the paragraph based on the answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_use_info_from_question_paragraph",
        "instruction": "Answer the question based on the paragraph. The answer is one of the answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Answer_Test",
        "instruction": "Given an article and a question, please answer the question based on the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Context_Contains_Answer",
        "instruction": "Given the input text, please identify the name of the scientist mentioned in the report."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Context_Contains_Answer",
        "instruction": "Given the input text, please identify the pseudonym used by Philip Arnold Heseltine."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Find_Answer",
        "instruction": "Please answer the question based on the article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Find_Answer",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Found_Context_Online",
        "instruction": "Please answer the question based on the article. The answer to every question is a segment of text from the corresponding reading passage."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Guess_Title_For_Context",
        "instruction": "Given the title of the article, can you write a summary of the context?"
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Read_And_Extract_",
        "instruction": "Create a question provided the paragraph."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_high_Read_the_article_and_answer_the_question_no_option_",
        "instruction": "Please select the correct answer based on the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_high_Read_the_article_and_answer_the_question_no_option_",
        "instruction": "Can you write a sentence that describes the difference between the husband and wife in terms of shopping?"
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_high_Select_the_best_answer",
        "instruction": "Select the best answer based on the given article. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_high_Select_the_best_answer_generate_span_",
        "instruction": "Select the best answer based on the given article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_high_Select_the_best_answer_generate_span_",
        "instruction": "Generate a sentence that describes the difference between two people's shopping habits."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_high_Select_the_best_answer_generate_span_",
        "instruction": "Can you write a question based on the given article?"
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_high_Select_the_best_answer_no_instructions_",
        "instruction": "Given the inputs and answer choices, select the best answer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_high_Taking_a_test",
        "instruction": "Given the article and answer choices, please select the correct answer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_high_Taking_a_test",
        "instruction": "Create a sentence that fits the article. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_high_Taking_a_test",
        "instruction": "Can you write a question based on the article? Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_middle_Select_the_best_answer_generate_span_",
        "instruction": "Select the best answer based on the given article and answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_middle_Select_the_best_answer_generate_span_",
        "instruction": "Generate a span based on the given article and answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_middle_Select_the_best_answer_no_instructions_",
        "instruction": "Please select the best answer based on the given context. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_plain_background_situation",
        "instruction": "Can you identify the cup with higher concentration of sugar?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_plain_background_situation",
        "instruction": "Can you identify the cup with lower concentration of sugar?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_plain_bottom_hint",
        "instruction": "Determine which cup has a higher concentration of sugar based on the given information."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_plain_bottom_hint",
        "instruction": "Determine which cup has a lower concentration of sugar based on the given information."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_prompt_beginning",
        "instruction": "Please answer correctly the following question related to the paragraph below. Which cup has a higher concentration of salt?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_prompt_beginning",
        "instruction": "Please answer correctly the following question related to the paragraph below. Which cup has a lower concentration of sugar?"
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_prompt_bottom_hint_beginning",
        "instruction": "Create a paragraph provided the answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_prompt_bottom_hint_beginning",
        "instruction": "Can you write a question for the paragraph?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_prompt_bottom_no_hint",
        "instruction": "Given a paragraph and a question, please answer correctly the following question: Which cup has a higher concentration of sugar?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_prompt_bottom_no_hint",
        "instruction": "Given a paragraph and a question, please answer correctly the following question: Which cup has a lower concentration of sugar?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_prompt_mix",
        "instruction": "Please answer the question based on the paragraph above. The answer is either \"cup A\" or \"cup B\"."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_prompt_mix",
        "instruction": "Please write a paragraph that includes two cups, cup A and cup B, filled with equal amounts of water on to a table and a child pouring different amounts of sugar into them. The paragraph should end with a question asking which cup has a higher concentration of sugar. The answer is either \"cup A\" or \"cup B\"."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_prompt_mix",
        "instruction": "Please write a paragraph that includes two cups, cup A and cup B, filled with equal amounts of water on to a table and a child pouring different amounts of sugar into them. The paragraph should end with a question asking which cup has a lower concentration of sugar. The answer is either \"cup A\" or \"cup B\"."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_read_background_situation",
        "instruction": "Given the inputs and targets, can you predict the correct target?"
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-rotten_tomatoes_Movie_Expressed_Sentiment_2",
        "instruction": "Create a movie review that expresses the given sentiment."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-rotten_tomatoes_Reviewer_Enjoyment_Yes_No",
        "instruction": "Create a movie review given whether the reviewer enjoyed the movie or not."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-rotten_tomatoes_Reviewer_Expressed_Sentiment",
        "instruction": "Create a movie review based on the sentiment expressed."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-rotten_tomatoes_Reviewer_Opinion_bad_good_choices",
        "instruction": "Create an input sentence and answer choices, and provide whether the reviewer found the movie good or bad."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-rotten_tomatoes_Sentiment_with_choices_",
        "instruction": "Create a review and its sentiment."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-rotten_tomatoes_Text_Expressed_Sentiment",
        "instruction": "Can you write a text expressing a certain sentiment?"
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-samsum_Generate_a_summary_for_this_dialogue",
        "instruction": "Generate a dialogue based on the given summary."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-samsum_Given_the_above_dialogue_write_a_summary",
        "instruction": "Given the above dialogue, write a summary of the conversation."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-samsum_Sum_up_the_following_dialogue",
        "instruction": "Summarize the following dialogue into a question and an answer."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-samsum_Sum_up_the_following_dialogue",
        "instruction": "Create a dialogue based on the given input."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-samsum_Summarize_this_dialogue_",
        "instruction": "Summarize the dialogue based on the input and output fields."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-samsum_Summarize_this_dialogue_",
        "instruction": "Can you generate a dialogue based on the input and output fields?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-samsum_To_sum_up_this_dialog",
        "instruction": "Please generate a dialog based on the input template."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-samsum_To_sum_up_this_dialog",
        "instruction": "Please write a summary of the dialog."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-sciq_Direct_Question",
        "instruction": "Answer the question based on the given paragraph."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-sciq_Direct_Question",
        "instruction": "Create a paragraph given the question and answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-sciq_Direct_Question",
        "instruction": "What is the phenomenon that makes global winds blow in a certain direction?"
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-sciq_Direct_Question_Closed_Book_",
        "instruction": "Given the question and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-sciq_Direct_Question_Closed_Book_",
        "instruction": "Create a question based on the answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-sciq_Multiple_Choice",
        "instruction": "Given a paragraph and a question, select the correct answer from a list of choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-sciq_Multiple_Choice_Closed_Book_",
        "instruction": "Given a question and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "answer_choices",
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-sciq_Multiple_Choice_Closed_Book_",
        "instruction": "Create a question based on the answer choices and the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-sciq_Multiple_Choice_Question_First",
        "instruction": "Given the input and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-social_i_qa_Check_if_a_random_answer_is_valid_or_not",
        "instruction": "Given the question and a possible answer, determine if the answer is valid or not. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-social_i_qa_Generate_answer",
        "instruction": "Given the context and answer choices, select the answer choice that best fits the context."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-social_i_qa_Generate_answer",
        "instruction": "Create a context and answer choices, and provide the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-social_i_qa_I_was_wondering",
        "instruction": "Given the inputs, choose the best answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-social_i_qa_Show_choices_and_generate_answer",
        "instruction": "Given the context and possible answers, select the answer that best fits the context."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-social_i_qa_Show_choices_and_generate_index",
        "instruction": "Given the context and answer choices, select the best answer that answers the question \"How would Others feel as a result?\" Answers must be one of A, C, B."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-social_i_qa_Show_choices_and_generate_index",
        "instruction": "Given the context and answer choices, select the best answer that answers the question \"What will Others want to do next?\" Answers must be one of A, C, B."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-squad_v2_Jeopardy_with_Context",
        "instruction": "Can you write a question for the given context?"
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-squad_v2_Jeopardy_without_Context",
        "instruction": "Given an answer, please generate a question that would elicit that answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-squad_v2_Jeopardy_without_Context",
        "instruction": "Given a question, please generate the corresponding answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-squad_v2_Questions_with_Context",
        "instruction": "Please answer the question based on the context."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-squad_v2_Questions_with_Context_unanswerable",
        "instruction": "Please answer the question based on the context. The answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-squad_v2_Topic_Prediction_Question_and_Answer_Pair",
        "instruction": "Given a topic, generate a question and answer pair."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-squad_v2_Trivia",
        "instruction": "Given an input, please provide the corresponding target."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-squad_v2_Trivia",
        "instruction": "Can you generate a new input based on the target?"
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_GPT_3_Style",
        "instruction": "Create a question based on the given input."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_GPT_3_Style",
        "instruction": "Determine if the answer to the question is Yes or No. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_I_wonder_",
        "instruction": "Given a statement and answer choices, determine whether the answer is correct or not. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_after_reading",
        "instruction": "After reading the passage, please answer the question with True or False. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_after_reading",
        "instruction": "Can you write a passage that would result in the given answer?"
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_based_on_the_following_passage",
        "instruction": "Based on the following passage, please select the correct answer choice. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_based_on_the_previous_passage",
        "instruction": "Based on the previous passage, please select the correct answer. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_could_you_tell_me_",
        "instruction": "Answer the question based on the given information. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_could_you_tell_me_",
        "instruction": "Create a question based on the given information."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_exam",
        "instruction": "Answer the question based on the given document. The answer is either Yes or No. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_exam",
        "instruction": "Create a document and answer choices for a given question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_exercise",
        "instruction": "Please answer the question by True or False based on the given text. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_valid_binary",
        "instruction": "Answer the question based on the given text. The answer is either True or False. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_valid_binary",
        "instruction": "Can you identify the correct answer choice based on the given text? Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_yes_no_question",
        "instruction": "Answer the following yes/no question based on the given text. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_GPT_3_style",
        "instruction": "Can you write a sentence that matches the given answer choice?"
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_MNLI_crowdsource_score_eval",
        "instruction": "Given an input template and a target template, determine whether the target template is correct or incorrect based on the input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_always_sometimes_never_score_eval",
        "instruction": "Given an input template and a target template, determine whether the target template is always, sometimes, or never true based on the input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_based_on_the_previous_passage_score_eval",
        "instruction": "Given an input template and a target template, determine whether the target template is correct based on the input template. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_based_on_the_previous_passage_score_eval",
        "instruction": "Given an input template and a target template, determine the label of the target template. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_can_we_infer_score_eval",
        "instruction": "Given an input template and a target template, determine whether the target template is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_can_we_infer_score_eval",
        "instruction": "Given an input template and a target template, predict the target template. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_claim_true_false_inconclusive",
        "instruction": "Create a sentence that matches the given target."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_claim_true_false_inconclusive_score_eval",
        "instruction": "Given an input and a claim, determine whether the claim is true, false, or inconclusive based on the information provided. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_claim_true_false_inconclusive_score_eval",
        "instruction": "Given an input and a claim, generate the corresponding target (true, false, or inconclusive). Answers must be one of True, Inconclusive, False."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-super_glue_cb_consider_always_sometimes_never",
        "instruction": "Create a text and answer choices, such that the answer choice is always correct."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_consider_always_sometimes_never_score_eval",
        "instruction": "Determine whether the given statement is always, sometimes, or never correct based on the input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_consider_always_sometimes_never_score_eval",
        "instruction": "Provide the correct answer (always, sometimes, or never) based on the input. Answers must be one of Always, Never, Sometimes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_does_it_follow_that_score_eval",
        "instruction": "Given an input template and a target template, determine whether the target follows from the input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_does_it_follow_that_score_eval",
        "instruction": "Given an input template and a target template, generate a target that does not follow from the input. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_does_this_imply_score_eval",
        "instruction": "Given an input template and a target template, determine whether the target template is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_does_this_imply_score_eval",
        "instruction": "Given an input template and a target template, predict the target template. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_guaranteed_possible_impossible_score_eval",
        "instruction": "Given an input template and a target template, determine whether the target is guaranteed, possible, or impossible based on the input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_guaranteed_true_score_eval",
        "instruction": "Given the input and target templates, please select the correct label choice. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_guaranteed_true_score_eval",
        "instruction": "Given the input and target templates, please predict the label choice. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_justified_in_saying_score_eval",
        "instruction": "Given an input template and a target template, please select whether the target template is justified in saying \"yes\", \"no\", or \"maybe\". Answers must be one of True, False."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-super_glue_cb_must_be_true",
        "instruction": "Create a new input and answer choices, given the target."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_must_be_true_score_eval",
        "instruction": "Given an input template and a target template, determine whether the target template is true or false based on the input template. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_must_be_true_score_eval",
        "instruction": "Given an input template and a target template, determine the correct answer to the target template. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_should_assume_score_eval",
        "instruction": "Given an input template and a target template, determine whether the target template is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_should_assume_score_eval",
        "instruction": "Given an input template and a target template, predict the target template. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_C1_or_C2_premise_so_because_",
        "instruction": "Choose the correct answer choice based on the given input and the word \"because\"."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_C1_or_C2_premise_so_because__score_eval",
        "instruction": "Given a sentence and a conjunction, determine which of the two options is the correct one to complete the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_C1_or_C2_premise_so_because__score_eval",
        "instruction": "Given a sentence and a conjunction, determine whether the correct option is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa__As_a_result_C1_or_C2_",
        "instruction": "Given the inputs, please select the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa__As_a_result_C1_or_C2__score_eval",
        "instruction": "Given the inputs, please select the correct target."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa__As_a_result_C1_or_C2__score_eval",
        "instruction": "Given the inputs and targets, please determine whether the target is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa__What_could_happen_next_C1_or_C2_",
        "instruction": "Given the inputs, please select the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized",
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa__What_could_happen_next_C1_or_C2__score_eval",
        "instruction": "Given an input template and two target templates, determine which target template is correct."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa__which_may_be_caused_by_score_eval",
        "instruction": "Given an input sentence and two possible causes, determine which cause may have caused the event described in the input sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa__which_may_be_caused_by_score_eval",
        "instruction": "Given an input sentence and two possible causes, determine whether the first or second cause may have caused the event described in the input sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa__why_C1_or_C2",
        "instruction": "Given the input and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa__why_C1_or_C2_score_eval",
        "instruction": "Given the input and target templates, please select the correct target template based on the input."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa__why_C1_or_C2_score_eval",
        "instruction": "Given the input and target templates, please determine whether the target template is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_best_option",
        "instruction": "Choose the best option that explains the cause of the given situation."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-super_glue_copa_best_option",
        "instruction": "Create a situation and two options, one of which explains the cause of the situation."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_best_option_score_eval",
        "instruction": "Given the input and options, please select the correct option. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_best_option_score_eval",
        "instruction": "Given the input and options, please select the incorrect option. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_cause_effect",
        "instruction": "Given a cause and effect statement, select the most plausible cause."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_cause_effect",
        "instruction": "Given a cause and effect statement, write a plausible cause."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_cause_effect_score_eval",
        "instruction": "Given an input and a target, select the correct target that is the most plausible cause of the input."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_cause_effect_score_eval",
        "instruction": "Given an input and a target, determine whether the target is the correct cause of the input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_choose",
        "instruction": "Choose the correct answer based on the given context."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-super_glue_copa_choose",
        "instruction": "Create a context and two answer choices based on the given target."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_choose_score_eval",
        "instruction": "Choose the correct option that completes the sentence based on the given context."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_choose_score_eval",
        "instruction": "Determine whether the given option is correct or not based on the given context. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_exercise",
        "instruction": "Choose the most plausible alternative given the context."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_exercise_score_eval",
        "instruction": "Choose the most plausible alternative given an input and two options. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_i_am_hesitating",
        "instruction": "Choose the correct option that explains the given situation."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_i_am_hesitating_score_eval",
        "instruction": "Given an input template and a target template, choose the correct target that matches the input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_i_am_hesitating_score_eval",
        "instruction": "Given an input template and a target template, predict whether the target is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_more_likely",
        "instruction": "Choose the correct answer choice based on the given input and target templates."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_more_likely",
        "instruction": "Create a new input and target template given the answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_more_likely_score_eval",
        "instruction": "Given a sentence and two possible reasons, choose the more likely reason that caused the shadow over the grass. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_more_likely_score_eval",
        "instruction": "Given a sentence and two possible reasons, choose the less likely reason that caused the shadow over the grass. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_plausible_alternatives",
        "instruction": "Choose the correct answer based on the given context."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_plausible_alternatives_score_eval",
        "instruction": "Given an input and two options, choose the more plausible option. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_plausible_alternatives_score_eval",
        "instruction": "Given an input and two options, choose the less plausible option. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_Would_it_be_good_to_answer_",
        "instruction": "Given the inputs, please select the correct answer choice. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_confirm",
        "instruction": "Given the inputs, please select the correct answer choice. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_confirm",
        "instruction": "Create a new input by replacing a word or phrase in the original input, and provide the corresponding target. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_correct",
        "instruction": "Given the inputs, please select the correct answer choice. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_decide_valid",
        "instruction": "Given the inputs, please select the correct answer choice. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_decide_valid",
        "instruction": "Create a new input by replacing a word or phrase in the original input, and provide the corresponding target. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_found_this_answer",
        "instruction": "Given the inputs, please select the correct answer choice. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_grading",
        "instruction": "Given the inputs, please select the correct answer choice. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_grading",
        "instruction": "Create a new input and target template for a new NLP task."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_is_a_correct_answer_",
        "instruction": "Given the inputs, please select the correct answer choice. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_is_a_correct_answer_",
        "instruction": "Create a new input and target template for a new NLP task."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_is_the_correct_answer_",
        "instruction": "Given the inputs, please select the correct answer from the answer choices. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_Add_sentence_after_after_continuation_choices_",
        "instruction": "Given the article, please write a new sentence to add to it."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_Add_sentence_after_after_continuation_choices_",
        "instruction": "Choose the correct answer choice based on the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_Add_sentence_after_continuation_choices_",
        "instruction": "Create a sentence that follows the given input sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_Can_you_figure_out_",
        "instruction": "Given the inputs and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_GPT_3_style_continuation_choices_",
        "instruction": "Given the inputs and answer choices, select the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_GPT_3_style_summary_only_continuation_choices_",
        "instruction": "Given the inputs and answer choices, select the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_GPT_3_style_with_labels_continuation_choices_",
        "instruction": "Given the article and answer choices, please select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_GPT_3_style_with_labels_without_hyphens_continuation_choices_",
        "instruction": "Given the article, please select the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_GPT_3_style_without_hyphens_continuation_choices_",
        "instruction": "Given the inputs and answer choices, select the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_In_the_question_above_the_placeholder_stands_for",
        "instruction": "Given the inputs and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_In_the_question_above_the_placeholder_stands_for",
        "instruction": "Create a sentence based on the given inputs."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_News_article_continuation_choices_",
        "instruction": "Please write a sentence to add to the news article."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_News_article_continuation_choices_",
        "instruction": "Choose the correct answer choice based on the news article."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_Summary_first_continuation_choices_",
        "instruction": "Given the inputs and answer choices, select the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_What_could_the_placeholder_be_",
        "instruction": "Given the inputs and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_What_could_the_placeholder_be_",
        "instruction": "Create a sentence based on the given inputs."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_Which_one_is_the_placeholder_",
        "instruction": "Given the inputs and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_Which_one_is_the_placeholder_",
        "instruction": "Create a sentence based on the given inputs."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_choose_between",
        "instruction": "Choose the correct answer based on the given context."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-super_glue_record_choose_between",
        "instruction": "Create a context and answer choices for a given target."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_corrupted",
        "instruction": "Please fill in the blank with the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_exercise",
        "instruction": "Extract the correct entity that \"@placeholder\" is referring to."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_exercise",
        "instruction": "Identify the country or city mentioned in the given text."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_pick_one_option",
        "instruction": "Given the inputs and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_the_placeholder_refers_to_",
        "instruction": "Given the inputs and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_trying_to_decide",
        "instruction": "Given the inputs and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_GPT_3_style",
        "instruction": "Determine whether the hypothesis is true or false based on the given input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_GPT_3_style",
        "instruction": "Given the input and answer choices, write a statement that is true. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_GPT_3_style_score_eval",
        "instruction": "Determine whether the statement is true or false based on the given input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_GPT_3_style_score_eval",
        "instruction": "Provide the correct statement based on the given input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_MNLI_crowdsource",
        "instruction": "Determine whether the given statement is definitely correct based on the provided description and world knowledge. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_MNLI_crowdsource_score_eval",
        "instruction": "Given the input and target templates, please select the correct label (entailment, neutral, or contradiction) based on the input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_based_on_the_previous_passage",
        "instruction": "Based on the previous passage, answer the question with \"Yes\" or \"No\". Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-super_glue_rte_based_on_the_previous_passage",
        "instruction": "Create a passage and a question with answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_based_on_the_previous_passage_score_eval",
        "instruction": "Given the input and target templates, determine whether the target is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_based_on_the_previous_passage_score_eval",
        "instruction": "Given the input and target templates, predict the target. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_can_we_infer",
        "instruction": "Given an input template and a target template, please select the correct answer choice. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_can_we_infer_score_eval",
        "instruction": "Given an input template and a target template, determine whether the target template is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_can_we_infer_score_eval",
        "instruction": "Given an input template and a target template, predict the target template. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_does_it_follow_that",
        "instruction": "Given the input and target templates, please select the correct answer choice. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_does_it_follow_that_score_eval",
        "instruction": "Given an input and a target, determine whether the input follows the target or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_does_it_follow_that_score_eval",
        "instruction": "Given an input and a target, predict the target. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_guaranteed_true",
        "instruction": "Given an input and a target, determine whether the target is guaranteed true based on the input. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-super_glue_rte_guaranteed_true",
        "instruction": "Create an input and a target based on a given label."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_guaranteed_true_score_eval",
        "instruction": "Given an input and a target, determine whether the target is guaranteed true based on the input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_guaranteed_true_score_eval",
        "instruction": "Given an input and a target, predict the target. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-super_glue_rte_justified_in_saying",
        "instruction": "Create an input and a target, and ask whether the input is justified in saying the target."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_justified_in_saying_score_eval",
        "instruction": "Given an input template and a target template, determine whether the target template is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_justified_in_saying_score_eval",
        "instruction": "Given an input template and a target template, predict the target template. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_must_be_true",
        "instruction": "Create an input and target template given the answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_must_be_true_score_eval",
        "instruction": "Given an input template and a target template, determine whether the target template is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_must_be_true_score_eval",
        "instruction": "Given an input template and a target template, predict the target template. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_should_assume_score_eval",
        "instruction": "Given an input and a target, determine whether the target is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_should_assume_score_eval",
        "instruction": "Given an input and a target, predict the target. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_GPT_3_prompt",
        "instruction": "Determine whether the word in question is used in the same sense in the two sentences provided. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_GPT_3_prompt_score_eval",
        "instruction": "Determine whether the word \"place\" is used in the same sense in the two sentences provided. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_GPT_3_prompt_score_eval",
        "instruction": "Provide the correct usage of the word \"place\" in the two sentences provided. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_GPT_3_prompt_with_label",
        "instruction": "Determine if the word in question is used in the same sense in the two sentences provided. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_GPT_3_prompt_with_label",
        "instruction": "Create a sentence using the same word in two different senses."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_GPT_3_prompt_with_label_score_eval",
        "instruction": "Determine whether the word \"place\" is used in the same sense in the two given sentences. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_GPT_3_prompt_with_label_score_eval",
        "instruction": "Given a sentence and a target word, determine whether the target word is used in the same sense as in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_affirmation_true_or_false_score_eval",
        "instruction": "Determine whether the two sentences have a word with similar meaning. True or False? Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_affirmation_true_or_false_score_eval",
        "instruction": "Provide the opposite answer to the previous task. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_grammar_homework",
        "instruction": "Decide whether the word used in two sentences has the same meaning. Answer by yes or no. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_grammar_homework_score_eval",
        "instruction": "Determine whether the word \"place\" is used with the same meaning in the two following sentences. Answer by yes or no. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_grammar_homework_score_eval",
        "instruction": "Provide the correct answer to the question. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_polysemous_score_eval",
        "instruction": "Given a sentence with a polysemous word, determine whether the word has the same meaning in two different contexts. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_polysemous_score_eval",
        "instruction": "Given a sentence with a polysemous word, identify whether the word has the same meaning in two different contexts. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct",
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_polysemous_score_eval",
        "instruction": "Given a sentence with a polysemous word, predict whether the word has the same meaning in two different contexts. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context",
        "instruction": "Determine if the word 'place' is used in the same way in the two sentences below. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context",
        "instruction": "Determine if the word 'approach' is used in the same way in the two sentences below. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context",
        "instruction": "Can you identify the correct sentence based on the given answer choices? Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context_meaning",
        "instruction": "Determine whether the two sentences have the same meaning based on the given context and answer choices. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context_meaning",
        "instruction": "Create a sentence that has a different meaning from the given sentence based on the context and answer choices. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context_meaning_score_eval",
        "instruction": "Determine whether the word \"place\" has the same meaning in two given sentences. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context_meaning_score_eval",
        "instruction": "Provide the correct meaning of the word \"place\" in the given sentences. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context_meaning_with_label",
        "instruction": "Determine whether the two sentences have the same meaning based on the context and the word in question. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context_meaning_with_label",
        "instruction": "Create a sentence using the same word with different meanings. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context_meaning_with_label_score_eval",
        "instruction": "Determine whether the word \"place\" has the same meaning in two given sentences. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context_meaning_with_label_score_eval",
        "instruction": "Provide the correct meaning of the word \"place\" in the given context. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context_score_eval",
        "instruction": "Determine if the word 'place' is used in the same way in the two sentences below. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context_score_eval",
        "instruction": "Provide the correct usage of the word \"place\" in the two sentences below. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_same_sense",
        "instruction": "Determine whether the word used in the same sense in both sentences. Yes or no? Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_same_sense_score_eval",
        "instruction": "Determine whether the word in question is used in the same sense in both sentences. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_same_sense_score_eval",
        "instruction": "Provide the correct answer to the question. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_similar_sense",
        "instruction": "Determine whether the two sentences have a similar sense of place. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_similar_sense",
        "instruction": "Determine whether the two sentences have a similar sense of approach. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_similar_sense",
        "instruction": "Can you write a sentence that has a similar sense of place? Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_similar_sense",
        "instruction": "Can you write a sentence that has a similar sense of approach? Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_similar_sense_score_eval",
        "instruction": "Given an input and a target, determine whether the target has a similar sense of the input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_similar_sense_score_eval",
        "instruction": "Given an input and a target, determine whether the target has a different sense of the input. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_GPT_3_Style",
        "instruction": "Given a passage and a question, determine whether the pronoun \"he\" refers to the subject mentioned in the passage. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_GPT_3_Style",
        "instruction": "Given a passage and a question, determine whether the pronoun \"them\" refers to the subject mentioned in the passage. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_GPT_3_Style_score_eval",
        "instruction": "Given a passage and a question, determine whether the pronoun \"He\" refers to Mark or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_GPT_3_Style_score_eval",
        "instruction": "Given a passage and a question, provide the correct answer to whether the pronoun \"He\" refers to Mark or not. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_I_think_they_mean",
        "instruction": "Answer the question based on the input and target templates. The answer is either \"Yes\" or \"No\". Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_I_think_they_mean",
        "instruction": "Create a new input template given the target template and answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_I_think_they_mean_score_eval",
        "instruction": "Given an input and a target, determine whether the target is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_Who_or_what_is_are",
        "instruction": "Given a sentence with a question and an answer, determine if the answer is correct or not. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_by_p_they_mean",
        "instruction": "Answer the question based on the given context. The answer is either \"Yes\" or \"No\". Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_by_p_they_mean",
        "instruction": "Determine what \"they\" refers to in the given context. The answer is either \"mothers\" or \"Arthur and Celeste\". Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_by_p_they_mean_score_eval",
        "instruction": "Given an input sentence and a target word or phrase, determine whether the target refers to a specific entity in the input sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_by_p_they_mean_score_eval",
        "instruction": "Given an input sentence and a target word or phrase, provide the correct referent for the target. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_does_p_stand_for",
        "instruction": "Given a sentence and a pronoun, determine whether the pronoun refers to the subject or not. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_does_p_stand_for",
        "instruction": "Create a sentence with a pronoun and ask whether the pronoun refers to the subject or not."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_does_p_stand_for_score_eval",
        "instruction": "Given an input sentence and a pronoun, determine whether the pronoun refers to the subject of the sentence or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_does_p_stand_for_score_eval",
        "instruction": "Given an input sentence and a pronoun, provide the correct referent for the pronoun. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_does_the_pronoun_refer_to",
        "instruction": "Does the pronoun in the sentence refer to the specified noun? Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_does_the_pronoun_refer_to",
        "instruction": "Provide the noun that the pronoun in the sentence refers to. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_does_the_pronoun_refer_to_score_eval",
        "instruction": "Given an input sentence and a pronoun, determine whether the pronoun refers to the subject of the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_does_the_pronoun_refer_to_score_eval",
        "instruction": "Given an input sentence and a pronoun, identify the referent of the pronoun. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_in_other_words",
        "instruction": "Create a sentence that is a paraphrase of the given sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_in_other_words_score_eval",
        "instruction": "Given an input sentence and a rephrased sentence, determine whether they have the same meaning. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_in_other_words_score_eval",
        "instruction": "Given an input sentence and a rephrased sentence, determine whether they have different meanings. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_in_other_words_score_eval",
        "instruction": "Given an input sentence and a rephrased sentence, determine whether the rephrased sentence is more accurate than the input sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_p_is_are_r",
        "instruction": "Determine whether the answer to the question is True or False based on the context. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_p_is_are_r",
        "instruction": "Create a context and a question, and provide the answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_p_is_are_r_score_eval",
        "instruction": "Given a context and a question, determine whether the answer is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_p_is_are_r_score_eval",
        "instruction": "Given a context and a question, provide the correct answer. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_replaced_with",
        "instruction": "Given the inputs and targets, can you generate a new input template?"
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_replaced_with",
        "instruction": "Given the inputs and targets, can you generate a new target template? Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_replaced_with_score_eval",
        "instruction": "Given an input sentence and a pronoun, determine whether the pronoun can be replaced with a specific name. Answer \"Yes\" or \"No\". Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_replaced_with_score_eval",
        "instruction": "Given an input sentence and a pronoun, provide the correct replacement for the pronoun. Answer with \"Yes\" or \"No\". Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct",
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_replaced_with_score_eval",
        "instruction": "Given an input sentence and a pronoun, determine whether the pronoun can be replaced with a specific name and provide the correct replacement if possible. Answer with \"Yes\" or \"No\". Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_the_pronoun_refers_to",
        "instruction": "Determine whether the pronoun in the given sentence refers to the correct antecedent. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_the_pronoun_refers_to",
        "instruction": "Create a sentence with a pronoun and ask if the pronoun refers to the correct antecedent."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_the_pronoun_refers_to_score_eval",
        "instruction": "Given an input template and a target template, determine whether the target template is true or false based on the input template. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_ABBR",
        "instruction": "Given an abbreviation or an expression abbreviated, please provide the full form."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_ABBR_context_first",
        "instruction": "Given an abbreviation or an expression abbreviated, please provide the full form."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_DESC_context_first",
        "instruction": "Given the inputs and answer choices, please select the correct answer choice."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_HUM",
        "instruction": "Given the input and target templates, please write a new input template."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_HUM_context_first",
        "instruction": "Given the inputs, please write a question that asks for the specified answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_LOC",
        "instruction": "Given a question, predict the answer. Answers must be one of state, mountain, country, city, other location."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_LOC_context_first",
        "instruction": "Given a question and a prompt, select the correct answer choice from a list of options. Answers must be one of state, mountain, country, city, other location."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_NUM_context_first",
        "instruction": "Given a question and a list of possible answer types, determine the correct answer type for the question."
    },
    {
        "input_fields": [
            "answer_choices",
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_open",
        "instruction": "Given the answer choices and the input prompt, predict the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_open_context_first",
        "instruction": "Given the input and target templates, please select the correct answer choice."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_open_context_first",
        "instruction": "Create an input and target template given the answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_pick_the_best_descriptor",
        "instruction": "Given a question and a list of possible descriptors, select the best descriptor for the question. Answers must be one of Entity, Description, Quantity, Location, Abbreviation, Person."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_which_category_best_describes",
        "instruction": "Given a question, choose the category that best describes it from a list of options. Answers must be one of Entity, Description, Quantity, Location, Abbreviation, Person."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_which_category_best_describes",
        "instruction": "Create a question based on the category provided."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trivia_qa_unfiltered_first_person_context",
        "instruction": "Given a question, provide the answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trivia_qa_unfiltered_first_person_context",
        "instruction": "Given a person and a year, provide the corresponding event that happened in that year."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trivia_qa_unfiltered_formal_description",
        "instruction": "Please answer the question based on the input English question."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-trivia_qa_unfiltered_formal_description",
        "instruction": "Create a question provided the answer."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-trivia_qa_unfiltered_guess_question",
        "instruction": "Guess a question that has the answer provided in the input. The model should generate a question that is related to the answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trivia_qa_unfiltered_guess_question",
        "instruction": "Guess an answer that fits the question provided in the input. The model should generate an answer that is related to the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trivia_qa_unfiltered_question_answer",
        "instruction": "Given a question, provide the answer."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-trivia_qa_unfiltered_question_answer",
        "instruction": "Create a question based on the answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-web_questions_get_the_answer",
        "instruction": "Answer the question based on the input. The answer should be a segment of text from the corresponding target."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-web_questions_get_the_answer",
        "instruction": "Create a question provided the target."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-web_questions_potential_correct_answer",
        "instruction": "Provide a possible correct answer to the given question."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-web_questions_potential_correct_answer",
        "instruction": "Create a question based on the given answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-web_questions_question_answer",
        "instruction": "Answer the question based on the given input."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-web_questions_question_answer",
        "instruction": "Create a question based on the given target."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-web_questions_short_general_knowledge_q",
        "instruction": "Answer the question based on the given information."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-web_questions_short_general_knowledge_q",
        "instruction": "Create a question based on the given information."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_bio_comprehension",
        "instruction": "Create a bio provided the details."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_bio_guess_person",
        "instruction": "Guess the person based on the given details."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_bio_guess_person",
        "instruction": "Guess the club, caps, position, years, height, youthclubs, youthyears, pcupdate, birth date, fullname, birth place, and goals based on the given details."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_bio_key_content",
        "instruction": "Given a bio, please extract the birth date, name, nationality, and occupation of the person."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_bio_key_content",
        "instruction": "Given a bio, please extract the clubs, caps, position, years, height, youthclubs, youthyears, pcupdate, birth date, fullname, name, birth place, and goals of the person."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_bio_what_content",
        "instruction": "Please provide the birth date of the person described in the bio."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_bio_what_content",
        "instruction": "Please provide the nationality of the person described in the bio."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_bio_what_content",
        "instruction": "Please provide the occupation of the person described in the bio."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_bio_who",
        "instruction": "Given the bullet points, write a short biography describing the life of the person."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_choose_best_object_affirmative_1",
        "instruction": "Choose the best object from the answer choices based on the given information."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_choose_best_object_affirmative_2",
        "instruction": "Choose the best object from the answer choices based on the given information."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_choose_best_object_affirmative_3",
        "instruction": "Choose the best object from the answer choices based on the given information."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_choose_best_object_interrogative_1",
        "instruction": "Choose the best object that matches the given description."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_choose_best_object_interrogative_2",
        "instruction": "Choose the best object from the answer choices based on the given information and write an interrogative sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_choose_best_object_interrogative_2",
        "instruction": "Can you write a sentence using the given information?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_generate_subject_and_object",
        "instruction": "Given the information, generate a subject and an object."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_generate_subject_and_object",
        "instruction": "Can you identify the type of religion based on the information?"
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-wiki_qa_Decide_good_answer",
        "instruction": "Create a question based on the answer and ask if it is correct or not."
    },
    {
        "input_fields": [
            "answer_choices",
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_qa_Is_This_True_",
        "instruction": "Create a question based on the answer choices and the answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_qa_Topic_Prediction_Answer_Only",
        "instruction": "Determine the topic of the passage."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_qa_Topic_Prediction_Answer_Only",
        "instruction": "Create a passage based on the topic."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_qa_Topic_Prediction_Question_and_Answer_Pair",
        "instruction": "Given a question-answer pair, determine the topic of the pair."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_qa_Topic_Prediction_Question_and_Answer_Pair",
        "instruction": "Create a question-answer pair based on the topic."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_Replace",
        "instruction": "Given a sentence with a blank and two options, identify which option is correct."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_Replace_score_eval",
        "instruction": "Given an input sentence with a blank, choose the correct option to fill in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_Replace_score_eval",
        "instruction": "Given an input sentence with a blank, determine whether the given option is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_does_underscore_refer_to",
        "instruction": "Determine whether the blank in the sentence refers to the first or second option in the answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_does_underscore_refer_to_score_eval",
        "instruction": "Given an input sentence with a blank, choose the correct word to fill in the blank based on the context and answer whether the choice is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_does_underscore_refer_to_score_eval",
        "instruction": "Given an input sentence with a blank, choose the correct word to fill in the blank based on the context and provide the correct word."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_fill_in_the_blank",
        "instruction": "Fill in the blank in the given sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_fill_in_the_blank",
        "instruction": "Given a sentence and two names, identify which name is associated with the given description."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_fill_in_the_blank_score_eval",
        "instruction": "Identify the correct choice for the blank in the given sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_fill_in_the_blank_score_eval",
        "instruction": "Can you write a sentence with a blank that has two possible choices?"
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_stand_for",
        "instruction": "Given the sentence and the blank, choose the correct answer choice."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_stand_for",
        "instruction": "Create a sentence with a blank and two answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct",
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_stand_for_score_eval",
        "instruction": "Given a sentence with a blank, determine whether the blank stands for a specific word or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_stand_for_score_eval",
        "instruction": "Given a sentence with a blank, write the word that the blank stands for."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_underscore_refer_to",
        "instruction": "Given the inputs and answer choices, please select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_underscore_refer_to",
        "instruction": "Please fill in the blank with the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_underscore_refer_to_score_eval",
        "instruction": "Given an input sentence with a blank, choose the correct word to fill in the blank based on the question. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_underscore_refer_to_score_eval",
        "instruction": "Given an input sentence with a blank, choose the incorrect word to fill in the blank based on the question. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_Replace",
        "instruction": "Given the inputs, select the correct option to replace the blank in the sentence."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_Replace",
        "instruction": "Create a sentence with a blank to be replaced by one of the options provided."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_Replace_score_eval",
        "instruction": "Given the sentence and the correct option, replace the blank with the correct option. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_Replace_score_eval",
        "instruction": "Given the sentence and the correct option, predict the correct option. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_does_underscore_refer_to",
        "instruction": "Given a sentence with a blank, choose the correct answer choice that fills in the blank based on the context of the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_does_underscore_refer_to",
        "instruction": "Given a sentence with a blank, determine whether the blank refers to the first or second person mentioned in the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct",
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_does_underscore_refer_to_score_eval",
        "instruction": "Given an input sentence with a blank, determine whether the blank refers to the first or second person mentioned in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_does_underscore_refer_to_score_eval",
        "instruction": "Given an input sentence with a blank, fill in the blank with the correct person mentioned in the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_fill_in_the_blank",
        "instruction": "Fill in the blank in the sentence provided with the correct answer choice."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_fill_in_the_blank",
        "instruction": "Create a sentence with a blank to be filled in with a given answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_fill_in_the_blank_score_eval",
        "instruction": "Fill in the blank in the given sentence with the correct answer from the given choices. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_stand_for",
        "instruction": "Given the sentence and the answer choices, please select the correct answer choice that stands for the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_underscore_refer_to",
        "instruction": "Given the inputs and answer choices, please select the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_underscore_refer_to_score_eval",
        "instruction": "Given the inputs_pretokenized and targets_pretokenized, determine whether is_correct is True or False. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_underscore_refer_to_score_eval",
        "instruction": "Given the inputs_pretokenized and is_correct, write the targets_pretokenized."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_does_the_supposed_perturbation_have_an_effect",
        "instruction": "Given a perturbation hypothesis and a process, determine if the supposed perturbation has an effect (direct or indirect) on the process. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_what_is_the_final_step_of_the_following_process",
        "instruction": "What is the final step of the following process?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_what_might_be_the_first_step_of_the_process",
        "instruction": "Given a process, please select the correct order of steps."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_what_might_be_the_first_step_of_the_process",
        "instruction": "Given the first step of a process, please select the correct process."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_what_might_be_the_last_step_of_the_process",
        "instruction": "Given a process, please select the correct order of steps."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_what_might_be_the_last_step_of_the_process",
        "instruction": "Given the last step of a process, please select the correct order of steps."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_which_of_the_following_is_the_supposed_perturbation",
        "instruction": "Given a process and a perturbation, identify whether the perturbation directly impacts a step of the process, indirectly impacts a step of the process, or does not impact any step of the process."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_DOC_how_would_you_rephrase_few_words",
        "instruction": "Given the inputs, please generate a summary of the article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_DOC_how_would_you_rephrase_few_words",
        "instruction": "Given the targets, please generate the inputs."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_DOC_how_would_you_rephrase_few_words",
        "instruction": "Given the inputs, please generate a headline for the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_DOC_tldr",
        "instruction": "Given the inputs, write a summary of the article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_DOC_tldr",
        "instruction": "Given the summary, write the inputs."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_DOC_write_summary_of_above",
        "instruction": "Given the inputs, write a summary of the text above."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_article_DOC_summary",
        "instruction": "Given the article, write a summary of the main points."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_college_roommate_asked_DOC_so_I_recap",
        "instruction": "Given the article, please write a summary in layman's terms."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_college_roommate_asked_DOC_so_I_recap",
        "instruction": "Given the article, please answer the question asked by the college roommate."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_college_roommate_asked_DOC_so_I_recap",
        "instruction": "Given the article, please identify the reason for the attacks on the office."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_read_below_DOC_write_abstract",
        "instruction": "Given the inputs, please write a summary of the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_summarize_DOC",
        "instruction": "Given the inputs, please generate a target template that maps a data example into natural language for the target sequence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_summarize_this_DOC_summary",
        "instruction": "Summarize the given document."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_summarize_this_DOC_summary",
        "instruction": "Given a document, write a headline that summarizes it."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_summarize_this_DOC_summary",
        "instruction": "Given a headline, write a document that summarizes it."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_based_on_that",
        "instruction": "Given the inputs_pretokenized and answer_choices, predict the targets_pretokenized. Answers must be one of 2 stars, 4 stars, 3 stars, 1 star, 5 stars."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_format_rating",
        "instruction": "Given the review text, predict the rating of the review. Answers must be one of 2 stars, 4 stars, 3 stars, 1 star, 5 stars."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_format_score",
        "instruction": "Given the review text, predict the review score between 1 and 5. Answers must be one of 3, 4, 5, 2, 1."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_format_score",
        "instruction": "Given the review score, generate the review text."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_format_star",
        "instruction": "Given the review text, predict the number of stars given by the reviewer. Answers must be one of 2 stars, 4 stars, 3 stars, 1 star, 5 stars."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_so_i_would",
        "instruction": "Given the inputs, predict the number of stars the reviewer would give. Answers must be one of 2 stars, 4 stars, 3 stars, 1 star, 5 stars."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_so_i_would",
        "instruction": "Given the inputs, predict the sentiment of the review (positive, negative, or neutral). Answers must be one of 2 stars, 4 stars, 3 stars, 1 star, 5 stars."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_so_i_would",
        "instruction": "Given the inputs, predict the number of hours it would take to get a response from the doctor's office. Answers must be one of 2 stars, 4 stars, 3 stars, 1 star, 5 stars."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_this_place",
        "instruction": "Given the inputs, please rate the place from 1 to 5 stars. Answers must be one of 2 stars, 4 stars, 3 stars, 1 star, 5 stars."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_this_place",
        "instruction": "Given the inputs, please predict the sentiment of the review (positive or negative). Answers must be one of 2 stars, 4 stars, 3 stars, 1 star, 5 stars."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "argilla/news-summary",
        "instruction": "Given a news article, generate a headline that summarizes the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "squad",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "squad",
        "instruction": "Given a question, can you provide a context where the question can be answered?"
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "anli",
        "instruction": "Given a premise and a label, please provide a hypothesis that is entailment, contradiction, or neutral to the premise."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "adversarial_qa-adversarialQA",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "title"
        ],
        "task_name": "adversarial_qa-adversarialQA",
        "instruction": "Can you write a title for the passage?"
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "adversarial_qa-droberta",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "plot"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "duorc-SelfRC",
        "instruction": "Given a plot, please write a question based on the plot."
    },
    {
        "input_fields": [
            "plot"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "duorc-ParaphraseRC",
        "instruction": "Given a plot, please write a question based on the plot."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-coordinate_structure_constraint_complex_left_branch",
        "instruction": "Given a sentence with a complex left branch, generate a corrected version of the sentence."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-determiner_noun_agreement_1",
        "instruction": "Given a sentence with a grammatical error in determiner-noun agreement, correct the error."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-ellipsis_n_bar_1",
        "instruction": "Given a sentence with ellipsis, please fill in the missing words."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-irregular_past_participle_verbs",
        "instruction": "Given a sentence with an irregular past participle verb, please provide the correct form of the verb."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-superlative_quantifiers_1",
        "instruction": "Given a sentence with a superlative quantifier, correct the sentence if it is bad."
    },
    {
        "input_fields": [
            "sentence_good"
        ],
        "output_field": [
            "sentence_bad"
        ],
        "task_name": "blimp-transitive",
        "instruction": "Given a sentence with a transitive verb, please replace the verb with a synonym that still makes sense in the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_only_question_answer",
        "instruction": "Given the answer choices and the question, select the correct answer."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-dbpedia_14_given_a_choice_of_categories_",
        "instruction": "Can you write a text that refers to the given answer choice?"
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answerKey"
        ],
        "task_name": "ai2_arc-ARC-Challenge",
        "instruction": "Given a question, please select the correct answer from the options. Answers must be one of A, D, B, C, 2."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answerKey"
        ],
        "task_name": "ai2_arc-ARC-Easy",
        "instruction": "Given a question and answer choices, please select the correct answer. Answers must be one of A, D, B, 3, C, 1."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answerKey"
        ],
        "task_name": "ai2_arc-ARC-Easy",
        "instruction": "Given a question and answer choices, please select the incorrect answer. Answers must be one of A, D, B, 3, C, 1."
    },
    {
        "input_fields": [
            "context",
            "question"
        ],
        "output_field": [
            "answer0",
            "answer1",
            "answer2",
            "answer3",
            "label"
        ],
        "task_name": "cosmos_qa",
        "instruction": "Given the context and the question, please select the correct answer from the four choices."
    },
    {
        "input_fields": [
            "context",
            "answer0",
            "answer1",
            "answer2",
            "answer3"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "cosmos_qa",
        "instruction": "Can you write a question based on the context and the answer choices?"
    },
    {
        "input_fields": [
            "context",
            "question"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "cosmos_qa",
        "instruction": "Can you predict the label based on the context and the question? Answers must be one of 2, 0, 3, 1."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "quail",
        "instruction": "Can you write a question based on the given context?"
    },
    {
        "input_fields": [
            "content"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "amazon_polarity",
        "instruction": "Given a review, predict the sentiment of the review (positive or negative). Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "question",
            "supports"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "wiki_hop-original",
        "instruction": "Given a question and a set of supports, please provide the answer."
    },
    {
        "input_fields": [
            "supports",
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "wiki_hop-masked",
        "instruction": "Fill in the blank in the given sentence with the correct word based on the supports."
    },
    {
        "input_fields": [
            "supports",
            "answer"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "wiki_hop-masked",
        "instruction": "Can you write a question based on the supports and answer?"
    },
    {
        "input_fields": [
            "concepts"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "common_gen",
        "instruction": "Given a set of common concepts, generate a coherent sentence describing an everyday scenario using these concepts."
    },
    {
        "input_fields": [
            "document"
        ],
        "output_field": [
            "summary"
        ],
        "task_name": "gigaword",
        "instruction": "Given a document, generate a summary of the article."
    },
    {
        "input_fields": [
            "summary"
        ],
        "output_field": [
            "document"
        ],
        "task_name": "gigaword",
        "instruction": "Given a summary, generate a document that summarizes the article."
    },
    {
        "input_fields": [
            "question",
            "support",
            "distractor3",
            "distractor1",
            "distractor2"
        ],
        "output_field": [
            "correct_answer"
        ],
        "task_name": "sciq",
        "instruction": "Given a question and its supporting evidence, select the correct answer from four options."
    },
    {
        "input_fields": [
            "question",
            "support",
            "correct_answer"
        ],
        "output_field": [
            "distractor1",
            "distractor2",
            "distractor3"
        ],
        "task_name": "sciq",
        "instruction": "Given a question and its supporting evidence, select the incorrect answer from four options."
    },
    {
        "input_fields": [
            "fact1",
            "fact2",
            "formatted_question"
        ],
        "output_field": [
            "answerKey"
        ],
        "task_name": "qasc",
        "instruction": "Given a fact and a question, select the correct answer from multiple choices. Answers must be one of H, A, C, F, B, D, E, G."
    },
    {
        "input_fields": [
            "question",
            "para"
        ],
        "output_field": [
            "answerKey"
        ],
        "task_name": "quartz",
        "instruction": "Given a question and a paragraph, select the correct answer from multiple choices. Answers must be one of A, B."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "timestamp",
            "url"
        ],
        "task_name": "c4-en",
        "instruction": "Given a text, extract the timestamp and the URL."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "timestamp"
        ],
        "task_name": "c4-realnewslike",
        "instruction": "Given a text, can you predict the timestamp?"
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "c4-realnewslike",
        "instruction": "Given a text, can you predict the URL?"
    },
    {
        "input_fields": [
            "timestamp"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "c4-realnewslike",
        "instruction": "Given a timestamp, can you predict the URL?"
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "timestamp"
        ],
        "task_name": "c4-en.noblocklist",
        "instruction": "Given a text, can you extract the date and time of an event mentioned in the text?"
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "c4-en.noblocklist",
        "instruction": "Given a text, can you extract the URL mentioned in the text?"
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "title",
            "content"
        ],
        "task_name": "dbpedia_14",
        "instruction": "Given a label, please provide the corresponding title and content."
    },
    {
        "input_fields": [
            "answer"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "gsm8k-main",
        "instruction": "Create a new question based on the given answer."
    },
    {
        "input_fields": [
            "answer"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "gsm8k-socratic",
        "instruction": "Create a question provided the answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gaming",
        "instruction": "Given a question, please provide the corresponding top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gaming",
        "instruction": "Given a question, please provide the corresponding downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gaming",
        "instruction": "Given a question, please provide the corresponding top-rated and downvoted answers."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-askubuntu",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-askubuntu",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body",
            "upvoted_answer"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-hermeneutics",
        "instruction": "Given a question and its corresponding upvoted answer, can you generate a downvoted answer?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-judaism",
        "instruction": "Given a question, please provide the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-judaism",
        "instruction": "Given a question, please provide the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-judaism",
        "instruction": "Given a question, please provide the top rated and downvoted answers."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-diy",
        "instruction": "Given a question, please provide the top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-diy",
        "instruction": "Given a question, please provide the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-diy",
        "instruction": "Given a question, please provide the top-rated and downvoted answers."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-history",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-history",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-history",
        "instruction": "Given a question, please provide the top rated and downvoted answers from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-drupal",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-drupal",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-chemistry",
        "instruction": "Given a question, please provide the corresponding top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-chemistry",
        "instruction": "Given a question, please provide the corresponding downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-mathoverflow",
        "instruction": "Given a question, please provide the corresponding top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-mathoverflow",
        "instruction": "Given a question, please provide the corresponding downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-buddhism",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-buddhism",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-buddhism",
        "instruction": "Given an answer, please provide the corresponding question from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gis",
        "instruction": "Given a question, please provide the corresponding top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gis",
        "instruction": "Given a question, please provide the corresponding downvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gis",
        "instruction": "Given an answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-graphicdesign",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-graphicdesign",
        "instruction": "Given a title and body of a question, predict the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-aviation",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-aviation",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-japanese",
        "instruction": "Given a question in Japanese, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-cs",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-cs",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-german",
        "instruction": "Given a German expression, please provide an English equivalent."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-german",
        "instruction": "Please provide a German expression that is equivalent to the English expression \"for all I know\"."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-german",
        "instruction": "Please provide a reason for the origin of the German expression \"Du hast ja einen Vogel!\""
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-biology",
        "instruction": "Given a question, please provide the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-biology",
        "instruction": "Given a question, please provide the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-bitcoin",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-bitcoin",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-bitcoin",
        "instruction": "Given an answer, please provide the corresponding question from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-blender",
        "instruction": "Given a question, please provide the corresponding top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-blender",
        "instruction": "Given a question, please provide the corresponding downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-anime",
        "instruction": "Given a question about manga, please provide an answer based on the information provided in the title and body of the post."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-anime",
        "instruction": "Given a question about One Piece, please provide an answer based on the information provided in the title and body of the post."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-boardgames",
        "instruction": "Given a question, please provide the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-boardgames",
        "instruction": "Given a question, please provide the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-hinduism",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-hinduism",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-economics",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-economics",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-astronomy",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-astronomy",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-astronomy",
        "instruction": "Given a question, provide the top rated and downvoted answers from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-arduino",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-arduino",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-chess",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-chess",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-cstheory",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-cstheory",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-ja",
        "instruction": "Given a title and body of a question, generate an answer that is likely to be upvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-ja",
        "instruction": "Given a title and body of a question, generate an answer that is likely to be downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-ethereum",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-ethereum",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-ethereum",
        "instruction": "Given a question, provide the top rated and downvoted answers from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-health",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-health",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-literature",
        "instruction": "Given a question, please provide the top-rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-literature",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-literature",
        "instruction": "Given a question, please provide the top-rated and downvoted answers from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-lifehacks",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-lifehacks",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-engineering",
        "instruction": "Given a question, please provide the top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-engineering",
        "instruction": "Given a question, please provide the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-engineering",
        "instruction": "Given a question, please provide the top-rated and downvoted answers."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-ham",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-ham",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-italian",
        "instruction": "Please identify the top rated answer for each question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-italian",
        "instruction": "Please identify the downvoted answer for each question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-avp",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-avp",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-hsm",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-hsm",
        "instruction": "Given an answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-expressionengine",
        "instruction": "Given a question, please provide the top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-expressionengine",
        "instruction": "Given a question, please provide the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-expressionengine",
        "instruction": "Given a question, please provide the top-rated and downvoted answers."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-joomla",
        "instruction": "Given a title and body of a question, provide the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-joomla",
        "instruction": "Given a title and body of a question, provide the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-crafts",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-crafts",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-latin",
        "instruction": "Given a question, generate a top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-hardwarerecs",
        "instruction": "Given a title and body of a question, predict the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-hardwarerecs",
        "instruction": "Given a title and body of a question, predict the corresponding downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-coffee",
        "instruction": "Given a question, generate its top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-coffee",
        "instruction": "Given a question, generate a downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-ebooks",
        "instruction": "Given a title and a question, provide the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-ebooks",
        "instruction": "Given a title and a question, provide the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-bricks",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-bricks",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-civicrm",
        "instruction": "Given a question, please provide the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-civicrm",
        "instruction": "Given a question, please provide the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-civicrm",
        "instruction": "Given a question, please provide the top rated answer and the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-esperanto",
        "instruction": "Given a question in Esperanto, provide the top rated answer in Esperanto."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-esperanto",
        "instruction": "Given a question in Esperanto, provide the downvoted answer in Esperanto."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-computergraphics",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-computergraphics",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-computergraphics",
        "instruction": "Given a question, please provide the top rated answer and the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-conlang",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-conlang",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-iot",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-iot",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-drones",
        "instruction": "Given a question, please provide the top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-drones",
        "instruction": "Given a question, please provide the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-cardano",
        "instruction": "Given a question, please provide the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-cardano",
        "instruction": "Given a question, please provide the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-softwareengineering",
        "instruction": "Given a question, provide the top-rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-softwareengineering",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-scifi",
        "instruction": "Given a question, please provide the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-scifi",
        "instruction": "Given a question, please provide the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-workplace",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-workplace",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-serverfault",
        "instruction": "Given a question, provide the corresponding top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-serverfault",
        "instruction": "Given a question, provide the corresponding downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-rpg",
        "instruction": "Given a question, please provide the top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-rpg",
        "instruction": "Given a question, please provide the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-physics",
        "instruction": "Given a question, please provide the top-rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-physics",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-physics",
        "instruction": "Given a question, please provide the top-rated answer and the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-superuser",
        "instruction": "Given a title and body of a question, predict the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-superuser",
        "instruction": "Given a title and body of a question, predict the least rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-pt",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-pt",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-unix",
        "instruction": "Given a question, please provide the corresponding top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-unix",
        "instruction": "Given a question, please provide the corresponding downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-unix",
        "instruction": "Given a question, please provide the corresponding top-rated and downvoted answers."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-stats",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-stats",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-movies",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-movies",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-movies",
        "instruction": "Given an answer, provide the corresponding question from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-photo",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-photo",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-photo",
        "instruction": "Given a question, please provide the top rated answer and the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-skeptics",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-skeptics",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-money",
        "instruction": "Given a question, please provide the top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-money",
        "instruction": "Given a question, please provide the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-salesforce",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-salesforce",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-salesforce",
        "instruction": "Given a question, please provide the top rated and downvoted answers from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-travel",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-travel",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-tex",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-tex",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-tex",
        "instruction": "Given a question, provide the top rated and downvoted answers from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-sharepoint",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-sharepoint",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-sharepoint",
        "instruction": "Given a question, provide the top rated and downvoted answers from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-webapps",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-webapps",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-networkengineering",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-networkengineering",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-sports",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-sports",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-rus",
        "instruction": "Given a title and a body, generate a new answer that can be upvoted on Stack Exchange."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-space",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-space",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-space",
        "instruction": "Given an answer, please provide the corresponding question from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-pm",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-pm",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-pm",
        "instruction": "Given a question, please provide the top rated and downvoted answers from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-spanish",
        "instruction": "Given a question in Spanish, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-quant",
        "instruction": "Given a title and body of a question, predict the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-quant",
        "instruction": "Given a title and body of a question, predict the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-sqa",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-sqa",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-sqa",
        "instruction": "Given a question, please provide the top rated answer and the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-outdoors",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-outdoors",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-outdoors",
        "instruction": "Given a question, please provide the top rated answer and the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-softwarerecs",
        "instruction": "Given a question, provide the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-softwarerecs",
        "instruction": "Given a question, provide the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-mythology",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-mythology",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-scicomp",
        "instruction": "Given a question, please provide the top-rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-scicomp",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-scicomp",
        "instruction": "Given a question, please provide the top-rated and downvoted answers from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-sustainability",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-sustainability",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-sustainability",
        "instruction": "Given a question, provide a list of possible solutions from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-poker",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-poker",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-poker",
        "instruction": "Given a question, please provide the top rated answer and the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-tor",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-tor",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-tor",
        "instruction": "Given a question, provide the top rated and downvoted answers from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-vi",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-vi",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-vi",
        "instruction": "Given an answer, provide the corresponding question from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-quantumcomputing",
        "instruction": "Given a question, please provide the top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-quantumcomputing",
        "instruction": "Given a question, please provide the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-musicfans",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-musicfans",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-tridion",
        "instruction": "Given a title and body of a question, predict whether the top rated answer will be upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-opendata",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-opendata",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-tezos",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-tezos",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-or",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-or",
        "instruction": "Given a question, please provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-or",
        "instruction": "Given a question, please provide the top rated and downvoted answers from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-stackapps",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-stackapps",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-apple",
        "instruction": "Given a question, please provide the corresponding top-rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-apple",
        "instruction": "Given an upvoted answer, please provide the corresponding question from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-dba",
        "instruction": "Given a question, provide the corresponding top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-mathoverflow",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-electronics",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-electronics",
        "instruction": "Given an upvoted answer, predict the question it answers."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-mathematica",
        "instruction": "Given a question, please provide the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-drupal",
        "instruction": "Given a question, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-drupal",
        "instruction": "Given a question, provide a solution or recommendation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-drupal",
        "instruction": "Given a question, provide a list of modules or functions that can be used to solve the problem."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-magento",
        "instruction": "Given a question, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-magento",
        "instruction": "Given an upvoted answer, provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-gaming",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-gaming",
        "instruction": "Given an upvoted answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-gamedev",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-gis",
        "instruction": "Given a question, please provide the corresponding top-rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-gis",
        "instruction": "Given an answer, please predict the corresponding question from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-askubuntu",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-diy",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-cs",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-chemistry",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-judaism",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-crypto",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-crypto",
        "instruction": "Given an upvoted answer, please provide the corresponding question from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-android",
        "instruction": "Given a question, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-christianity",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-graphicdesign",
        "instruction": "Provide a critique and advice for a given logo design."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-aviation",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-aviation",
        "instruction": "Given an upvoted answer, please predict the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ethereum",
        "instruction": "Given the code snippet, please identify the error and provide a solution."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ethereum",
        "instruction": "Can you provide a warning or error message that may occur when running the code snippet?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ethereum",
        "instruction": "Can you provide a reason why inline assembly should be avoided?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-law",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-hermeneutics",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-bicycles",
        "instruction": "Given a question, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-bicycles",
        "instruction": "Given an upvoted answer, provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-arduino",
        "instruction": "Given the code, can you identify the error and suggest a fix?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-history",
        "instruction": "Given a question, provide the corresponding top-rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-cooking",
        "instruction": "Given a question, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-cooking",
        "instruction": "Given an upvoted answer, provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-hinduism",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-hinduism",
        "instruction": "Given an upvoted answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-boardgames",
        "instruction": "Given a question, please provide the corresponding top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-emacs",
        "instruction": "Given a title and body of a post, predict the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-gardening",
        "instruction": "Given a question, provide the corresponding top-rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-astronomy",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-islam",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-german",
        "instruction": "Given a sentence in German, please provide its English translation."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-german",
        "instruction": "Given a sentence in English, please provide its German translation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-german",
        "instruction": "Given a sentence in German, please provide a paraphrased version in German."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-fitness",
        "instruction": "Given a question about ice skating, please provide the corresponding answer from the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-fitness",
        "instruction": "Given a question about fitness, please provide the corresponding answer from the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-craftcms",
        "instruction": "Given a title and body of a question, output the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-buddhism",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ai",
        "instruction": "Given the stock data with features such as stocks' closing prices, national interest rate, and unemployment rate, predict the stock prices for the next day."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ai",
        "instruction": "Given a graph with weighted edges, implement a graph convolution network (GCN) using an open-source implementation."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ai",
        "instruction": "Given a set of numeric variables, select the relevant predictor variables that potentially have some relationship with the response variable."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-expressionengine",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-expressionengine",
        "instruction": "Given an upvoted answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-cogsci",
        "instruction": "Given a question, provide the corresponding top-rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-civicrm",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-literature",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-health",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-health",
        "instruction": "Given an answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-avp",
        "instruction": "Given a question, provide the corresponding top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-earthscience",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-earthscience",
        "instruction": "Given an answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-joomla",
        "instruction": "Given a question, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-joomla",
        "instruction": "Given an upvoted answer, provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-homebrew",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-expatriates",
        "instruction": "Given a question about unemployment benefits in European states, please provide the answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-expatriates",
        "instruction": "Given a question about safety in Cape Town, please provide the answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-latin",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-latin",
        "instruction": "Given an upvoted answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-matheducators",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ham",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ham",
        "instruction": "Given an upvoted answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-genealogy",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-3dprinting",
        "instruction": "Given a question, please provide the corresponding top-rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-elementaryos",
        "instruction": "Given a question, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-elementaryos",
        "instruction": "Given an upvoted answer, provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-bioinformatics",
        "instruction": "Given the title and body of a Stack Exchange post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-bioinformatics",
        "instruction": "Given the upvoted answer of a Stack Exchange post, predict the title and body of the post."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-hsm",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-computergraphics",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-computergraphics",
        "instruction": "Given an upvoted answer, can you predict the question it answers?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-bricks",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-freelancing",
        "instruction": "Given a question, provide the corresponding top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-crafts",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-hardwarerecs",
        "instruction": "Given a question, provide the corresponding top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-iot",
        "instruction": "Given a question, provide the corresponding top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-eosio",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-eosio",
        "instruction": "Given an answer, provide the corresponding question from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-languagelearning",
        "instruction": "Given a title and body of a post, can you predict the upvoted answer?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-korean",
        "instruction": "Given a title and body of a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-coffee",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-beer",
        "instruction": "Given a title and body of a question, predict whether the answer is correct or not."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-iota",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-cardano",
        "instruction": "Given a question about Cardano network, please provide an answer based on the information provided in the title and body."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-drones",
        "instruction": "Given a question, provide the corresponding top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-drones",
        "instruction": "Given an upvoted answer, predict the corresponding question from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-conlang",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-physics",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-tex",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-tex",
        "instruction": "Given an equation, remove a strange Greek letter that appears in it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-serverfault",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-serverfault",
        "instruction": "Given an upvoted answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-serverfault",
        "instruction": "Given a title and a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-salesforce",
        "instruction": "Given a question, provide the corresponding top rated answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-salesforce",
        "instruction": "Given an answer, predict the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-wordpress",
        "instruction": "Given a page template name, return the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-softwareengineering",
        "instruction": "Given a question, provide the corresponding top-rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-softwareengineering",
        "instruction": "Given an answer, predict the corresponding question from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-scifi",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-security",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ru",
        "instruction": "Given a question, please provide the corresponding top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-superuser",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-superuser",
        "instruction": "Given an upvoted answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-sharepoint",
        "instruction": "Given a question, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-rpg",
        "instruction": "Given a question, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-rpg",
        "instruction": "Given an upvoted answer, provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-travel",
        "instruction": "Given a question, generate a valid answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-worldbuilding",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-worldbuilding",
        "instruction": "Given an answer, predict the question it belongs to."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-meta",
        "instruction": "Given a question, provide the corresponding top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-workplace",
        "instruction": "Given a question, please provide the corresponding top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ux",
        "instruction": "Given a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-money",
        "instruction": "Given a question, provide the corresponding top rated answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-money",
        "instruction": "Given an answer, predict the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-webmasters",
        "instruction": "Given a title and body of a question, generate a top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-raspberrypi",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-photo",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-music",
        "instruction": "Given a question, provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-philosophy",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-philosophy",
        "instruction": "Given an upvoted answer, please provide the question it answers."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-movies",
        "instruction": "Given a title and a body of a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-quant",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-politics",
        "instruction": "Given a question, provide the corresponding top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-politics",
        "instruction": "Given a top rated answer, provide the corresponding question from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-space",
        "instruction": "Given a question, provide the corresponding upvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-space",
        "instruction": "Given an upvoted answer, provide the corresponding question from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-rus",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-rus",
        "instruction": "Given an answer, provide the corresponding question from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-writers",
        "instruction": "Given a title and body of a question, predict the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-webapps",
        "instruction": "Given a question, provide the corresponding top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-networkengineering",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-parenting",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-parenting",
        "instruction": "Given an answer, predict the question it belongs to."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-scicomp",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-sqa",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-sitecore",
        "instruction": "Given a question, provide the corresponding top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-sitecore",
        "instruction": "Given an answer, provide the corresponding question from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-vi",
        "instruction": "Given a title and body of a question, predict the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-pm",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-pets",
        "instruction": "Given a question, provide the corresponding top-rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-sound",
        "instruction": "Given a title and body of a post, predict the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-reverseengineering",
        "instruction": "Given a question, please provide the corresponding answer from the upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-reverseengineering",
        "instruction": "Given a question, please provide the corresponding answer from the title_body."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-outdoors",
        "instruction": "Given a question, please provide the corresponding top-rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-tridion",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-retrocomputing",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-robotics",
        "instruction": "Given a question, please provide the corresponding top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-quantumcomputing",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-quantumcomputing",
        "instruction": "Given an answer, please predict the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-sports",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-sports",
        "instruction": "Given an upvoted answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-russian",
        "instruction": "Given a question in Russian, please provide the corresponding answer in Russian."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-opensource",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-opensource",
        "instruction": "Given an upvoted answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-woodworking",
        "instruction": "Given a question, provide the corresponding top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-opendata",
        "instruction": "Given a question, provide the corresponding top-rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-sustainability",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-portuguese",
        "instruction": "Given a question, provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-portuguese",
        "instruction": "Given a phrase, provide its meaning."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-portuguese",
        "instruction": "Given a phrase, provide its origin."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-mythology",
        "instruction": "Given a question, please provide the corresponding top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-musicfans",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-or",
        "instruction": "Given a title and body of a question, predict the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-poker",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-windowsphone",
        "instruction": "Given a question, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-windowsphone",
        "instruction": "Given an upvoted answer, provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-moderators",
        "instruction": "Given a question/answers website, how can we keep users engaged in the initial stage when there are only a few users?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-moderators",
        "instruction": "Given a question/answers website, how can we advertise as much as we can and prime the pump with enough questions and answers until we have enough users to start reliably creating enough user-generated content?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-moderators",
        "instruction": "Given a question/answers website, how can we recruit a cadre of subject matter experts who are in fact trying to solve problems in our topic area?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-stackapps",
        "instruction": "Given a question, please provide the corresponding top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-stellar",
        "instruction": "Given a title and body of a question, predict the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-askubuntu",
        "instruction": "Given a title or body of a question, predict the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-askubuntu",
        "instruction": "Given an upvoted answer, predict the corresponding title or body of the question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-electronics",
        "instruction": "Given a title and a body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-gamedev",
        "instruction": "Given a title and a body of a post, predict the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-mathematica",
        "instruction": "Given a title or body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ell",
        "instruction": "Given a title or a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ell",
        "instruction": "Given an upvoted answer, please provide the corresponding title or question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-judaism",
        "instruction": "Given a title or body of a question, please provide the upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-judaism",
        "instruction": "Given an upvoted answer, please provide the title or body of the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-drupal",
        "instruction": "Given a title or body of a question, generate the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-drupal",
        "instruction": "Given an upvoted answer, generate the corresponding title or body of a question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-christianity",
        "instruction": "Given a question, please provide the corresponding top-rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-academia",
        "instruction": "Given a title and a body, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-chemistry",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-chemistry",
        "instruction": "Given an upvoted answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-aviation",
        "instruction": "Given a title and a body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-history",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-history",
        "instruction": "Given an answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-japanese",
        "instruction": "Given a phrase, please provide a Japanese word that means \"overrated\"."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-japanese",
        "instruction": "What is the usage of \u305d\u3053\u306f\u305d\u308c?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-japanese",
        "instruction": "Can you provide a Japanese word that means \"underrated\"?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-cooking",
        "instruction": "Given a title or a question, generate the corresponding answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-cooking",
        "instruction": "Given an answer, generate the corresponding title or question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-hermeneutics",
        "instruction": "Given a title or a question, please generate the corresponding answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-hermeneutics",
        "instruction": "Given an answer, please generate the corresponding title or question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-hinduism",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-hinduism",
        "instruction": "Given an answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-graphicdesign",
        "instruction": "Given a question, please provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-graphicdesign",
        "instruction": "Given an answer, can you predict the question it belongs to?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ja",
        "instruction": "Given a title and a question, please provide the best voted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-islam",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-islam",
        "instruction": "Given an answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-codegolf",
        "instruction": "Given a title, output the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-codegolf",
        "instruction": "Given an upvoted answer, output the corresponding title."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-boardgames",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-boardgames",
        "instruction": "Given an upvoted answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-emacs",
        "instruction": "Given a title or body of a question, predict the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-astronomy",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-astronomy",
        "instruction": "Given an upvoted answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-cstheory",
        "instruction": "Given a title or body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-cstheory",
        "instruction": "Given an upvoted answer, predict the title or body of the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-engineering",
        "instruction": "Given a title or body of a question, predict the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-cogsci",
        "instruction": "Given a question, predict the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-french",
        "instruction": "Provide a definition for the given French word."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-french",
        "instruction": "Can you write a question based on the given French sentence?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ai",
        "instruction": "Given a title and a body of a post, predict the most upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-chess",
        "instruction": "Given a question, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-chess",
        "instruction": "Given an upvoted answer, provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-earthscience",
        "instruction": "Given a title or a body of a question, can you predict the upvoted answer?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-joomla",
        "instruction": "Given a title, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-joomla",
        "instruction": "Given an upvoted answer, please provide the corresponding title."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-homebrew",
        "instruction": "Given the title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-latin",
        "instruction": "Given a title or a question, generate the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-latin",
        "instruction": "Given an upvoted answer, generate the corresponding title or question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-hsm",
        "instruction": "Given a title or a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-expatriates",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-expatriates",
        "instruction": "Given an upvoted answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-matheducators",
        "instruction": "Given a title or body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-genealogy",
        "instruction": "Given a title or title and body of a genealogy question, can you predict the upvoted answer?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-computergraphics",
        "instruction": "Given a title and a body, can you predict the upvoted answer?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-martialarts",
        "instruction": "Given a title or a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-lifehacks",
        "instruction": "Given a question, provide the corresponding top rated answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-lifehacks",
        "instruction": "Given an upvoted answer, predict the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-italian",
        "instruction": "Given a sentence, please provide its English translation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-bricks",
        "instruction": "Given the title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-eosio",
        "instruction": "Given a title or body of a post, generate the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ebooks",
        "instruction": "Given a title and a body, generate a new upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-esperanto",
        "instruction": "Given a sentence in Esperanto, please provide its English translation."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-esperanto",
        "instruction": "Given an English sentence, please provide its Esperanto translation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-esperanto",
        "instruction": "Given a sentence in Esperanto, please provide a word or phrase that has a similar meaning in English."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-meta",
        "instruction": "Given a title, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-meta",
        "instruction": "Given an upvoted answer, provide the corresponding title."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-photo",
        "instruction": "Given a question, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-photo",
        "instruction": "Given an upvoted answer, provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-workplace",
        "instruction": "Given a title and a body of a question, predict the upvoted answer to the question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-music",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-movies",
        "instruction": "Given a title or a question, generate the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-movies",
        "instruction": "Given an upvoted answer, predict the corresponding title or question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-webapps",
        "instruction": "Given a title or body of a question, return the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-mechanics",
        "instruction": "Given a title or body of a question, generate the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-parenting",
        "instruction": "Given a question, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-parenting",
        "instruction": "Given an upvoted answer, provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-spanish",
        "instruction": "Given a question in Spanish, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-pets",
        "instruction": "Given a title and a body, predict the best upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-outdoors",
        "instruction": "Given a title and a body, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-retrocomputing",
        "instruction": "Given a question, provide the corresponding top-rated answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-retrocomputing",
        "instruction": "Given an answer, predict the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-quantumcomputing",
        "instruction": "Given a question, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-quantumcomputing",
        "instruction": "Given an upvoted answer, predict the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-sports",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-sports",
        "instruction": "Given an upvoted answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-opensource",
        "instruction": "Given a title or a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-opensource",
        "instruction": "Given an upvoted answer, please provide the corresponding title or question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ukrainian",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-opendata",
        "instruction": "Given a title or body of a post, predict the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-mythology",
        "instruction": "Given a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-portuguese",
        "instruction": "Given a title or a question, generate the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-portuguese",
        "instruction": "Given an upvoted answer, generate the corresponding title or question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-monero",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-monero",
        "instruction": "Given an answer, please provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-sustainability",
        "instruction": "Given a title, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-musicfans",
        "instruction": "Given a title or a body of a post, predict the top rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-or",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-stackapps",
        "instruction": "Given a question, retrieve the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-vegetarianism",
        "instruction": "Given a title or a question, please provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-stellar",
        "instruction": "Given a title and a body, predict whether the transaction was successful or not."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-stellar",
        "instruction": "Given a title and a body, predict whether the payment was made to an inactive account or not."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-pt",
        "instruction": "Given a title and a body, predict the upvoted answer."
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "financial_phrasebank-sentences_66agree",
        "instruction": "Can you write a sentence with the opposite sentiment?"
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "financial_phrasebank-sentences_50agree",
        "instruction": "Can you write a sentence with the opposite sentiment?"
    },
    {
        "input_fields": [
            "sentence_A",
            "sentence_B"
        ],
        "output_field": [
            "relatedness_score"
        ],
        "task_name": "sick",
        "instruction": "Given two sentences, please rate their relatedness in meaning on a scale of 1 to 5."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "snli",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "snli",
        "instruction": "This task asks models to generate a new hypothesis given a premise and a label."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "snli",
        "instruction": "This task asks models to generate a new premise given a hypothesis and a label."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-android",
        "instruction": "Given a question, provide the corresponding top-rated answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-android",
        "instruction": "Given a question, provide the corresponding downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-android",
        "instruction": "Given a question, provide the corresponding top-rated and downvoted answers."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-mechanics",
        "instruction": "Given a title and a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-mechanics",
        "instruction": "Given a title and a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-windowsphone",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-windowsphone",
        "instruction": "Given a question, provide the downvoted answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-academia",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-blender",
        "instruction": "Given a question, provide the top rated answer from Stack Exchange network."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-simple",
        "instruction": "Given a command, generate the corresponding action sequence."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-simple",
        "instruction": "Given an action sequence, generate the corresponding command."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-addprim_jump",
        "instruction": "Given a command, output the corresponding action."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-addprim_jump",
        "instruction": "Given an action, output the corresponding command."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-addprim_turn_left",
        "instruction": "Given a command, output the corresponding action."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-addprim_turn_left",
        "instruction": "Given a sequence of actions, output the corresponding command."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-filler_num0",
        "instruction": "Given a sequence of commands, generate the corresponding sequence of actions."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-filler_num0",
        "instruction": "Given a sequence of actions, generate the corresponding sequence of commands."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-filler_num1",
        "instruction": "Given a sequence of commands, generate the corresponding sequence of actions."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-filler_num1",
        "instruction": "Given a sequence of actions, generate the corresponding sequence of commands."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-filler_num2",
        "instruction": "Given a sequence of actions, generate the corresponding commands."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-filler_num2",
        "instruction": "Given a sequence of commands, generate the corresponding actions."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-length",
        "instruction": "Given a command, output the corresponding action."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-length",
        "instruction": "Given an action, output the corresponding command."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-template_around_right",
        "instruction": "Given a sequence of commands, generate the corresponding sequence of actions, but with all \"opposite right\" commands replaced by \"opposite left\" commands and vice versa."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-template_jump_around_right",
        "instruction": "Given a sequence of commands, generate the corresponding sequence of actions, but with all \"opposite\" commands reversed."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-template_opposite_right",
        "instruction": "Please generate a command that involves turning right and then turning left."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-template_opposite_right",
        "instruction": "Please generate a command that involves turning left and then turning right."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-template_opposite_right",
        "instruction": "Please generate a command that involves running and then looking around left."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "multi_nli",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "multi_nli",
        "instruction": "Can you generate a new hypothesis given the premise?"
    },
    {
        "input_fields": [
            "question",
            "question_concept"
        ],
        "output_field": [
            "answerKey"
        ],
        "task_name": "commonsense_qa",
        "instruction": "Given a question and its concept, choose the correct answer from a set of options. Answers must be one of A, D, B, C, E."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "question_concept"
        ],
        "task_name": "commonsense_qa",
        "instruction": "Provide a concept that fits the given question."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "violence"
        ],
        "task_name": "ethos-multilabel",
        "instruction": "Given a text containing hate speech, predict whether it incites violence or not. Answers must be one of not_violent, violent."
    },
    {
        "input_fields": [
            "Problem",
            "options"
        ],
        "output_field": [
            "correct"
        ],
        "task_name": "math_qa",
        "instruction": "Given the problem and options, please select the correct answer. Answers must be one of b, d, a, c, e."
    },
    {
        "input_fields": [
            "category"
        ],
        "output_field": [
            "Problem"
        ],
        "task_name": "math_qa",
        "instruction": "Can you generate a problem given the category?"
    },
    {
        "input_fields": [
            "Problem"
        ],
        "output_field": [
            "annotated_formula"
        ],
        "task_name": "math_qa",
        "instruction": "Can you generate a formula given the problem?"
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qg_squad",
        "instruction": "Please generate a question based on the given sentence."
    },
    {
        "input_fields": [
            "answer"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "lmqg/qg_squad",
        "instruction": "Please generate a sentence that contains the given answer."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "summary"
        ],
        "task_name": "billsum",
        "instruction": "Given the text of a bill, summarize it."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "title"
        ],
        "task_name": "billsum",
        "instruction": "Given the text of a bill, can you write a title for it?"
    },
    {
        "input_fields": [
            "summary"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "billsum",
        "instruction": "Given the summary of a bill, can you generate the text of the bill?"
    },
    {
        "input_fields": [
            "sentences",
            "question",
            "options"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "cbt-CN",
        "instruction": "Given a sentence with a blank, please fill in the blank with the correct option."
    },
    {
        "input_fields": [
            "sentences",
            "answer",
            "options"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "cbt-CN",
        "instruction": "Given a sentence with a blank, please fill in the blank with the correct option and the corresponding question."
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "banking77",
        "instruction": "Given an intent, generate a query that corresponds to it."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "long_answer",
            "final_decision"
        ],
        "task_name": "pubmed_qa-pqa_artificial",
        "instruction": "Given a question, please provide the corresponding long answer and final decision."
    },
    {
        "input_fields": [
            "long_answer"
        ],
        "output_field": [
            "question",
            "final_decision"
        ],
        "task_name": "pubmed_qa-pqa_artificial",
        "instruction": "Given a long answer, please provide the corresponding question and final decision."
    },
    {
        "input_fields": [
            "final_decision"
        ],
        "output_field": [
            "question",
            "long_answer"
        ],
        "task_name": "pubmed_qa-pqa_artificial",
        "instruction": "Given a final decision, please provide the corresponding question and long answer."
    },
    {
        "input_fields": [
            "masked_sentence"
        ],
        "output_field": [
            "obj_label"
        ],
        "task_name": "lama-squad",
        "instruction": "Given a masked sentence and a label, please provide the correct word to fill in the blank."
    },
    {
        "input_fields": [
            "masked_sentence",
            "obj_label"
        ],
        "output_field": [
            "negated"
        ],
        "task_name": "lama-squad",
        "instruction": "Given a masked sentence and a label, please provide a sentence that contradicts the masked sentence."
    },
    {
        "input_fields": [
            "sub",
            "pred"
        ],
        "output_field": [
            "obj"
        ],
        "task_name": "lama-google_re",
        "instruction": "Given a subject and a predicate, predict the object."
    },
    {
        "input_fields": [
            "sub",
            "masked_sentence"
        ],
        "output_field": [
            "obj"
        ],
        "task_name": "lama-conceptnet",
        "instruction": "Given a masked sentence and the subject, predict the object."
    },
    {
        "input_fields": [
            "obj",
            "masked_sentence"
        ],
        "output_field": [
            "sub"
        ],
        "task_name": "lama-conceptnet",
        "instruction": "Given a masked sentence and the object, predict the subject."
    },
    {
        "input_fields": [
            "pred",
            "masked_sentence"
        ],
        "output_field": [
            "sub",
            "obj"
        ],
        "task_name": "lama-conceptnet",
        "instruction": "Given a masked sentence and the predicate, predict the subject and object."
    },
    {
        "input_fields": [
            "Idx"
        ],
        "output_field": [
            "Utterance"
        ],
        "task_name": "silicone-dyda_da",
        "instruction": "Given an index, predict the utterance."
    },
    {
        "input_fields": [
            "Dialogue_ID",
            "Utterance_ID",
            "Utterance",
            "Emotion"
        ],
        "output_field": [
            "Label"
        ],
        "task_name": "silicone-iemocap",
        "instruction": "Given the dialogue and emotion, predict the label. Answers must be one of exc, fru, ang, xxx, neu, sur, hap."
    },
    {
        "input_fields": [
            "Dialogue_ID",
            "Utterance_ID",
            "Utterance",
            "Label"
        ],
        "output_field": [
            "Emotion"
        ],
        "task_name": "silicone-iemocap",
        "instruction": "Given the dialogue and label, predict the emotion. Answers must be one of exc, fru, ang, xxx, neu, sur, hap."
    },
    {
        "input_fields": [
            "Dialogue_ID",
            "Utterance_ID",
            "Utterance",
            "Emotion"
        ],
        "output_field": [
            "Idx"
        ],
        "task_name": "silicone-iemocap",
        "instruction": "Given the dialogue and emotion, predict the index."
    },
    {
        "input_fields": [
            "Speaker",
            "Utterance"
        ],
        "output_field": [
            "Dialogue_Act"
        ],
        "task_name": "silicone-maptask",
        "instruction": "Given the Speaker and Utterance, predict the Dialogue_Act."
    },
    {
        "input_fields": [
            "Speaker",
            "Dialogue_Act"
        ],
        "output_field": [
            "Utterance"
        ],
        "task_name": "silicone-maptask",
        "instruction": "Given the Speaker and Dialogue_Act, generate an Utterance."
    },
    {
        "input_fields": [
            "Speaker",
            "Label"
        ],
        "output_field": [
            "Dialogue_Act"
        ],
        "task_name": "silicone-maptask",
        "instruction": "Given the Speaker and Label, predict the Dialogue_Act."
    },
    {
        "input_fields": [
            "Dialogue_Act"
        ],
        "output_field": [
            "Label"
        ],
        "task_name": "silicone-oasis",
        "instruction": "Given a dialogue act, please predict the label."
    },
    {
        "input_fields": [
            "Label"
        ],
        "output_field": [
            "Dialogue_Act"
        ],
        "task_name": "silicone-oasis",
        "instruction": "Given a label, please generate a dialogue act."
    },
    {
        "input_fields": [
            "Utterance"
        ],
        "output_field": [
            "Speaker"
        ],
        "task_name": "silicone-oasis",
        "instruction": "Given an utterance, please predict the speaker. Answers must be one of b, a."
    },
    {
        "input_fields": [
            "Dialogue_ID"
        ],
        "output_field": [
            "Speaker"
        ],
        "task_name": "silicone-sem",
        "instruction": "Given the dialogue ID, please predict the speaker of the next utterance. Answers must be one of User, Agent."
    },
    {
        "input_fields": [
            "Utterance"
        ],
        "output_field": [
            "Sentiment"
        ],
        "task_name": "silicone-sem",
        "instruction": "Given the utterance, please predict the sentiment of the speaker. Answers must be one of Positive, Negative, Neutral."
    },
    {
        "input_fields": [
            "Dialogue_ID",
            "SpeechTurn"
        ],
        "output_field": [
            "Label"
        ],
        "task_name": "silicone-sem",
        "instruction": "Given the dialogue ID and the speech turn, please predict the label of the utterance. Answers must be one of Positive, Negative, Neutral."
    },
    {
        "input_fields": [
            "Utterance"
        ],
        "output_field": [
            "Dialogue_Act"
        ],
        "task_name": "silicone-swda",
        "instruction": "Given an utterance, predict the dialogue act. Answers must be one of b, +, %, qo, na, sv_fx, x, ba_fe, qy^d, sd."
    },
    {
        "input_fields": [
            "Conv_ID"
        ],
        "output_field": [
            "Label"
        ],
        "task_name": "silicone-swda",
        "instruction": "Given a conversation, predict the label of each utterance. Answers must be one of b, +, %, qo, na, sv_fx, x, ba_fe, qy^d, sd."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "snips_built_in_intents",
        "instruction": "Given a text, predict the intent class."
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "snips_built_in_intents",
        "instruction": "Create a text that belongs to a specific intent class."
    },
    {
        "input_fields": [
            "questions",
            "answers"
        ],
        "output_field": [
            "paragraph"
        ],
        "task_name": "lmqg/qag_squad",
        "instruction": "Please generate a paragraph based on the questions and answers."
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "claim"
        ],
        "task_name": "fever-v1.0",
        "instruction": "Create a new claim that is supported by the given label."
    },
    {
        "input_fields": [
            "claim"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "fever-v2.0",
        "instruction": "Given a claim, please determine whether it is supported, refuted or not enough information. Answers must be one of Not Enough Info, SUPPORTS, NOT ENOUGH INFO, REFUTES."
    },
    {
        "input_fields": [
            "prompt",
            "utterance"
        ],
        "output_field": [
            "selfeval"
        ],
        "task_name": "empathetic_dialogues",
        "instruction": "Given a prompt and an utterance, predict the self-evaluation score of the speaker on a scale of 1-5 for each of the following categories: fluency, coherence, and relevance."
    },
    {
        "input_fields": [
            "context",
            "question-X"
        ],
        "output_field": [
            "answer-Y"
        ],
        "task_name": "circa",
        "instruction": "Given a context and a question-X, predict the answer-Y."
    },
    {
        "input_fields": [
            "context",
            "answer-Y"
        ],
        "output_field": [
            "question-X"
        ],
        "task_name": "circa",
        "instruction": "Given a context and an answer-Y, predict the question-X."
    },
    {
        "input_fields": [
            "context",
            "question-X",
            "answer-Y"
        ],
        "output_field": [
            "judgements"
        ],
        "task_name": "circa",
        "instruction": "Given a context and an answer-Y, predict whether the answer is a positive or negative response to the question-X."
    },
    {
        "input_fields": [
            "description"
        ],
        "output_field": [
            "abstract"
        ],
        "task_name": "big_patent-all",
        "instruction": "Given a description of an implantable medical device, please write an abstract for the patent application."
    },
    {
        "input_fields": [
            "abstract"
        ],
        "output_field": [
            "description"
        ],
        "task_name": "big_patent-all",
        "instruction": "Given an abstract of a patent application, please write a description of the implantable medical device."
    },
    {
        "input_fields": [
            "description"
        ],
        "output_field": [
            "abstract"
        ],
        "task_name": "big_patent-a",
        "instruction": "Given a description of an implantable medical device, please write an abstract for the patent application."
    },
    {
        "input_fields": [
            "abstract"
        ],
        "output_field": [
            "description"
        ],
        "task_name": "big_patent-a",
        "instruction": "Given an abstract of a patent application, please write a description of the implantable medical device."
    },
    {
        "input_fields": [
            "abstract"
        ],
        "output_field": [
            "description"
        ],
        "task_name": "big_patent-b",
        "instruction": "Given an abstract of a patent, please write a description of the invention."
    },
    {
        "input_fields": [
            "description"
        ],
        "output_field": [
            "abstract"
        ],
        "task_name": "big_patent-b",
        "instruction": "Given a description of an invention, please write an abstract of the patent."
    },
    {
        "input_fields": [
            "description"
        ],
        "output_field": [
            "abstract"
        ],
        "task_name": "big_patent-d",
        "instruction": "Given a description of a fiber removing apparatus and a quantity measuring apparatus, please write an abstract that summarizes the method of forming fiber mixtures from different kinds of fiber."
    },
    {
        "input_fields": [
            "description"
        ],
        "output_field": [
            "abstract"
        ],
        "task_name": "big_patent-d",
        "instruction": "Given a description of an apparatus for cleaning fibers of recycled paper, please write an abstract that summarizes the method of cleaning the fibers."
    },
    {
        "input_fields": [
            "abstract"
        ],
        "output_field": [
            "description"
        ],
        "task_name": "big_patent-d",
        "instruction": "Given an abstract that summarizes the method of forming fiber mixtures from different kinds of fiber, please write a description of a fiber removing apparatus and a quantity measuring apparatus."
    },
    {
        "input_fields": [
            "abstract"
        ],
        "output_field": [
            "description"
        ],
        "task_name": "big_patent-d",
        "instruction": "Given an abstract that summarizes the method of cleaning fibers of recycled paper, please write a description of an apparatus for cleaning fibers."
    },
    {
        "input_fields": [
            "abstract"
        ],
        "output_field": [
            "description"
        ],
        "task_name": "big_patent-h",
        "instruction": "Given an abstract of a patent, please generate a description of the patent."
    },
    {
        "input_fields": [
            "description"
        ],
        "output_field": [
            "abstract"
        ],
        "task_name": "big_patent-h",
        "instruction": "Given a description of a patent, please generate an abstract of the patent."
    },
    {
        "input_fields": [
            "abstract"
        ],
        "output_field": [
            "description"
        ],
        "task_name": "big_patent-y",
        "instruction": "Given an abstract of a patent, please predict the description of the invention."
    },
    {
        "input_fields": [
            "question",
            "passage"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "boolq",
        "instruction": "Please answer the question based on the passage. The answer is either True or False. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "passage"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "boolq",
        "instruction": "Create a question provided the passage."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "passage"
        ],
        "task_name": "boolq",
        "instruction": "Can you write a passage that answers the question?"
    },
    {
        "input_fields": [
            "intent"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "clinc_oos-small",
        "instruction": "Given an intent class, generate a query that belongs to this class."
    },
    {
        "input_fields": [
            "meaning_representation"
        ],
        "output_field": [
            "human_reference"
        ],
        "task_name": "e2e_nlg",
        "instruction": "Generate a human reference text given the meaning representation."
    },
    {
        "input_fields": [
            "human_reference"
        ],
        "output_field": [
            "meaning_representation"
        ],
        "task_name": "e2e_nlg",
        "instruction": "Given a human reference text, generate the corresponding meaning representation."
    },
    {
        "input_fields": [
            "Tweet"
        ],
        "output_field": [
            "Question"
        ],
        "task_name": "tweet_qa",
        "instruction": "Create a question based on the tweet."
    },
    {
        "input_fields": [
            "question",
            "options"
        ],
        "output_field": [
            "correct"
        ],
        "task_name": "aqua_rat-raw",
        "instruction": "Given a question and options, select the correct answer. Answers must be one of A, C, B, D, E."
    },
    {
        "input_fields": [
            "question",
            "correct"
        ],
        "output_field": [
            "rationale"
        ],
        "task_name": "aqua_rat-raw",
        "instruction": "Given a question and the correct answer, provide the explanation step-by-step using natural language."
    },
    {
        "input_fields": [
            "question",
            "options"
        ],
        "output_field": [
            "correct"
        ],
        "task_name": "aqua_rat-tokenized",
        "instruction": "Given a question and options, select the correct answer. Answers must be one of A, C, B, D, E."
    },
    {
        "input_fields": [
            "question",
            "options"
        ],
        "output_field": [
            "rationale"
        ],
        "task_name": "aqua_rat-tokenized",
        "instruction": "Given a question and options, provide the rationale for the correct answer."
    },
    {
        "input_fields": [
            "question",
            "article"
        ],
        "output_field": [
            "is_paraphrase"
        ],
        "task_name": "selqa-answer_selection_analysis",
        "instruction": "Given a question and an article, determine whether the question is a paraphrase of a previous question in the article. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "selqa-answer_selection_experiments",
        "instruction": "Given a label, generate a question that would have that label."
    },
    {
        "input_fields": [
            "article",
            "topic"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "selqa-answer_triggering_analysis",
        "instruction": "Given the article and topic, generate a question that can be answered by the article."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "is_paraphrase"
        ],
        "task_name": "selqa-answer_triggering_analysis",
        "instruction": "Determine if the question is a paraphrase of another question in the dataset. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "article",
            "question"
        ],
        "output_field": [
            "section"
        ],
        "task_name": "selqa-answer_triggering_analysis",
        "instruction": "Given the article and question, determine the section of the article that contains the answer."
    },
    {
        "input_fields": [
            "query"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "spider",
        "instruction": "Given a query, please generate the corresponding question."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "query"
        ],
        "task_name": "spider",
        "instruction": "Given a question, please generate the corresponding query."
    },
    {
        "input_fields": [
            "incoherent_first_sentence",
            "coherent_second_sentence"
        ],
        "output_field": [
            "connective_string"
        ],
        "task_name": "discofuse-discofuse-sport",
        "instruction": "Given an incoherent sentence and a coherent sentence, please select the connective string that can connect them."
    },
    {
        "input_fields": [
            "incoherent_first_sentence",
            "coherent_second_sentence"
        ],
        "output_field": [
            "discourse_type"
        ],
        "task_name": "discofuse-discofuse-sport",
        "instruction": "Given a pair of incoherent and coherent sentences, please identify the discourse type."
    },
    {
        "input_fields": [
            "incoherent_first_sentence",
            "coherent_second_sentence"
        ],
        "output_field": [
            "connective_string"
        ],
        "task_name": "discofuse-discofuse-wikipedia",
        "instruction": "Given an incoherent sentence and a coherent sentence, identify the connective string that can be used to fuse them together."
    },
    {
        "input_fields": [
            "incoherent_first_sentence",
            "coherent_second_sentence"
        ],
        "output_field": [
            "discourse_type"
        ],
        "task_name": "discofuse-discofuse-wikipedia",
        "instruction": "Given a pair of incoherent and coherent sentences, identify the discourse type that connects them."
    },
    {
        "input_fields": [
            "incoherent_first_sentence",
            "coherent_second_sentence"
        ],
        "output_field": [
            "has_coref_type_pronoun",
            "has_coref_type_nominal"
        ],
        "task_name": "discofuse-discofuse-wikipedia",
        "instruction": "Given a pair of incoherent and coherent sentences, identify whether the second sentence has a coreference type pronoun or nominal. Answers must be one of 1.0, 0.0."
    },
    {
        "input_fields": [
            "text",
            "original"
        ],
        "output_field": [
            "class_type"
        ],
        "task_name": "md_gender_bias-new_data",
        "instruction": "Given a sentence and its original version, determine whether the gender of the person being spoken about is biased or not. Answers must be one of about, partner, self."
    },
    {
        "input_fields": [
            "text",
            "original"
        ],
        "output_field": [
            "class_type"
        ],
        "task_name": "md_gender_bias-new_data",
        "instruction": "Given a sentence and its original version, determine whether the gender of the person being spoken to is biased or not. Answers must be one of about, partner, self."
    },
    {
        "input_fields": [
            "text",
            "original"
        ],
        "output_field": [
            "class_type"
        ],
        "task_name": "md_gender_bias-new_data",
        "instruction": "Given a sentence and its original version, determine whether the gender of the speaker is biased or not. Answers must be one of about, partner, self."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "binary_label"
        ],
        "task_name": "md_gender_bias-convai2_inferred",
        "instruction": "Given a text, predict the binary label of the text (male or female). Answers must be one of ABOUT:male, ABOUT:female."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "ternary_label"
        ],
        "task_name": "md_gender_bias-convai2_inferred",
        "instruction": "Given a text, predict the ternary label of the text (male, female, or gender-neutral)."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "binary_score"
        ],
        "task_name": "md_gender_bias-convai2_inferred",
        "instruction": "Given a text, predict the binary score of the text (the probability of the text being male)."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "ternary_score"
        ],
        "task_name": "md_gender_bias-convai2_inferred",
        "instruction": "Given a text, predict the ternary score of the text (the probability of the text being male, female, or gender-neutral)."
    },
    {
        "input_fields": [
            "word_masculine"
        ],
        "output_field": [
            "word_feminine"
        ],
        "task_name": "md_gender_bias-gendered_words",
        "instruction": "Given a masculine word, provide the corresponding feminine word."
    },
    {
        "input_fields": [
            "word_feminine"
        ],
        "output_field": [
            "word_masculine"
        ],
        "task_name": "md_gender_bias-gendered_words",
        "instruction": "Given a feminine word, provide the corresponding masculine word."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "neural_code_search-evaluation_dataset",
        "instruction": "Given a natural language query, please provide a code snippet that answers the query."
    },
    {
        "input_fields": [
            "answer"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "neural_code_search-evaluation_dataset",
        "instruction": "Given a code snippet, please provide a natural language query that the code snippet answers."
    },
    {
        "input_fields": [
            "method_name"
        ],
        "output_field": [
            "filepath"
        ],
        "task_name": "neural_code_search-search_corpus",
        "instruction": "Given a method name, can you find the corresponding file path?"
    },
    {
        "input_fields": [
            "filepath"
        ],
        "output_field": [
            "method_name"
        ],
        "task_name": "neural_code_search-search_corpus",
        "instruction": "Given a file path, can you find the corresponding method name?"
    },
    {
        "input_fields": [
            "start_line",
            "end_line"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "neural_code_search-search_corpus",
        "instruction": "Given a code snippet, can you find the corresponding URL?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/web_nlg-en",
        "instruction": "Given an input triple, generate a short text that describes the entity and its property."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/web_nlg-ru",
        "instruction": "Given an input triple set, generate a short text that covers the DBpedia properties."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "target",
            "category"
        ],
        "task_name": "GEM/web_nlg-ru",
        "instruction": "Given an input triple set, generate a short text that covers the DBpedia properties and the category."
    },
    {
        "input_fields": [
            "question",
            "viewed_doc_titles"
        ],
        "output_field": [
            "nq_doc_title"
        ],
        "task_name": "ambig_qa-full",
        "instruction": "Given a question and a list of viewed document titles, select the correct document title that answers the question."
    },
    {
        "input_fields": [
            "content"
        ],
        "output_field": [
            "rating"
        ],
        "task_name": "google_wellformed_query",
        "instruction": "Determine whether a query is well-formed or not. Answers must be one of 0.8, 0.0, 0.4, 0.2, 1.0, 0.6."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "numer_sense",
        "instruction": "Given a sentence with a masked number between 0-10, please predict the correct number."
    },
    {
        "input_fields": [
            "question",
            "correct_answers",
            "incorrect_answers"
        ],
        "output_field": [
            "best_answer"
        ],
        "task_name": "truthful_qa-generation",
        "instruction": "Given a question and a list of possible answers, select the correct answer."
    },
    {
        "input_fields": [
            "answer"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "nq_open",
        "instruction": "Create a question provided the answer."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "biosses",
        "instruction": "Given a sentence pair, please predict the similarity score ranging from 0 (no relation) to 4 (equivalent)."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "relatedness_score"
        ],
        "task_name": "sem_eval_2014_task_1",
        "instruction": "Given a premise and a hypothesis, predict the relatedness score between them."
    },
    {
        "input_fields": [
            "premise",
            "entailment_judgment"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "sem_eval_2014_task_1",
        "instruction": "Given a premise and an entailment judgment, write a hypothesis that is consistent with the premise and has the opposite entailment judgment."
    },
    {
        "input_fields": [
            "source_tokens"
        ],
        "output_field": [
            "error_location"
        ],
        "task_name": "great_code",
        "instruction": "Given the source code, identify the line number where the bug is located."
    },
    {
        "input_fields": [
            "source_tokens"
        ],
        "output_field": [
            "bug_kind_name"
        ],
        "task_name": "great_code",
        "instruction": "Given the source code, identify the type of bug. Answers must be one of VARIABLE_MISUSE, NONE."
    },
    {
        "input_fields": [
            "source_tokens"
        ],
        "output_field": [
            "has_bug"
        ],
        "task_name": "great_code",
        "instruction": "Given the source code, identify whether there is a bug or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "question_query"
        ],
        "output_field": [
            "category_name"
        ],
        "task_name": "covid_qa_castorini",
        "instruction": "Identify the category of the given question."
    },
    {
        "input_fields": [
            "scenario",
            "label"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "nlu_evaluation_data",
        "instruction": "Given a scenario and a label, generate a text that matches the intent."
    },
    {
        "input_fields": [
            "text",
            "label"
        ],
        "output_field": [
            "scenario"
        ],
        "task_name": "nlu_evaluation_data",
        "instruction": "Given a text and a label, determine the scenario."
    },
    {
        "input_fields": [
            "text",
            "scenario"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "nlu_evaluation_data",
        "instruction": "Given a text and a scenario, determine the intent."
    },
    {
        "input_fields": [
            "human_reference"
        ],
        "output_field": [
            "meaning_representation"
        ],
        "task_name": "e2e_nlg_cleaned",
        "instruction": "Generate a meaning representation based on the human reference."
    },
    {
        "input_fields": [
            "meaning_representation"
        ],
        "output_field": [
            "human_reference"
        ],
        "task_name": "e2e_nlg_cleaned",
        "instruction": "Generate a human reference based on the meaning representation."
    },
    {
        "input_fields": [
            "meaning_representation"
        ],
        "output_field": [
            "human_reference"
        ],
        "task_name": "e2e_nlg_cleaned",
        "instruction": "Given a meaning representation, can you generate a more concise human reference?"
    },
    {
        "input_fields": [
            "context",
            "question"
        ],
        "output_field": [
            "reference"
        ],
        "task_name": "mocha",
        "instruction": "Given a context and a question, generate a reference answer."
    },
    {
        "input_fields": [
            "context",
            "reference"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "mocha",
        "instruction": "Given a context and a reference answer, generate a question."
    },
    {
        "input_fields": [
            "normal_sentence"
        ],
        "output_field": [
            "simple_sentence"
        ],
        "task_name": "wiki_auto-manual",
        "instruction": "Given a sentence in English Wikipedia, please provide the corresponding sentence in Simple English Wikipedia."
    },
    {
        "input_fields": [
            "simple_sentence"
        ],
        "output_field": [
            "normal_sentence"
        ],
        "task_name": "wiki_auto-manual",
        "instruction": "Given a sentence in Simple English Wikipedia, please provide the corresponding sentence in English Wikipedia."
    },
    {
        "input_fields": [
            "normal_sentence",
            "simple_sentence"
        ],
        "output_field": [
            "gleu_score"
        ],
        "task_name": "wiki_auto-manual",
        "instruction": "Can you predict the GLEU score of a sentence pair?"
    },
    {
        "input_fields": [
            "snippet",
            "question",
            "scenario"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "sharc",
        "instruction": "Given a scenario and a question, determine whether the answer is yes or no based on the snippet provided."
    },
    {
        "input_fields": [
            "question1",
            "question2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "adv_glue-adv_qqp",
        "instruction": "Determine if two questions are duplicates or not. Answers must be one of duplicate, not_duplicate."
    },
    {
        "input_fields": [
            "question1"
        ],
        "output_field": [
            "question2"
        ],
        "task_name": "adv_glue-adv_qqp",
        "instruction": "Create a question that is similar to the given question."
    },
    {
        "input_fields": [
            "question1"
        ],
        "output_field": [
            "question2"
        ],
        "task_name": "adv_glue-adv_qqp",
        "instruction": "Create a question that is dissimilar to the given question."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "adv_glue-adv_mnli",
        "instruction": "Given a premise and a hypothesis, determine whether the hypothesis is entailment, contradiction, or neutral. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "adv_glue-adv_mnli",
        "instruction": "Create a new hypothesis that contradicts the given premise."
    },
    {
        "input_fields": [
            "hypothesis"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "adv_glue-adv_mnli",
        "instruction": "Create a new premise that entails the given hypothesis."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "adv_glue-adv_mnli_mismatched",
        "instruction": "Given a premise and a hypothesis, determine whether the hypothesis is entailment, contradiction, or neutral. Answers must be one of entailment, neutral, contradiction."
    },
    {
        "input_fields": [
            "question",
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "adv_glue-adv_qnli",
        "instruction": "Determine whether the sentence entails, contradicts, or is neutral to the question. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "label",
            "sentence"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "adv_glue-adv_qnli",
        "instruction": "Create a question that entails, contradicts, or is neutral to the sentence."
    },
    {
        "input_fields": [
            "label",
            "question"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "adv_glue-adv_qnli",
        "instruction": "Create a sentence that entails, contradicts, or is neutral to the question."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "adv_glue-adv_rte",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "label",
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "adv_glue-adv_rte",
        "instruction": "This task asks models to write a sentence that entails, contradicts to, or is neutral to the given sentence."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "adv_glue-adv_rte",
        "instruction": "Can you identify the sentence that is more likely to be true? Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "created_timestamp"
        ],
        "task_name": "pile-of-law/pile-of-law-all",
        "instruction": "Given a legal text, predict the timestamp when it was created."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "created_timestamp"
        ],
        "task_name": "pile-of-law/pile-of-law-r_legaladvice",
        "instruction": "Given a legal advice post, can you predict the date it was created?"
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "downloaded_timestamp"
        ],
        "task_name": "pile-of-law/pile-of-law-r_legaladvice",
        "instruction": "Given a legal advice post, can you predict the date it was downloaded?"
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "pile-of-law/pile-of-law-r_legaladvice",
        "instruction": "Given a legal advice post, can you predict the URL of the post?"
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "pile-of-law/pile-of-law-courtlistener_opinions",
        "instruction": "Given the text, please predict the URL of the judgment."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "created_timestamp"
        ],
        "task_name": "pile-of-law/pile-of-law-us_bills",
        "instruction": "Given the text of a bill, please extract the date when it was created."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "created_timestamp"
        ],
        "task_name": "pile-of-law/pile-of-law-scotus_oral_arguments",
        "instruction": "Given the text, please extract the created timestamp."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "created_timestamp"
        ],
        "task_name": "pile-of-law/pile-of-law-congressional_hearings",
        "instruction": "Extract the created timestamp of the hearing from the text."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "pile-of-law/pile-of-law-congressional_hearings",
        "instruction": "Extract the URL of the hearing from the text."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "created_timestamp"
        ],
        "task_name": "pile-of-law/pile-of-law-oig",
        "instruction": "Given the text, please extract the created timestamp. Answers must be one of 2008, 2014, 2009, 2011."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "created_timestamp"
        ],
        "task_name": "pile-of-law/pile-of-law-olc_memos",
        "instruction": "Given a memo, please predict the year it was created."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "pile-of-law/pile-of-law-olc_memos",
        "instruction": "Given a memo, please predict the URL where it can be downloaded."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "created_timestamp"
        ],
        "task_name": "pile-of-law/pile-of-law-founding_docs",
        "instruction": "Given a text, can you extract the date it was created?"
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "pile-of-law/pile-of-law-founding_docs",
        "instruction": "Given a text, can you extract the URL?"
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "created_timestamp",
            "downloaded_timestamp"
        ],
        "task_name": "pile-of-law/pile-of-law-eoir",
        "instruction": "Given a text, please extract the created timestamp and downloaded timestamp."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "pile-of-law/pile-of-law-eoir",
        "instruction": "Given a text, please extract the URL."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "pile-of-law/pile-of-law-ed_policy_guidance",
        "instruction": "Given a text, can you extract the URL?"
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "created_timestamp"
        ],
        "task_name": "pile-of-law/pile-of-law-resource_contracts",
        "instruction": "Extract the created timestamp from the contract text."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "pile-of-law/pile-of-law-resource_contracts",
        "instruction": "Extract the URL from the contract text."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "pile-of-law/pile-of-law-irs_legal_advice_memos",
        "instruction": "Given a text, can you extract the URL?"
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "zest",
        "instruction": "Given a context, please generate a question."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "triple"
        ],
        "task_name": "kelm",
        "instruction": "Generate a triple given a sentence."
    },
    {
        "input_fields": [
            "triple"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "kelm",
        "instruction": "Generate a sentence given a triple."
    },
    {
        "input_fields": [
            "target",
            "bias_type"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "stereoset-intersentence",
        "instruction": "Given a target and a bias type, generate a sentence that contains the target and the bias type."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "dynabench/qa-dynabench.qa.r1.all",
        "instruction": "Given a context, please generate a question."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "dynabench/qa-dynabench.qa.r1.dbert",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "title"
        ],
        "task_name": "dynabench/qa-dynabench.qa.r1.dbert",
        "instruction": "Can you write a title for the passage?"
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "dynabench/qa-dynabench.qa.r1.droberta",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "covid_qa_deepset",
        "instruction": "Can you write a question based on the scientific article?"
    },
    {
        "input_fields": [
            "service_name"
        ],
        "output_field": [
            "description"
        ],
        "task_name": "schema_guided_dstc8-schema",
        "instruction": "Given a service name, please provide its description."
    },
    {
        "input_fields": [
            "description"
        ],
        "output_field": [
            "service_name"
        ],
        "task_name": "schema_guided_dstc8-schema",
        "instruction": "Given a description, please provide the corresponding service name."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "totalcount"
        ],
        "task_name": "proto_qa-proto_qa",
        "instruction": "Given a question, please provide the total count of answers."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "totalcount"
        ],
        "task_name": "proto_qa-proto_qa_cs",
        "instruction": "Given a question, please provide the number of possible answers."
    },
    {
        "input_fields": [
            "answerstrings"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "proto_qa-proto_qa_cs",
        "instruction": "Given a list of possible answers, please provide a question that could elicit one of these answers."
    },
    {
        "input_fields": [
            "assessments"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "proto_qa-proto_qa_cs_assessments",
        "instruction": "Given a list of assessments, please generate a question that could lead to these assessments."
    },
    {
        "input_fields": [
            "target"
        ],
        "output_field": [
            "title"
        ],
        "task_name": "GEM/wiki_cat_sum-animal",
        "instruction": "Given a target animal, predict the title of the corresponding Wikipedia article."
    },
    {
        "input_fields": [
            "paragraphs"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/wiki_cat_sum-film",
        "instruction": "Please summarize the given paragraphs."
    },
    {
        "input_fields": [
            "domain",
            "title"
        ],
        "output_field": [
            "doc_text"
        ],
        "task_name": "multidoc2dial-document_domain",
        "instruction": "Given a domain and a title, please provide the corresponding document text."
    },
    {
        "input_fields": [
            "domain",
            "title"
        ],
        "output_field": [
            "doc_html_ts"
        ],
        "task_name": "multidoc2dial-document_domain",
        "instruction": "Given a domain and a title, please provide the corresponding document HTML in text span format."
    },
    {
        "input_fields": [
            "domain",
            "title"
        ],
        "output_field": [
            "doc_html_raw"
        ],
        "task_name": "multidoc2dial-document_domain",
        "instruction": "Given a domain and a title, please provide the corresponding document HTML in raw format."
    },
    {
        "input_fields": [
            "context",
            "da"
        ],
        "output_field": [
            "utterance"
        ],
        "task_name": "multidoc2dial-multidoc2dial",
        "instruction": "Given a context and a DA, generate an utterance that responds to the DA."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_math_jsonl-titlebody_answer",
        "instruction": "Given a question, please provide the corresponding answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "masked_uri"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "ekinakyurek/ftrace-abstracts",
        "instruction": "Given a masked sentence and a masked entity, please fill in the blank with the correct entity."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "masked_uri"
        ],
        "output_field": [
            "masked_type"
        ],
        "task_name": "ekinakyurek/ftrace-abstracts",
        "instruction": "Given a masked sentence and a masked entity, please identify the type of the masked entity. Answers must be one of subject, object."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "masked_uri"
        ],
        "output_field": [
            "facts"
        ],
        "task_name": "ekinakyurek/ftrace-abstracts",
        "instruction": "Given a masked sentence and a masked entity, please provide the facts associated with the masked entity."
    },
    {
        "input_fields": [
            "sub_surface"
        ],
        "output_field": [
            "sub_uri"
        ],
        "task_name": "ekinakyurek/ftrace-queries",
        "instruction": "Given a sub_surface, please provide the corresponding sub_uri."
    },
    {
        "input_fields": [
            "obj_surface"
        ],
        "output_field": [
            "obj_uri"
        ],
        "task_name": "ekinakyurek/ftrace-queries",
        "instruction": "Given an obj_surface, please provide the corresponding obj_uri."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "ekinakyurek/ftrace-queries",
        "instruction": "Given an inputs_pretokenized, please provide the corresponding targets_pretokenized."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "act_tag"
        ],
        "task_name": "swda",
        "instruction": "Given the text, please predict the act tag."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "damsl_act_tag"
        ],
        "task_name": "swda",
        "instruction": "Given the text, please predict the damsl act tag."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "caller"
        ],
        "task_name": "swda",
        "instruction": "Given the text, please predict the caller. Answers must be one of A, B."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "wiki_movies",
        "instruction": "Given a movie title, please provide a list of actors/actresses who starred in the movie."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "wiki_movies",
        "instruction": "Given an actor/actress name, please provide a list of movies they starred in."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "wiki_movies",
        "instruction": "Given a movie title, please provide a brief summary of the plot."
    },
    {
        "input_fields": [
            "func1",
            "func2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "code_x_glue_cc_clone_detection_big_clone_bench",
        "instruction": "Given two codes as the input, the task is to do binary classification (0/1), where 1 stands for semantic equivalence and 0 for others. Models are evaluated by F1 score. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "claim_text"
        ],
        "output_field": [
            "review_rating"
        ],
        "task_name": "datacommons_factcheck-fctchk_politifact_wapo",
        "instruction": "Given a claim, can you predict the rating given by the fact-checker?"
    },
    {
        "input_fields": [
            "claim_text"
        ],
        "output_field": [
            "claim_author_name"
        ],
        "task_name": "datacommons_factcheck-fctchk_politifact_wapo",
        "instruction": "Given a claim, can you predict the author of the claim?"
    },
    {
        "input_fields": [
            "claim_text"
        ],
        "output_field": [
            "claim_date"
        ],
        "task_name": "datacommons_factcheck-fctchk_politifact_wapo",
        "instruction": "Given a claim, can you predict the date when the claim was made?"
    },
    {
        "input_fields": [
            "claim_text"
        ],
        "output_field": [
            "claim_author_name"
        ],
        "task_name": "datacommons_factcheck-weekly_standard",
        "instruction": "Given a claim, can you predict the author of the claim?"
    },
    {
        "input_fields": [
            "claim_text"
        ],
        "output_field": [
            "claim_date"
        ],
        "task_name": "datacommons_factcheck-weekly_standard",
        "instruction": "Given a claim, can you predict the date of the claim?"
    },
    {
        "input_fields": [
            "arg1",
            "rel"
        ],
        "output_field": [
            "arg2"
        ],
        "task_name": "ascent_kb-canonical",
        "instruction": "Given an argument and a relation, please provide the second argument."
    },
    {
        "input_fields": [
            "arg1",
            "rel"
        ],
        "output_field": [
            "support"
        ],
        "task_name": "ascent_kb-canonical",
        "instruction": "Given an argument and a relation, please provide the support for the assertion."
    },
    {
        "input_fields": [
            "subject",
            "predicate"
        ],
        "output_field": [
            "object"
        ],
        "task_name": "ascent_kb-open",
        "instruction": "Given a subject and a predicate, please provide the object that completes the assertion."
    },
    {
        "input_fields": [
            "subject",
            "object"
        ],
        "output_field": [
            "predicate"
        ],
        "task_name": "ascent_kb-open",
        "instruction": "Given a subject and an object, please provide the predicate that completes the assertion."
    },
    {
        "input_fields": [
            "publication_year"
        ],
        "output_field": [
            "text",
            "relevant_documents"
        ],
        "task_name": "eu_regulatory_ir-eu2uk",
        "instruction": "Given a publication year, please provide the corresponding text and relevant documents."
    },
    {
        "input_fields": [
            "relevant_documents"
        ],
        "output_field": [
            "publication_year",
            "text"
        ],
        "task_name": "eu_regulatory_ir-eu2uk",
        "instruction": "Given a relevant document, please provide the corresponding publication year and text. Answers must be one of 1981, 1978, 1984, 1983, 1982, 1980, 1977, 1979."
    },
    {
        "input_fields": [
            "publication_year"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "eu_regulatory_ir-uk2eu",
        "instruction": "Given a publication year, can you extract the text of the corresponding act?"
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "publication_year"
        ],
        "task_name": "eu_regulatory_ir-uk2eu",
        "instruction": "Given a document, can you extract the publication year?"
    },
    {
        "input_fields": [
            "user_query",
            "tree_str_mr"
        ],
        "output_field": [
            "response"
        ],
        "task_name": "GEM/conversational_weather-default",
        "instruction": "Given a user query and a tree structure, generate a response to a weather-related query."
    },
    {
        "input_fields": [
            "user_query",
            "tree_str_mr"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/conversational_weather-default",
        "instruction": "Given a user query and a tree structure, generate a target response to a weather-related query."
    },
    {
        "input_fields": [
            "subreddit.name",
            "title"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/reddit-crypto-aug-2021-posts",
        "instruction": "Given the subreddit name and the title of a post, predict the score of the post."
    },
    {
        "input_fields": [
            "type",
            "subreddit.name",
            "subreddit.nsfw",
            "created_utc",
            "permalink",
            "domain",
            "url",
            "selftext",
            "title"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/ten-million-reddit-answers-posts",
        "instruction": "Given a post, predict its sentiment."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "med_hop-original",
        "instruction": "Given a drug interaction, please provide the drug that interacts with the given drug."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "med_hop-masked",
        "instruction": "Fill in the blank with the name of the drug that interacts with DB00773."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "med_hop-masked",
        "instruction": "Fill in the blank with the name of the drug that interacts with DB09079."
    },
    {
        "input_fields": [
            "type",
            "subreddit.name",
            "subreddit.nsfw",
            "created_utc",
            "permalink",
            "domain",
            "url",
            "selftext",
            "title"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/one-year-of-r-india-posts",
        "instruction": "Given a post, predict its score (upvotes minus downvotes)."
    },
    {
        "input_fields": [
            "body"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/one-year-of-r-india-comments",
        "instruction": "Given a comment, predict its score."
    },
    {
        "input_fields": [
            "type",
            "subreddit.nsfw",
            "created_utc",
            "permalink",
            "domain",
            "url",
            "selftext",
            "title"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/reddit-wallstreetbets-aug-2021-posts",
        "instruction": "Predict the score of a post based on the post information."
    },
    {
        "input_fields": [
            "subreddit.name",
            "body"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/reddit-wallstreetbets-aug-2021-comments",
        "instruction": "Predict the score of the comment based on the subreddit name and the body of the comment."
    },
    {
        "input_fields": [
            "target",
            "references"
        ],
        "output_field": [
            "subtree_was_extended"
        ],
        "task_name": "GEM/dart",
        "instruction": "Given the target and references, determine whether the subtree was extended or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "target_sources",
            "subtree_was_extended"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/dart",
        "instruction": "Given the target_sources and subtree_was_extended, determine the target."
    },
    {
        "input_fields": [
            "type",
            "subreddit.name",
            "created_utc",
            "permalink",
            "domain",
            "url",
            "selftext",
            "title"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/top-american-universities-on-reddit-posts",
        "instruction": "Given a post, can you predict the score of the post?"
    },
    {
        "input_fields": [
            "subreddit.name"
        ],
        "output_field": [
            "body"
        ],
        "task_name": "SocialGrep/top-american-universities-on-reddit-comments",
        "instruction": "Given a subreddit name, return all comments from that subreddit."
    },
    {
        "input_fields": [
            "sentiment"
        ],
        "output_field": [
            "body"
        ],
        "task_name": "SocialGrep/top-american-universities-on-reddit-comments",
        "instruction": "Given a sentiment score, return all comments with a sentiment score above a certain threshold."
    },
    {
        "input_fields": [
            "score"
        ],
        "output_field": [
            "body"
        ],
        "task_name": "SocialGrep/top-american-universities-on-reddit-comments",
        "instruction": "Given a score, return all comments with a score above a certain threshold."
    },
    {
        "input_fields": [
            "bytecode"
        ],
        "output_field": [
            "address"
        ],
        "task_name": "mwritescode/slither-audited-smart-contracts-all-plain-text",
        "instruction": "Given the deployed bytecode of a Solidity Smart Contract, can you retrieve its address?"
    },
    {
        "input_fields": [
            "source_code"
        ],
        "output_field": [
            "bytecode"
        ],
        "task_name": "mwritescode/slither-audited-smart-contracts-all-plain-text",
        "instruction": "Given the source code of a Solidity Smart Contract, can you retrieve its bytecode?"
    },
    {
        "input_fields": [
            "bytecode"
        ],
        "output_field": [
            "address"
        ],
        "task_name": "mwritescode/slither-audited-smart-contracts-all-multilabel",
        "instruction": "Given the deployed bytecode of a Solidity Smart Contract, can you identify its address?"
    },
    {
        "input_fields": [
            "source_code"
        ],
        "output_field": [
            "bytecode"
        ],
        "task_name": "mwritescode/slither-audited-smart-contracts-all-multilabel",
        "instruction": "Given the source code of a Solidity Smart Contract, can you generate its bytecode?"
    },
    {
        "input_fields": [
            "bytecode"
        ],
        "output_field": [
            "source_code"
        ],
        "task_name": "mwritescode/slither-audited-smart-contracts-big-plain-text",
        "instruction": "Given the bytecode of a Solidity Smart Contract, can you provide the corresponding source code?"
    },
    {
        "input_fields": [
            "source_code"
        ],
        "output_field": [
            "address"
        ],
        "task_name": "mwritescode/slither-audited-smart-contracts-big-plain-text",
        "instruction": "Can you identify the address of the Solidity Smart Contract given its source code?"
    },
    {
        "input_fields": [
            "source_code"
        ],
        "output_field": [
            "address"
        ],
        "task_name": "mwritescode/slither-audited-smart-contracts-small-plain-text",
        "instruction": "Can you identify the contract address based on the source code?"
    },
    {
        "input_fields": [
            "bytecode"
        ],
        "output_field": [
            "address"
        ],
        "task_name": "mwritescode/slither-audited-smart-contracts-small-multilabel",
        "instruction": "Given the deployed bytecode of a Solidity Smart Contract, can you identify its address?"
    },
    {
        "input_fields": [
            "tokens"
        ],
        "output_field": [
            "start_index",
            "end_index"
        ],
        "task_name": "numeric_fused_head-identification",
        "instruction": "Identify the numeric fused head in the given sentence."
    },
    {
        "input_fields": [
            "tokens",
            "start_index",
            "end_index"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "numeric_fused_head-identification",
        "instruction": "Determine whether the numeric fused head in the given sentence is positive or negative. Answers must be one of pos, neg."
    },
    {
        "input_fields": [
            "paragraph"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "onestop_qa",
        "instruction": "Create a question provided the paragraph."
    },
    {
        "input_fields": [
            "paragraph"
        ],
        "output_field": [
            "level"
        ],
        "task_name": "onestop_qa",
        "instruction": "Determine the level of the paragraph. Answers must be one of Ele, Int, Adv."
    },
    {
        "input_fields": [
            "bytecode"
        ],
        "output_field": [
            "address"
        ],
        "task_name": "mwritescode/slither-audited-smart-contracts-big-multilabel",
        "instruction": "Given the deployed bytecode of a Solidity Smart Contract, can you identify its address?"
    },
    {
        "input_fields": [
            "source_code"
        ],
        "output_field": [
            "bytecode"
        ],
        "task_name": "mwritescode/slither-audited-smart-contracts-big-multilabel",
        "instruction": "Given the source code of a Solidity Smart Contract, can you generate its bytecode?"
    },
    {
        "input_fields": [
            "facts"
        ],
        "output_field": [
            "casename"
        ],
        "task_name": "lbox/lbox_open-statute_classification",
        "instruction": "Please provide the case name based on the given facts. Answers must be one of \uac15\uc81c\ucd94\ud589, \uacf5\ubb34\uc9d1\ud589\ubc29\ud574, \uacf5\ubb34\uc9d1\ud589\ubc29\ud574, \uc0c1\ud574."
    },
    {
        "input_fields": [
            "context",
            "service",
            "prompt"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/schema_guided_dialog",
        "instruction": "Given the context, service, and prompt, generate a target question that can be asked to the user."
    },
    {
        "input_fields": [
            "context",
            "service",
            "target"
        ],
        "output_field": [
            "prompt"
        ],
        "task_name": "GEM/schema_guided_dialog",
        "instruction": "Given the context, service, and target, generate a prompt that can be used to ask the user for more information."
    },
    {
        "input_fields": [
            "context",
            "service",
            "target"
        ],
        "output_field": [
            "linearized_input"
        ],
        "task_name": "GEM/schema_guided_dialog",
        "instruction": "Given the context, service, and target, generate a response that answers the target question."
    },
    {
        "input_fields": [
            "target"
        ],
        "output_field": [
            "meaning_representation"
        ],
        "task_name": "GEM/viggo",
        "instruction": "Please generate a meaning representation based on the target response."
    },
    {
        "input_fields": [
            "meaning_representation"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/viggo",
        "instruction": "Please generate a target response based on the meaning representation."
    },
    {
        "input_fields": [
            "post"
        ],
        "output_field": [
            "annotatorGender"
        ],
        "task_name": "social_bias_frames",
        "instruction": "Given a post, can you predict the annotator's gender? Answers must be one of woman, man."
    },
    {
        "input_fields": [
            "post"
        ],
        "output_field": [
            "annotatorAge"
        ],
        "task_name": "social_bias_frames",
        "instruction": "Given a post, can you predict the annotator's age?"
    },
    {
        "input_fields": [
            "post"
        ],
        "output_field": [
            "annotatorPolitics"
        ],
        "task_name": "social_bias_frames",
        "instruction": "Given a post, can you predict the annotator's political affiliation? Answers must be one of libert, mod-liberal, liberal, cons, other, mod-cons."
    },
    {
        "input_fields": [
            "domain",
            "seed_entity"
        ],
        "output_field": [
            "seed_entity_text"
        ],
        "task_name": "conv_questions",
        "instruction": "Given a domain and a seed entity, please provide the corresponding seed entity text."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-full",
        "instruction": "Given a situation and an intention, please generate a normative action that fulfills the intention and observes the norm."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "immoral_action"
        ],
        "task_name": "demelin/moral_stories-full",
        "instruction": "Given a situation and an intention, please generate a divergent action that fulfills the intention and diverges from the norm."
    },
    {
        "input_fields": [
            "moral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action-lexical_bias",
        "instruction": "Given a moral action, predict whether it is a normative or divergent action. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "intention",
            "moral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action-lexical_bias",
        "instruction": "Given a situation and an intention, predict whether the moral action is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action-lexical_bias",
        "instruction": "Given a moral action and its consequence, predict whether it is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "moral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action-minimal_pairs",
        "instruction": "Given a moral action, predict whether it is a normative or divergent action. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "immoral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action-minimal_pairs",
        "instruction": "Given an immoral action, predict whether it is a normative or divergent action. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action-minimal_pairs",
        "instruction": "Given a situation and an intention, predict whether the moral action is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "moral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action-norm_distance",
        "instruction": "Given a moral action, predict whether it is a normative or divergent action. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "intention",
            "moral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action-norm_distance",
        "instruction": "Given a situation and an intention, predict whether the moral action is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action-norm_distance",
        "instruction": "Given a moral action and its consequence, predict whether it is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "intention",
            "moral_action",
            "immoral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context-lexical_bias",
        "instruction": "Given a situation and an intention, please predict whether the action taken by the actor is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-cls-action+context-lexical_bias",
        "instruction": "Given a situation and an intention, please generate a normative action that fulfills the intention and observes the norm."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "immoral_action"
        ],
        "task_name": "demelin/moral_stories-cls-action+context-lexical_bias",
        "instruction": "Given a situation and an intention, please generate a divergent action that fulfills the intention and diverges from the norm."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "intention"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context-minimal_pairs",
        "instruction": "Given a norm and a situation, predict whether the actor will take a normative or divergent action to fulfill their intention. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence"
        ],
        "output_field": [
            "intention"
        ],
        "task_name": "demelin/moral_stories-cls-action+context-minimal_pairs",
        "instruction": "Given a normative action and its consequence, predict the intention of the actor."
    },
    {
        "input_fields": [
            "immoral_action",
            "immoral_consequence"
        ],
        "output_field": [
            "intention"
        ],
        "task_name": "demelin/moral_stories-cls-action+context-minimal_pairs",
        "instruction": "Given an immoral action and its consequence, predict the intention of the actor."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "moral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context-norm_distance",
        "instruction": "Given a norm and a situation, predict whether the moral action taken by the actor is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "intention"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-cls-action+context-norm_distance",
        "instruction": "Given a norm and a situation, generate a moral action that fulfills the intention and observes the norm."
    },
    {
        "input_fields": [
            "situation",
            "intention",
            "moral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-lexical_bias",
        "instruction": "Given a situation and an intention, predict whether the action taken by the actor is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-lexical_bias",
        "instruction": "Given a situation and an intention, generate a normative action that fulfills the intention and observes the norm."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "immoral_action"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-lexical_bias",
        "instruction": "Given a situation and an intention, generate a divergent action that fulfills the intention and diverges from the norm."
    },
    {
        "input_fields": [
            "situation",
            "intention",
            "moral_action",
            "immoral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-minimal_pairs",
        "instruction": "Given a situation and an intention, please predict whether the action taken by the actor is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-minimal_pairs",
        "instruction": "Given a situation and an intention, please generate a normative action that fulfills the intention and observes the norm."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "immoral_action"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-minimal_pairs",
        "instruction": "Given a situation and an intention, please generate a divergent action that fulfills the intention and diverges from the norm."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-norm_distance",
        "instruction": "Given a moral action and its consequence, predict whether it is a normative or divergent action. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-norm_distance",
        "instruction": "Given a situation and an intention, generate a moral action that fulfills the intention and observes the norm."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence"
        ],
        "output_field": [
            "immoral_action"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-norm_distance",
        "instruction": "Given a moral action and its consequence, generate an immoral action that fulfills the intention and diverges from the norm."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "intention",
            "moral_action",
            "immoral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+norm-lexical_bias",
        "instruction": "Given a norm, situation, and intention, predict whether the action taken by the actor is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "intention",
            "moral_action",
            "immoral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+norm-lexical_bias",
        "instruction": "Given a norm, situation, intention, and action, predict whether the action taken by the actor is moral or immoral. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "intention",
            "moral_action",
            "immoral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+norm-minimal_pairs",
        "instruction": "Given a norm, situation, and intention, predict whether the action taken by the actor is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "intention"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-cls-action+norm-minimal_pairs",
        "instruction": "Given a norm, situation, and intention, generate a moral action that fulfills the intention and observes the norm."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "intention"
        ],
        "output_field": [
            "immoral_action"
        ],
        "task_name": "demelin/moral_stories-cls-action+norm-minimal_pairs",
        "instruction": "Given a norm, situation, and intention, generate an immoral action that fulfills the intention and diverges from the norm."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "intention",
            "moral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+norm-norm_distance",
        "instruction": "Given a norm, situation, and intention, predict whether the moral action taken by the actor is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "intention",
            "label"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-cls-action+norm-norm_distance",
        "instruction": "Given a norm, situation, and intention, generate a moral action that is normative or divergent."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action-lexical_bias",
        "instruction": "Given a moral action and its consequence, predict whether the action is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence"
        ],
        "output_field": [
            "immoral_action",
            "immoral_consequence"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action-lexical_bias",
        "instruction": "Given a moral action and its consequence, generate an immoral action that leads to a divergent consequence."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "moral_action",
            "moral_consequence"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action-lexical_bias",
        "instruction": "Given a situation and an intention, generate a moral action that fulfills the intention and observes the norm."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action-norm_distance",
        "instruction": "Given a moral action and its consequence, predict whether the action is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "immoral_action",
            "immoral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action-norm_distance",
        "instruction": "Given an immoral action and its consequence, predict whether the action is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action-norm_distance",
        "instruction": "Given a situation and an intention, predict whether the action is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "intention",
            "moral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action+context-lexical_bias",
        "instruction": "Given a situation and an intention, predict whether the moral action taken by the actor is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action+context-lexical_bias",
        "instruction": "Given a situation and an intention, generate a normative action that fulfills the intention and observes the norm."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "immoral_action"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action+context-lexical_bias",
        "instruction": "Given a situation and an intention, generate a divergent action that fulfills the intention and diverges from the norm."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "intention",
            "moral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action+context-minimal_pairs",
        "instruction": "Given a norm, situation, and intention, determine whether the moral action taken by the actor is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "intention",
            "label"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action+context-minimal_pairs",
        "instruction": "Given a norm, situation, and intention, generate a moral action that is normative or divergent."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "intention",
            "label"
        ],
        "output_field": [
            "moral_consequence",
            "immoral_consequence"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action+context-minimal_pairs",
        "instruction": "Given a norm, situation, and intention, generate a moral consequence that is normative or divergent."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "moral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action+context-norm_distance",
        "instruction": "Given a norm and a situation, predict whether the moral action taken by the actor is normative or divergent. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "moral_action"
        ],
        "output_field": [
            "moral_consequence"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action+context-norm_distance",
        "instruction": "Given a norm, a situation, and a moral action, predict the moral consequence of the action."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "immoral_consequence"
        ],
        "output_field": [
            "immoral_action"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action+context-norm_distance",
        "instruction": "Given a norm, a situation, and an immoral consequence, predict the immoral action taken by the actor."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "intention"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-gen-action$context-norm_distance",
        "instruction": "Given a norm and a situation, please predict whether the actor will take a normative or divergent action to fulfill their intention. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "intention"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-gen-action$context-norm_distance",
        "instruction": "Given a norm, a situation, and an intention, please generate a moral action that fulfills the intention and observes the norm."
    },
    {
        "input_fields": [
            "norm",
            "situation",
            "intention"
        ],
        "output_field": [
            "immoral_action"
        ],
        "task_name": "demelin/moral_stories-gen-action$context-norm_distance",
        "instruction": "Given a norm, a situation, and an intention, please generate a divergent action that fulfills the intention and diverges from the norm."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-gen-action$context+consequence-norm_distance",
        "instruction": "Given a situation and an intention, please generate a moral action that fulfills the intention and observes the norm."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "immoral_action"
        ],
        "task_name": "demelin/moral_stories-gen-action$context+consequence-norm_distance",
        "instruction": "Given a situation and an intention, please generate a divergent action that fulfills the intention and diverges from the norm."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-gen-action$context+consequence-norm_distance",
        "instruction": "Given a moral action and its consequence, please determine whether the action is normative or not. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "norm",
            "situation"
        ],
        "output_field": [
            "moral_action",
            "immoral_action"
        ],
        "task_name": "demelin/moral_stories-gen-consequence$action-norm_distance",
        "instruction": "Given a norm and a situation, predict the moral or immoral action that would be taken."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-gen-consequence$action+context-norm_distance",
        "instruction": "Given a situation and an intention, generate a moral action that fulfills the intention and observes the norm."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-gen-norm$actions-norm_distance",
        "instruction": "Given a situation and an intention, generate a normative action."
    },
    {
        "input_fields": [
            "norm",
            "situation"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-gen-norm$actions+context-norm_distance",
        "instruction": "Given a norm and a situation, generate a moral action that is normative."
    },
    {
        "input_fields": [
            "norm",
            "situation"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-gen-norm$actions+context-norm_distance",
        "instruction": "Given a norm and a situation, generate a moral action that is divergent."
    },
    {
        "input_fields": [
            "norm",
            "situation"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-gen-norm$actions+context+consequences-norm_distance",
        "instruction": "Given a norm and a situation, generate a moral action that is normative."
    },
    {
        "input_fields": [
            "norm",
            "situation"
        ],
        "output_field": [
            "moral_consequence"
        ],
        "task_name": "demelin/moral_stories-gen-norm$actions+context+consequences-norm_distance",
        "instruction": "Given a norm and a situation, generate a moral consequence that is normative."
    },
    {
        "input_fields": [
            "func"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "code_x_glue_cc_defect_detection",
        "instruction": "Given a source code, identify whether it is an insecure code that may attack software systems, such as resource leaks, use-after-free vulnerabilities and DoS attack. Treat the task as binary classification (0/1), where 1 stands for insecure code and 0 for secure code. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "func"
        ],
        "output_field": [
            "project"
        ],
        "task_name": "code_x_glue_cc_defect_detection",
        "instruction": "Given a source code, predict the project it belongs to. Answers must be one of FFmpeg, qemu."
    },
    {
        "input_fields": [
            "summaries"
        ],
        "output_field": [
            "review_sents"
        ],
        "task_name": "opinosis",
        "instruction": "Given a summary, can you generate a review sentence?"
    },
    {
        "input_fields": [
            "chunk"
        ],
        "output_field": [
            "alignment_score"
        ],
        "task_name": "crd3",
        "instruction": "Given a chunk of dialogue, can you predict the alignment score?"
    },
    {
        "input_fields": [
            "chunk"
        ],
        "output_field": [
            "turn_start",
            "turn_end"
        ],
        "task_name": "crd3",
        "instruction": "Given a chunk of dialogue, can you predict the turn start and turn end?"
    },
    {
        "input_fields": [
            "turn_start",
            "turn_end"
        ],
        "output_field": [
            "alignment_score"
        ],
        "task_name": "crd3",
        "instruction": "Given a turn start and turn end, can you predict the alignment score?"
    },
    {
        "input_fields": [
            "conversationId"
        ],
        "output_field": [
            "respondentWorkerId",
            "initiatorWorkerId"
        ],
        "task_name": "re_dial",
        "instruction": "Given a conversation ID, please provide the IDs of the respondent and initiator workers."
    },
    {
        "input_fields": [
            "respondentWorkerId"
        ],
        "output_field": [
            "conversationId"
        ],
        "task_name": "re_dial",
        "instruction": "Given a respondent worker ID, please provide the conversation IDs where this worker was the respondent."
    },
    {
        "input_fields": [
            "initiatorWorkerId"
        ],
        "output_field": [
            "conversationId"
        ],
        "task_name": "re_dial",
        "instruction": "Given an initiator worker ID, please provide the conversation IDs where this worker was the initiator."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "GEM/squad_v2",
        "instruction": "Please generate a question based on the given context."
    },
    {
        "input_fields": [
            "context",
            "question"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/squad_v2",
        "instruction": "Please generate a target based on the given context and question."
    },
    {
        "input_fields": [
            "disfluent question",
            "context"
        ],
        "output_field": [
            "original question"
        ],
        "task_name": "disfl_qa",
        "instruction": "Given a disfluent question and its corresponding context, please generate the original question."
    },
    {
        "input_fields": [
            "title",
            "context"
        ],
        "output_field": [
            "disfluent question"
        ],
        "task_name": "disfl_qa",
        "instruction": "Given a title and context, please generate a disfluent question."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "filepath"
        ],
        "task_name": "eth_py150_open",
        "instruction": "Given a license, can you predict the file path of the code?"
    },
    {
        "input_fields": [
            "concepts"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/common_gen",
        "instruction": "Given a set of common concepts, generate a coherent sentence describing an everyday scenario using these concepts."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "multi_nli_mismatch",
        "instruction": "Given a premise and a hypothesis, determine whether the hypothesis is entailment, contradiction, or neutral. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "multi_nli_mismatch",
        "instruction": "Given a premise and a label, write a hypothesis that entails, contradicts, or is neutral to the premise."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "multi_nli_mismatch",
        "instruction": "Given a hypothesis and a label, write a premise that entails, contradicts, or is neutral to the hypothesis."
    },
    {
        "input_fields": [
            "source"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/cochrane-simplification",
        "instruction": "Given a source sentence, generate a simplified version of the sentence."
    },
    {
        "input_fields": [
            "paper_title",
            "paper_abstract"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/SciDuet",
        "instruction": "Given a paper title and abstract, predict the target of the paper (e.g. Translation Model vs Language Model)."
    },
    {
        "input_fields": [
            "target",
            "references"
        ],
        "output_field": [
            "paper_title",
            "paper_abstract"
        ],
        "task_name": "GEM/SciDuet",
        "instruction": "Given a target and a list of references, recommend a paper title and abstract that is related to the target and cites at least one of the references."
    },
    {
        "input_fields": [
            "object_group"
        ],
        "output_field": [
            "text",
            "template_group"
        ],
        "task_name": "corypaik/coda",
        "instruction": "Provide a template for the given object group."
    },
    {
        "input_fields": [
            "phrase1",
            "phrase2",
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "PiC/phrase_similarity",
        "instruction": "Please determine whether two multi-word noun phrases are semantically similar or not given the same context sentence. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "description"
        ],
        "output_field": [
            "difficulty"
        ],
        "task_name": "deepmind/code_contests",
        "instruction": "Given a programming problem description, provide a difficulty rating."
    },
    {
        "input_fields": [
            "prediction_ts",
            "beer_ABV",
            "beer_name",
            "beer_style",
            "review_appearance",
            "review_palette",
            "review_taste",
            "review_aroma",
            "text"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "arize-ai/beer_reviews_label_drift_neg",
        "instruction": "Predict the label of the beer review based on the given input fields. Answers must be one of neutral, negative, positive."
    },
    {
        "input_fields": [
            "beer_name",
            "beer_style",
            "review_appearance",
            "review_palette",
            "review_taste",
            "review_aroma",
            "text"
        ],
        "output_field": [
            "beer_ABV"
        ],
        "task_name": "arize-ai/beer_reviews_label_drift_neg",
        "instruction": "Predict the ABV of the beer based on the given input fields."
    },
    {
        "input_fields": [
            "beer_name",
            "review_appearance",
            "review_palette",
            "review_taste",
            "review_aroma",
            "text"
        ],
        "output_field": [
            "beer_style"
        ],
        "task_name": "arize-ai/beer_reviews_label_drift_neg",
        "instruction": "Predict the beer style based on the given input fields."
    },
    {
        "input_fields": [
            "task_name",
            "task_description",
            "code"
        ],
        "output_field": [
            "language_name"
        ],
        "task_name": "bigcode/rosetta-code",
        "instruction": "Given a task name, task description, and code, can you identify the programming language used?"
    },
    {
        "input_fields": [
            "task_name",
            "task_description",
            "code"
        ],
        "output_field": [
            "language_url"
        ],
        "task_name": "bigcode/rosetta-code",
        "instruction": "Given a task name, task description, and code, can you identify the URL of the programming language used?"
    },
    {
        "input_fields": [
            "source_text",
            "label"
        ],
        "output_field": [
            "reply_text"
        ],
        "task_name": "strombergnlp/rumoureval_2019",
        "instruction": "Given a source text and a label, generate a reply text that supports, denies, questions, or simply comments on the claim."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "EMBO/BLURB-BIOSSES",
        "instruction": "Given two sentences, predict the score of their semantic similarity."
    },
    {
        "input_fields": [
            "table_source",
            "sub_sentence",
            "question",
            "table_content"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "kasnerz/hitab",
        "instruction": "Given a sub-sentence and a table, please answer the question based on the table. The answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable."
    },
    {
        "input_fields": [
            "table_source",
            "sub_sentence",
            "table_content"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "kasnerz/hitab",
        "instruction": "Given a sub-sentence and a table, please write a question based on the table."
    },
    {
        "input_fields": [
            "nl_statement"
        ],
        "output_field": [
            "nl_proof",
            "formal_statement",
            "src_header"
        ],
        "task_name": "hoskinson-center/proofnet",
        "instruction": "Prove the theorem given the natural language statement."
    },
    {
        "input_fields": [
            "formal_statement"
        ],
        "output_field": [
            "nl_statement"
        ],
        "task_name": "hoskinson-center/proofnet",
        "instruction": "Write a natural language statement given the formal statement."
    },
    {
        "input_fields": [
            "nl_statement"
        ],
        "output_field": [
            "formal_statement"
        ],
        "task_name": "hoskinson-center/proofnet",
        "instruction": "Write a formal statement given the natural language statement."
    },
    {
        "input_fields": [
            "summary"
        ],
        "output_field": [
            "document"
        ],
        "task_name": "launch/gov_report-plain_text",
        "instruction": "Given a summary, generate a longer document that covers the salient points."
    },
    {
        "input_fields": [
            "summary"
        ],
        "output_field": [
            "document"
        ],
        "task_name": "launch/gov_report-plain_text_with_recommendations",
        "instruction": "Given a summary, please generate a document that covers all the salient words to be summarized."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "title"
        ],
        "task_name": "PiC/phrase_sense_disambiguation",
        "instruction": "Can you identify the document title based on the context?"
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lhoestq/custom_squad",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lhoestq/custom_squad",
        "instruction": "Given a question, can you generate a context that answers the question?"
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-sst2",
        "instruction": "Given a sentence, predict its sentiment label as positive or negative. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-mrpc",
        "instruction": "Determine whether two sentences are equivalent or not. Answers must be one of not_equivalent, equivalent."
    },
    {
        "input_fields": [
            "sentence2"
        ],
        "output_field": [
            "sentence1"
        ],
        "task_name": "evaluate/glue-ci-mrpc",
        "instruction": "Rewrite the first sentence to make it equivalent to the second sentence."
    },
    {
        "input_fields": [
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "evaluate/glue-ci-mrpc",
        "instruction": "Rewrite the second sentence to make it equivalent to the first sentence."
    },
    {
        "input_fields": [
            "question1",
            "question2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-qqp",
        "instruction": "Given two questions, determine whether they are duplicates or not. Answers must be one of duplicate, not_duplicate."
    },
    {
        "input_fields": [
            "question1"
        ],
        "output_field": [
            "question2"
        ],
        "task_name": "evaluate/glue-ci-qqp",
        "instruction": "Create a question based on a given question."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-stsb",
        "instruction": "Determine whether the two sentences are semantically equivalent or not."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-stsb",
        "instruction": "Given two sentences, predict the degree of similarity between them."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-mnli",
        "instruction": "Given a premise and a hypothesis, determine whether the hypothesis is entailment, contradiction, or neutral. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "evaluate/glue-ci-mnli",
        "instruction": "Given a premise and a label, write a hypothesis that is entailment, contradiction, or neutral to the premise."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "evaluate/glue-ci-mnli",
        "instruction": "Given a hypothesis and a label, write a premise that is entailment, contradiction, or neutral to the hypothesis."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-mnli_mismatched",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of entailment, neutral, contradiction."
    },
    {
        "input_fields": [
            "hypothesis"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "evaluate/glue-ci-mnli_mismatched",
        "instruction": "This task asks models to generate a premise that contradicts the hypothesis."
    },
    {
        "input_fields": [
            "premise"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "evaluate/glue-ci-mnli_mismatched",
        "instruction": "This task asks models to generate a hypothesis that contradicts the premise."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-mnli_matched",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "evaluate/glue-ci-mnli_matched",
        "instruction": "This task asks models to generate a hypothesis given a premise and a label."
    },
    {
        "input_fields": [
            "hypothesis"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "evaluate/glue-ci-mnli_matched",
        "instruction": "Can you write a premise that contradicts the hypothesis?"
    },
    {
        "input_fields": [
            "question",
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-qnli",
        "instruction": "Given a sentence, please answer the question if it is entailed by the sentence. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence",
            "label"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "evaluate/glue-ci-qnli",
        "instruction": "Given a sentence and a label, please write a question that is entailed by the sentence if the label is \"entailment\", or write a question that is not entailed by the sentence if the label is \"not_entailment\"."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-rte",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "label",
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "evaluate/glue-ci-rte",
        "instruction": "This task asks models to write a sentence that entails, contradicts to, or is neutral to the given sentence."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-rte",
        "instruction": "Can you identify the sentence that is more likely to be true? Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-wnli",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "label",
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "evaluate/glue-ci-wnli",
        "instruction": "This task asks models to write a sentence that entails, contradicts to, or is neutral to the given sentence."
    },
    {
        "input_fields": [
            "year",
            "name",
            "label"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "pile-of-law/eoir_privacy-eoir_privacy",
        "instruction": "Can you write a new text that has the same year and name but a different label?"
    },
    {
        "input_fields": [
            "document"
        ],
        "output_field": [
            "summary"
        ],
        "task_name": "polinaeterna/xsum",
        "instruction": "Given a news article, please write a one-sentence summary of the article."
    },
    {
        "input_fields": [
            "document",
            "summary"
        ],
        "output_field": [
            "id"
        ],
        "task_name": "polinaeterna/xsum",
        "instruction": "Given a news article and its summary, please identify the BBC ID of the article."
    },
    {
        "input_fields": [
            "word"
        ],
        "output_field": [
            "countrycode"
        ],
        "task_name": "quickdraw-raw",
        "instruction": "Can you predict the country code based on the drawing?"
    },
    {
        "input_fields": [
            "word"
        ],
        "output_field": [
            "countrycode"
        ],
        "task_name": "quickdraw-preprocessed_simplified_drawings",
        "instruction": "Can you predict the country code based on the drawing?"
    },
    {
        "input_fields": [
            "smiles"
        ],
        "output_field": [
            "selfies"
        ],
        "task_name": "zpn/clintox",
        "instruction": "Convert a SMILES representation to a SELFIES representation."
    },
    {
        "input_fields": [
            "comment"
        ],
        "output_field": [
            "number"
        ],
        "task_name": "usc-isi/WikiConvert",
        "instruction": "Given a comment, can you extract the number mentioned in it?"
    },
    {
        "input_fields": [
            "number"
        ],
        "output_field": [
            "comment"
        ],
        "task_name": "usc-isi/WikiConvert",
        "instruction": "Given a number, can you find the comment that mentions it?"
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "StatsGary/socialmedia-abuse",
        "instruction": "Classify the text as abusive or not abusive. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "masked",
            "spans"
        ],
        "output_field": [
            "full_text"
        ],
        "task_name": "beki/privy-small",
        "instruction": "Given the masked text, please fill in the blanks with the correct PII types."
    },
    {
        "input_fields": [
            "Question",
            "Context"
        ],
        "output_field": [
            "Answer"
        ],
        "task_name": "McGill-NLP/TopiOCQA",
        "instruction": "Answer the question based on the given context. If the answer is not available, please return \"UNANSWERABLE\"."
    },
    {
        "input_fields": [
            "Context"
        ],
        "output_field": [
            "Topic"
        ],
        "task_name": "McGill-NLP/TopiOCQA",
        "instruction": "Identify the topic of the given context."
    },
    {
        "input_fields": [
            "Question",
            "Context"
        ],
        "output_field": [
            "Topic_section"
        ],
        "task_name": "McGill-NLP/TopiOCQA",
        "instruction": "Identify the section of the given context that provides the answer to the question."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "arize-ai/ecommerce_reviews_with_language_drift",
        "instruction": "Given the text, predict the label of the review. Answers must be one of neutral, negative, positive."
    },
    {
        "input_fields": [
            "text",
            "product_category"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "arize-ai/ecommerce_reviews_with_language_drift",
        "instruction": "Given the text and the product category, predict the label of the review. Answers must be one of neutral, negative, positive."
    },
    {
        "input_fields": [
            "text",
            "reviewer_age"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "arize-ai/ecommerce_reviews_with_language_drift",
        "instruction": "Given the text and the reviewer age, predict the label of the review. Answers must be one of neutral, negative, positive."
    },
    {
        "input_fields": [
            "answer",
            "paragraph",
            "sentence"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qg_squadshifts-all",
        "instruction": "Please generate a question based on the given context and answer."
    },
    {
        "input_fields": [
            "answer",
            "paragraph"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "lmqg/qg_squadshifts-all",
        "instruction": "Please generate a sentence that contains the answer."
    },
    {
        "input_fields": [
            "answer"
        ],
        "output_field": [
            "paragraph"
        ],
        "task_name": "lmqg/qg_squadshifts-all",
        "instruction": "Please generate a paragraph that contains the answer."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qg_squadshifts-amazon",
        "instruction": "Please generate a question based on the given sentence."
    },
    {
        "input_fields": [
            "paragraph"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "lmqg/qg_squadshifts-amazon",
        "instruction": "Please generate a sentence based on the given paragraph."
    },
    {
        "input_fields": [
            "answer",
            "paragraph"
        ],
        "output_field": [
            "sentence_answer"
        ],
        "task_name": "lmqg/qg_squadshifts-amazon",
        "instruction": "Please generate a sentence that contains the answer."
    },
    {
        "input_fields": [
            "answer",
            "paragraph",
            "sentence"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qg_squadshifts-new_wiki",
        "instruction": "Please generate a question based on the given context and answer."
    },
    {
        "input_fields": [
            "answer",
            "paragraph",
            "question"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "lmqg/qg_squadshifts-new_wiki",
        "instruction": "Please generate a sentence that contains the answer."
    },
    {
        "input_fields": [
            "answer",
            "sentence",
            "question"
        ],
        "output_field": [
            "paragraph"
        ],
        "task_name": "lmqg/qg_squadshifts-new_wiki",
        "instruction": "Please generate a paragraph that contains the answer."
    },
    {
        "input_fields": [
            "paragraph",
            "answer"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qg_squadshifts-nyt",
        "instruction": "Please generate a question based on the given paragraph and answer."
    },
    {
        "input_fields": [
            "paragraph",
            "answer"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "lmqg/qg_squadshifts-nyt",
        "instruction": "Please generate a sentence based on the given paragraph and answer."
    },
    {
        "input_fields": [
            "sentence",
            "answer"
        ],
        "output_field": [
            "paragraph"
        ],
        "task_name": "lmqg/qg_squadshifts-nyt",
        "instruction": "Please generate a paragraph based on the given sentence and answer."
    },
    {
        "input_fields": [
            "answer",
            "paragraph"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qg_squadshifts-reddit",
        "instruction": "Please generate a question based on the given paragraph and answer."
    },
    {
        "input_fields": [
            "question",
            "paragraph"
        ],
        "output_field": [
            "sentence_answer"
        ],
        "task_name": "lmqg/qg_squadshifts-reddit",
        "instruction": "Please generate a sentence that answers the given question."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "paragraph_answer"
        ],
        "task_name": "lmqg/qg_squadshifts-reddit",
        "instruction": "Please generate a paragraph that answers the given question."
    },
    {
        "input_fields": [
            "query"
        ],
        "output_field": [
            "query_type"
        ],
        "task_name": "din0s/msmarco-nlgen",
        "instruction": "Determine the type of the query. Answers must be one of NUMERIC, DESCRIPTION, PERSON, ENTITY, LOCATION."
    },
    {
        "input_fields": [
            "answers"
        ],
        "output_field": [
            "query"
        ],
        "task_name": "din0s/msmarco-nlgen",
        "instruction": "Generate a query based on the answer."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-sst2",
        "instruction": "Determine whether the sentiment of the sentence is positive or negative. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-mrpc",
        "instruction": "Determine whether two sentences are equivalent or not. Answers must be one of not_equivalent, equivalent."
    },
    {
        "input_fields": [
            "sentence2"
        ],
        "output_field": [
            "sentence1"
        ],
        "task_name": "mariosasko/glue-mrpc",
        "instruction": "Rewrite the first sentence to make it equivalent to the second sentence."
    },
    {
        "input_fields": [
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "mariosasko/glue-mrpc",
        "instruction": "Rewrite the second sentence to make it equivalent to the first sentence."
    },
    {
        "input_fields": [
            "question1",
            "question2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-qqp",
        "instruction": "Determine whether two questions are duplicates or not. Answers must be one of duplicate, not_duplicate."
    },
    {
        "input_fields": [
            "question1"
        ],
        "output_field": [
            "question2"
        ],
        "task_name": "mariosasko/glue-qqp",
        "instruction": "Given a question, generate a similar question."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-stsb",
        "instruction": "Given two sentences, please predict the similarity score between them."
    },
    {
        "input_fields": [
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "mariosasko/glue-stsb",
        "instruction": "Given a sentence, please generate a similar sentence with a higher similarity score."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-mnli",
        "instruction": "Given a premise and a hypothesis, determine whether the hypothesis is entailment, contradiction, or neutral to the premise. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "label",
            "hypothesis"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "mariosasko/glue-mnli",
        "instruction": "Create a premise that entails, contradicts, or is neutral to the hypothesis."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "mariosasko/glue-mnli",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailment, contradiction, or neutral to the premise."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-mnli_mismatched",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of entailment, neutral, contradiction."
    },
    {
        "input_fields": [
            "premise"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "mariosasko/glue-mnli_mismatched",
        "instruction": "This task asks models to generate a hypothesis that contradicts the premise."
    },
    {
        "input_fields": [
            "hypothesis"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "mariosasko/glue-mnli_mismatched",
        "instruction": "This task asks models to generate a premise that entails the hypothesis."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-mnli_matched",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "mariosasko/glue-mnli_matched",
        "instruction": "This task asks models to generate a hypothesis given a premise and a label."
    },
    {
        "input_fields": [
            "hypothesis"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "mariosasko/glue-mnli_matched",
        "instruction": "Can you write a premise that contradicts the hypothesis?"
    },
    {
        "input_fields": [
            "question",
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-qnli",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "mariosasko/glue-qnli",
        "instruction": "Create a sentence that is entailed by the question."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "mariosasko/glue-qnli",
        "instruction": "Create a question that entails the sentence."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-rte",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "label",
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "mariosasko/glue-rte",
        "instruction": "This task asks models to write a sentence that entails, contradicts to, or is neutral to the given sentence."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-wnli",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "label",
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "mariosasko/glue-wnli",
        "instruction": "This task asks models to write a sentence that entails, contradicts to, or is neutral to the given sentence."
    },
    {
        "input_fields": [
            "img_bytes"
        ],
        "output_field": [
            "facial_hair"
        ],
        "task_name": "cgarciae/cartoonset-100k+features",
        "instruction": "Given the cartoon avatar image, predict the category of the facial hair."
    },
    {
        "input_fields": [
            "img_bytes"
        ],
        "output_field": [
            "hair_color"
        ],
        "task_name": "cgarciae/cartoonset-100k+features",
        "instruction": "Given the cartoon avatar image, predict the category of the hair color. Answers must be one of 9, 8, 7, 0, 3, 6, 4, 5, 2, 1."
    },
    {
        "input_fields": [
            "img_bytes"
        ],
        "output_field": [
            "glasses_color"
        ],
        "task_name": "cgarciae/cartoonset-100k+features",
        "instruction": "Given the cartoon avatar image, predict the category of the glasses color. Answers must be one of 0, 3, 6, 4, 5, 2, 1."
    },
    {
        "input_fields": [
            "nl",
            "canonical_cmd"
        ],
        "output_field": [
            "cmd"
        ],
        "task_name": "neulab/docprompting-conala-data",
        "instruction": "Given natural language and canonical command, generate the corresponding code command."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "mbartolo/synQA",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "Path"
        ],
        "output_field": [
            "Height"
        ],
        "task_name": "bazyl/GTSRB",
        "instruction": "Given an image, predict the height of the traffic sign."
    },
    {
        "input_fields": [
            "Path"
        ],
        "output_field": [
            "Roi.X1",
            "Roi.Y1",
            "Roi.X2",
            "Roi.Y2"
        ],
        "task_name": "bazyl/GTSRB",
        "instruction": "Given an image, predict the coordinates of the bounding box around the traffic sign. Answers must be one of 9, 8, 7, 6, 5, 10, 12, 15."
    },
    {
        "input_fields": [
            "smiles"
        ],
        "output_field": [
            "selfies"
        ],
        "task_name": "zpn/zinc20",
        "instruction": "Given a SMILES string, convert it to a SELFIES string."
    },
    {
        "input_fields": [
            "selfies"
        ],
        "output_field": [
            "smiles"
        ],
        "task_name": "zpn/zinc20",
        "instruction": "Given a SELFIES string, convert it to a SMILES string."
    },
    {
        "input_fields": [
            "color"
        ],
        "output_field": [
            "normal"
        ],
        "task_name": "dream-textures/textures-color-normal-1k",
        "instruction": "Given a color image, generate a normal map texture."
    },
    {
        "input_fields": [
            "normal"
        ],
        "output_field": [
            "color"
        ],
        "task_name": "dream-textures/textures-color-normal-1k",
        "instruction": "Given a normal map texture, generate a color image."
    },
    {
        "input_fields": [
            "date"
        ],
        "output_field": [
            "title"
        ],
        "task_name": "LLukas22/NLQuAD",
        "instruction": "Given a date, please provide the corresponding news article title."
    },
    {
        "input_fields": [
            "title"
        ],
        "output_field": [
            "date"
        ],
        "task_name": "LLukas22/NLQuAD",
        "instruction": "Given a news article title, please provide the corresponding date."
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "rajistics/auditor_review",
        "instruction": "Given a label, generate a sentence with the corresponding sentiment."
    },
    {
        "input_fields": [
            "split_text"
        ],
        "output_field": [
            "prediction_ts"
        ],
        "task_name": "arize-ai/xtreme_en_token_drift",
        "instruction": "Predict the timestamp of when the inference took place."
    },
    {
        "input_fields": [
            "type",
            "subreddit.name",
            "subreddit.nsfw",
            "created_utc",
            "permalink",
            "domain",
            "url",
            "selftext",
            "title"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/reddit-r-bitcoin-data-for-jun-2022-posts",
        "instruction": "Given a post, predict its score."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "sagnikrayc/snli-cf-kaushik",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "sagnikrayc/snli-cf-kaushik",
        "instruction": "Can you generate a counterfactual hypothesis given the premise?"
    },
    {
        "input_fields": [
            "hypothesis"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "sagnikrayc/snli-cf-kaushik",
        "instruction": "Can you generate a counterfactual premise given the hypothesis?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_phonearena-com",
        "instruction": "Given the hardware information of two phones, please compare and determine which phone has a better system chip."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_phonearena-com",
        "instruction": "Given the hardware information of two phones, please compare and determine which phone has a better processor."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_wkdu-org",
        "instruction": "Given the album and title, please predict the artist."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster05",
        "instruction": "Given the input, predict the output. Answers must be one of We don\u2019t share, We Don't Share, We don't share, No, Yes, WE DON\u2019T SHARE, NO."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_rated-low",
        "instruction": "Given the input, please predict the output."
    },
    {
        "input_fields": [
            "texts"
        ],
        "output_field": [
            "avg_score"
        ],
        "task_name": "tomekkorbak/pile-pii-scrubadub",
        "instruction": "Given a document, predict the average score of PII in each sentence."
    },
    {
        "input_fields": [
            "year"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "severo/embellishments",
        "instruction": "Given the year, return all the images in the dataset that were published in that year."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "fname"
        ],
        "task_name": "severo/embellishments",
        "instruction": "Given the path of an image, return the filename."
    },
    {
        "input_fields": [
            "task"
        ],
        "output_field": [
            "transcriptions",
            "performers",
            "gt"
        ],
        "task_name": "toloka/CrowdSpeech",
        "instruction": "Transcribe the audio file."
    },
    {
        "input_fields": [
            "transcriptions"
        ],
        "output_field": [
            "gt"
        ],
        "task_name": "toloka/CrowdSpeech",
        "instruction": "Identify the correct transcription among the given transcriptions."
    },
    {
        "input_fields": [
            "performers"
        ],
        "output_field": [
            "gt"
        ],
        "task_name": "toloka/CrowdSpeech",
        "instruction": "Identify the performer who transcribed the audio file."
    },
    {
        "input_fields": [
            "hashtag"
        ],
        "output_field": [
            "segmentation"
        ],
        "task_name": "ruanchaves/stan_large",
        "instruction": "Given a hashtag, provide the associated tweet."
    },
    {
        "input_fields": [
            "segmentation"
        ],
        "output_field": [
            "hashtag"
        ],
        "task_name": "ruanchaves/stan_large",
        "instruction": "Given a tweet, provide the associated hashtag."
    },
    {
        "input_fields": [
            "type",
            "subreddit.name",
            "subreddit.nsfw",
            "created_utc",
            "permalink",
            "domain",
            "url",
            "selftext",
            "title"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/the-antiwork-subreddit-dataset-posts",
        "instruction": "Given a post, predict its score."
    },
    {
        "input_fields": [
            "permalink"
        ],
        "output_field": [
            "domain"
        ],
        "task_name": "SocialGrep/the-reddit-dataset-dataset-posts",
        "instruction": "Identify the domain of the post."
    },
    {
        "input_fields": [
            "subreddit.name"
        ],
        "output_field": [
            "body",
            "score"
        ],
        "task_name": "SocialGrep/the-reddit-dataset-dataset-comments",
        "instruction": "Given a subreddit name, find the comments with the highest score."
    },
    {
        "input_fields": [
            "sentiment"
        ],
        "output_field": [
            "body",
            "score"
        ],
        "task_name": "SocialGrep/the-reddit-dataset-dataset-comments",
        "instruction": "Given a sentiment score, find the comments with the highest sentiment."
    },
    {
        "input_fields": [
            "created_utc"
        ],
        "output_field": [
            "body",
            "score"
        ],
        "task_name": "SocialGrep/the-reddit-dataset-dataset-comments",
        "instruction": "Given a time range, find the comments created within that range."
    },
    {
        "input_fields": [
            "body"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/the-reddit-place-dataset-comments",
        "instruction": "Can you predict the score of a comment?"
    },
    {
        "input_fields": [
            "body"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/the-reddit-irl-dataset-comments",
        "instruction": "Given a comment, predict its score."
    },
    {
        "input_fields": [
            "type",
            "subreddit.nsfw",
            "created_utc",
            "permalink",
            "domain",
            "url",
            "selftext",
            "title",
            "score"
        ],
        "output_field": [
            "subreddit.name"
        ],
        "task_name": "SocialGrep/the-reddit-nft-dataset-posts",
        "instruction": "Given a post, predict its subreddit."
    },
    {
        "input_fields": [
            "paragraph",
            "question"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "lmqg/qg_subjqa-all",
        "instruction": "Given a paragraph and a question, generate a sentence that answers the question."
    },
    {
        "input_fields": [
            "paragraph",
            "sentence_answer"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qg_subjqa-all",
        "instruction": "Given a paragraph and a sentence that answers a question, generate the question."
    },
    {
        "input_fields": [
            "paragraph",
            "question"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "lmqg/qg_subjqa-books",
        "instruction": "Given a paragraph and a question, generate a sentence that answers the question."
    },
    {
        "input_fields": [
            "paragraph",
            "sentence_answer"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qg_subjqa-books",
        "instruction": "Given a paragraph and a sentence that answers a question, generate the question."
    },
    {
        "input_fields": [
            "paragraph",
            "question"
        ],
        "output_field": [
            "sentence_answer"
        ],
        "task_name": "lmqg/qg_subjqa-electronics",
        "instruction": "Given a paragraph and a question, generate a sentence that answers the question."
    },
    {
        "input_fields": [
            "paragraph",
            "question"
        ],
        "output_field": [
            "paragraph_question"
        ],
        "task_name": "lmqg/qg_subjqa-electronics",
        "instruction": "Given a paragraph and a question, generate a question that can be answered by a sentence in the paragraph."
    },
    {
        "input_fields": [
            "sentence",
            "question"
        ],
        "output_field": [
            "paragraph_sentence"
        ],
        "task_name": "lmqg/qg_subjqa-electronics",
        "instruction": "Given a sentence and a question, generate a paragraph that contains the sentence and answers the question."
    },
    {
        "input_fields": [
            "answer",
            "paragraph_question"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qg_subjqa-grocery",
        "instruction": "Please generate a question based on the given answer and context."
    },
    {
        "input_fields": [
            "paragraph"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "lmqg/qg_subjqa-grocery",
        "instruction": "Please generate a sentence based on the given paragraph."
    },
    {
        "input_fields": [
            "answer"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "lmqg/qg_subjqa-grocery",
        "instruction": "Please generate a sentence based on the given answer."
    },
    {
        "input_fields": [
            "paragraph"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qg_subjqa-movies",
        "instruction": "Please generate a question based on the given paragraph."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "lmqg/qg_subjqa-movies",
        "instruction": "Please generate a sentence that answers the given question."
    },
    {
        "input_fields": [
            "sentence",
            "paragraph"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qg_subjqa-restaurants",
        "instruction": "Given a sentence or paragraph, generate a question based on the context."
    },
    {
        "input_fields": [
            "sentence",
            "paragraph"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "lmqg/qg_subjqa-restaurants",
        "instruction": "Given a sentence or paragraph, generate an answer based on the context."
    },
    {
        "input_fields": [
            "question",
            "sentence",
            "paragraph"
        ],
        "output_field": [
            "sentence_answer",
            "paragraph_answer"
        ],
        "task_name": "lmqg/qg_subjqa-restaurants",
        "instruction": "Given a sentence or paragraph, generate a sentence or paragraph that answers a given question."
    },
    {
        "input_fields": [
            "answer",
            "paragraph"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qg_subjqa-tripadvisor",
        "instruction": "Please generate a question based on the given paragraph and answer."
    },
    {
        "input_fields": [
            "paragraph"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "lmqg/qg_subjqa-tripadvisor",
        "instruction": "Please generate a sentence based on the given paragraph."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "paragraph"
        ],
        "task_name": "lmqg/qg_subjqa-tripadvisor",
        "instruction": "Please generate a paragraph based on the given sentence."
    },
    {
        "input_fields": [
            "pixel_values"
        ],
        "output_field": [
            "num_patches"
        ],
        "task_name": "Team-PIXEL/rendered-wikipedia-english",
        "instruction": "Given a pixel image of a Wikipedia article, can you predict the number of patches in the image?"
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "pub_date"
        ],
        "task_name": "Livingwithmachines/hmd-erwt-training",
        "instruction": "Given the text, can you predict the publication date of the article?"
    },
    {
        "input_fields": [
            "answer"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "launch/reddit_qg",
        "instruction": "Given an answer, please generate a question that could lead to this answer."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "launch/reddit_qg",
        "instruction": "Given a question, please generate an answer that could fit the question."
    },
    {
        "input_fields": [
            "question",
            "answer"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "launch/reddit_qg",
        "instruction": "Given a question-answer pair, please predict the score of the answer based on the question."
    },
    {
        "input_fields": [
            "split_text"
        ],
        "output_field": [
            "prediction_ts"
        ],
        "task_name": "arize-ai/xtreme_en",
        "instruction": "Predict the timestamp when the inference took place."
    },
    {
        "input_fields": [
            "split_text"
        ],
        "output_field": [
            "prediction_ts"
        ],
        "task_name": "arize-ai/xtreme_en_language_drift_es",
        "instruction": "Predict the timestamp when the inference took place."
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_full",
        "instruction": "Given the task and input, output the output column name."
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "MicPie/unpredictable_full",
        "instruction": "Given the task and input, output the URL of the webpage."
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "MicPie/unpredictable_full",
        "instruction": "Given the task and input, output the webpage title."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_baseball-fantasysports-yahoo-com",
        "instruction": "Given the input, what is the position of the player?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_baseball-fantasysports-yahoo-com",
        "instruction": "What is the meaning of the given input?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_mgoblog-com",
        "instruction": "Given the input text, predict the output text."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "MicPie/unpredictable_mgoblog-com",
        "instruction": "Given the input text, predict the page title."
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_gamefaqs-com",
        "instruction": "Given the task and input, provide the output."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_sittercity-com",
        "instruction": "Given the input schedule, determine the earliest time period the care provider is available. Answers must be one of 9-12 PM, 6-9 PM, 3-6 PM, 6-9 AM, 12-3 PM, 9-12 AM."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_sittercity-com",
        "instruction": "Given the input schedule, determine the latest time period the care provider is available. Answers must be one of 9-12 PM, 6-9 PM, 3-6 PM, 6-9 AM, 12-3 PM, 9-12 AM."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cappex-com",
        "instruction": "Given a comment, predict the category it belongs to."
    },
    {
        "input_fields": [
            "output"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "MicPie/unpredictable_cappex-com",
        "instruction": "Given a category, write a comment that belongs to it."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_en-wikipedia-org",
        "instruction": "Given the year, award category and result, predict the production."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_en-wikipedia-org",
        "instruction": "Given the production, year and award category, predict the result."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cram-com",
        "instruction": "Given the input, please write a question that matches the output."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output",
            "options"
        ],
        "task_name": "MicPie/unpredictable_cram-com",
        "instruction": "Given the input, please write a multiple-choice question with options that matches the output."
    },
    {
        "input_fields": [
            "input",
            "output"
        ],
        "output_field": [
            "task"
        ],
        "task_name": "MicPie/unpredictable_sporcle-com",
        "instruction": "Given the input and output of a Sporcle quiz, can you identify the correct quiz?"
    },
    {
        "input_fields": [
            "input",
            "output"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_sporcle-com",
        "instruction": "Given the input and output of a Sporcle quiz, can you identify the output column name?"
    },
    {
        "input_fields": [
            "input",
            "output"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "MicPie/unpredictable_sporcle-com",
        "instruction": "Given the input and output of a Sporcle quiz, can you identify the URL of the quiz?"
    },
    {
        "input_fields": [
            "task"
        ],
        "output_field": [
            "input",
            "output"
        ],
        "task_name": "MicPie/unpredictable_ensembl-org",
        "instruction": "Given the transcript ID, please provide the corresponding protein ID."
    },
    {
        "input_fields": [
            "task"
        ],
        "output_field": [
            "input",
            "output"
        ],
        "task_name": "MicPie/unpredictable_ensembl-org",
        "instruction": "Can you provide the biotype of the transcript?"
    },
    {
        "input_fields": [
            "task"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_ensembl-org",
        "instruction": "What is the GENCODE set for human and mouse?"
    },
    {
        "input_fields": [
            "title",
            "selftext"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/one-year-of-tsla-on-reddit-posts",
        "instruction": "Predict the score of a post based on its title and selftext."
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_unique",
        "instruction": "Given the task and input format, please write the output format."
    },
    {
        "input_fields": [
            "input",
            "outputColName"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster-noise",
        "instruction": "Given the input and output column names, can you predict the output value?"
    },
    {
        "input_fields": [
            "input",
            "outputColName",
            "task"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster-noise",
        "instruction": "Can you predict the output value given the input column name and the time of day?"
    },
    {
        "input_fields": [
            "input",
            "outputColName",
            "task"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster-noise",
        "instruction": "Can you predict the output value given the input column name and the day of the week?"
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "MicPie/unpredictable_cluster00",
        "instruction": "Given the URL of a webpage, can you extract the page title?"
    },
    {
        "input_fields": [
            "url",
            "outputColName"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster00",
        "instruction": "Given the URL of a webpage and a column name, can you extract the value of the column? Answers must be one of Yes, YES, No, NO."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster00",
        "instruction": "Can you determine if a website shares user information for marketing purposes? Answers must be one of Yes, YES, No, NO."
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster10",
        "instruction": "Given the task and input, please provide the output."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster12",
        "instruction": "Given the input text, please generate the output text."
    },
    {
        "input_fields": [
            "input",
            "pageTitle",
            "url"
        ],
        "output_field": [
            "wdcFile"
        ],
        "task_name": "MicPie/unpredictable_cluster12",
        "instruction": "Can you identify the website where the input text comes from?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_cluster12",
        "instruction": "Can you identify the version of the King James Version of the Bible based on the input text?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_cluster14",
        "instruction": "Given the input, predict the output column name. Answers must be one of Event, Status, Who is Eligible, What it Means, Effect, Description, Parent, Stat, Spec."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster14",
        "instruction": "Given the input, predict the type of the output."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_cluster15",
        "instruction": "Given the input, write the output column name."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster16",
        "instruction": "Given the tourist attraction category, type, venue name, price range, rating, and value rating, please predict the hours of operation."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "task"
        ],
        "task_name": "MicPie/unpredictable_cluster16",
        "instruction": "Given the tourist attraction category, type, venue name, price range, rating, value rating, and hours of operation, please predict the tourist attraction category."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster17",
        "instruction": "Given the input, please provide the output."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster18",
        "instruction": "Given the input, please write a skincare product that matches the description."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster18",
        "instruction": "Given the input, please write a description of a skincare product that matches the description."
    },
    {
        "input_fields": [
            "pageTitle"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster18",
        "instruction": "Given the input, please write a webpage title that matches the description."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "MicPie/unpredictable_cluster19",
        "instruction": "Given the input description, output the corresponding webpage title."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster19",
        "instruction": "Given the input description, output the type of the event or job."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_cluster19",
        "instruction": "Given the input description, output the type of the contact."
    },
    {
        "input_fields": [
            "outputColName",
            "input"
        ],
        "output_field": [
            "task"
        ],
        "task_name": "MicPie/unpredictable_cluster02",
        "instruction": "Given the input and output columns, can you identify the directive?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster02",
        "instruction": "Can you predict the output given the directive and the local value?"
    },
    {
        "input_fields": [
            "input",
            "outputColName"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster20",
        "instruction": "Given the input and output columns, predict the value of the output column."
    },
    {
        "input_fields": [
            "input",
            "outputColName"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "MicPie/unpredictable_cluster20",
        "instruction": "Given the input and output columns, predict the webpage title."
    },
    {
        "input_fields": [
            "input",
            "outputColName"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "MicPie/unpredictable_cluster20",
        "instruction": "Given the input and output columns, predict the URL of the webpage."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_cluster21",
        "instruction": "Given the input, please provide the output column name."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster21",
        "instruction": "Given the input, please provide the output."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "task"
        ],
        "task_name": "MicPie/unpredictable_cluster21",
        "instruction": "Given the input, please provide the task name."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster22",
        "instruction": "Given a list of words, please select the word that is most similar in meaning to the output word."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster22",
        "instruction": "Given a list of words, please select the word that is least similar in meaning to the output word."
    },
    {
        "input_fields": [
            "input",
            "output"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "MicPie/unpredictable_cluster23",
        "instruction": "Given the input and output columns, can you identify the website?"
    },
    {
        "input_fields": [
            "input",
            "output"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "MicPie/unpredictable_cluster23",
        "instruction": "Given the input and output columns, can you identify the page title?"
    },
    {
        "input_fields": [
            "input",
            "output"
        ],
        "output_field": [
            "task"
        ],
        "task_name": "MicPie/unpredictable_cluster23",
        "instruction": "Given the input and output columns, can you identify the task name?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster24",
        "instruction": "Given the input list, please output the corresponding output."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster25",
        "instruction": "Given the artist and album, please predict the title of the song."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_cluster27",
        "instruction": "Given the input, please output the output column name."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "MicPie/unpredictable_cluster27",
        "instruction": "Given the input, please output the URL."
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_cluster28",
        "instruction": "Given the task and input, please provide the output column name."
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "MicPie/unpredictable_cluster28",
        "instruction": "Given the task and input, please provide the webpage title."
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster03",
        "instruction": "Given the task and the input, please provide the output."
    },
    {
        "input_fields": [
            "task"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "MicPie/unpredictable_cluster03",
        "instruction": "Can you provide the URL of the webpage where the task is from?"
    },
    {
        "input_fields": [
            "task",
            "outputColName"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "MicPie/unpredictable_cluster03",
        "instruction": "What is the name of the cookie?"
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "MicPie/unpredictable_cluster04",
        "instruction": "Given the webpage URL, extract the page title."
    },
    {
        "input_fields": [
            "url",
            "outputColName"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster04",
        "instruction": "Given the webpage URL and the output column name, extract the value of the output column. Answers must be one of We don\u2019t share, We Don't Share, We don't share, No, Yes, YES, NO."
    },
    {
        "input_fields": [
            "url",
            "task"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "MicPie/unpredictable_cluster04",
        "instruction": "Given the webpage URL and the input column name, extract the value of the input column."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_cluster06",
        "instruction": "Given the input, can you extract the output column name?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster06",
        "instruction": "Given the input, can you extract the output value?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "MicPie/unpredictable_cluster06",
        "instruction": "Given the input, can you extract the webpage title?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster07",
        "instruction": "Given the name of a cookie and its description, can you predict its expiration time?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "MicPie/unpredictable_cluster07",
        "instruction": "Given the name of a cookie and its description, can you predict the website it belongs to?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_cluster07",
        "instruction": "Given the name of a cookie and its description, can you predict the column name where it is stored?"
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "MicPie/unpredictable_cluster08",
        "instruction": "Given a webpage URL, extract the FAQ page title."
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "MicPie/unpredictable_rated-medium",
        "instruction": "Given the URL of a webpage, extract the page title."
    },
    {
        "input_fields": [
            "url",
            "outputColName"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_rated-medium",
        "instruction": "Given the URL of a webpage and a column name, extract the value of the column."
    },
    {
        "input_fields": [
            "url",
            "input"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_rated-medium",
        "instruction": "Given the URL of a webpage and a column name, extract the input field."
    },
    {
        "input_fields": [
            "sentence",
            "context",
            "target"
        ],
        "output_field": [
            "animacy"
        ],
        "task_name": "biglam/atypical_animacy",
        "instruction": "Given a sentence and its context, please predict the animacy of the target. Answers must be one of 1.0, 0.0."
    },
    {
        "input_fields": [
            "utterance",
            "response"
        ],
        "output_field": [
            "incoherent_implicature"
        ],
        "task_name": "UCL-DARK/ludwig-0-shot",
        "instruction": "Given an utterance and a response, determine whether the response is coherent with the utterance or not. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "utterance",
            "response"
        ],
        "output_field": [
            "implicature"
        ],
        "task_name": "UCL-DARK/ludwig-0-shot",
        "instruction": "Given an utterance and a response, determine whether the response implies something or not. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "utterance"
        ],
        "output_field": [
            "response"
        ],
        "task_name": "UCL-DARK/ludwig-0-shot",
        "instruction": "Given an utterance, generate a response that implies something."
    },
    {
        "input_fields": [
            "utterance"
        ],
        "output_field": [
            "response"
        ],
        "task_name": "UCL-DARK/ludwig-1-shot",
        "instruction": "Given an utterance and a response, generate a new response that is coherent with the implicature."
    },
    {
        "input_fields": [
            "utterance",
            "response"
        ],
        "output_field": [
            "incoherent_implicature"
        ],
        "task_name": "UCL-DARK/ludwig-5-shot",
        "instruction": "Given an utterance and a response, determine whether the response is coherent with the utterance or not. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "utterance",
            "response"
        ],
        "output_field": [
            "implicature"
        ],
        "task_name": "UCL-DARK/ludwig-5-shot",
        "instruction": "Given an utterance and a response, determine whether the response implies something or not. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "utterance"
        ],
        "output_field": [
            "response"
        ],
        "task_name": "UCL-DARK/ludwig-5-shot",
        "instruction": "Given an utterance, generate a response that implies something."
    },
    {
        "input_fields": [
            "utterance",
            "response"
        ],
        "output_field": [
            "incoherent_implicature"
        ],
        "task_name": "UCL-DARK/ludwig-15-shot",
        "instruction": "Given an utterance and a response, determine whether the response is coherent with the utterance or not. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "utterance",
            "response"
        ],
        "output_field": [
            "implicature"
        ],
        "task_name": "UCL-DARK/ludwig-15-shot",
        "instruction": "Given an utterance and a response, determine whether the response implies something or not. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "utterance"
        ],
        "output_field": [
            "response"
        ],
        "task_name": "UCL-DARK/ludwig-15-shot",
        "instruction": "Given an utterance, generate a response that implies something."
    },
    {
        "input_fields": [
            "task"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "unpredictable/unpredictable_full",
        "instruction": "Given the task description, please provide the corresponding website URL."
    },
    {
        "input_fields": [
            "task"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "unpredictable/unpredictable_full",
        "instruction": "Given the task description, please provide the corresponding input field."
    },
    {
        "input_fields": [
            "task"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "unpredictable/unpredictable_full",
        "instruction": "Given the task description, please provide the corresponding output field."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "unpredictable/unpredictable_5k",
        "instruction": "Given the input text, can you identify the website it comes from?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "unpredictable/unpredictable_5k",
        "instruction": "Given the input text, can you identify the website URL it comes from?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "wdcFile"
        ],
        "task_name": "unpredictable/unpredictable_5k",
        "instruction": "Given the input text, can you identify the website file name it comes from?"
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "unpredictable/unpredictable_unique",
        "instruction": "Given the task and input, please provide the output."
    },
    {
        "input_fields": [
            "selftext",
            "title"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/the-reddit-climate-change-dataset-posts",
        "instruction": "Predict the score of the post based on its content."
    },
    {
        "input_fields": [
            "title",
            "date",
            "close",
            "pct_change"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "Tidrael/tsl_news",
        "instruction": "Given the title, date, close, and percent change of a news article, predict the label (positive, negative, or neutral). Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "title",
            "date"
        ],
        "output_field": [
            "close"
        ],
        "task_name": "Tidrael/tsl_news",
        "instruction": "Given the title and date of a news article, predict the closing price of the stock. Answers must be one of 300.8, 302.61, 309.07, 249.44, 275.33."
    },
    {
        "input_fields": [
            "title",
            "date"
        ],
        "output_field": [
            "pct_change"
        ],
        "task_name": "Tidrael/tsl_news",
        "instruction": "Given the title and date of a news article, predict the percent change of the stock. Answers must be one of -4.59, -2.57, 2.9, 3.59, 1.89."
    },
    {
        "input_fields": [
            "prediction_ts",
            "beer_ABV",
            "beer_name",
            "beer_style",
            "review_appearance",
            "review_palette",
            "review_taste",
            "review_aroma",
            "text"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "arize-ai/beer_reviews_label_drift_neutral",
        "instruction": "Predict the label of the beer review based on the given input fields. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "prediction_ts",
            "beer_ABV",
            "beer_name",
            "review_appearance",
            "review_palette",
            "review_taste",
            "review_aroma",
            "text"
        ],
        "output_field": [
            "beer_style"
        ],
        "task_name": "arize-ai/beer_reviews_label_drift_neutral",
        "instruction": "Predict the beer style based on the given input fields."
    },
    {
        "input_fields": [
            "prediction_ts",
            "beer_name",
            "beer_style",
            "review_appearance",
            "review_palette",
            "review_taste",
            "review_aroma",
            "text"
        ],
        "output_field": [
            "beer_ABV"
        ],
        "task_name": "arize-ai/beer_reviews_label_drift_neutral",
        "instruction": "Predict the ABV of the beer based on the given input fields."
    },
    {
        "input_fields": [
            "question",
            "sparql",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "drt/kqa_pro-train_val",
        "instruction": "Please answer the question based on the SPARQL query and choices."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "sparql"
        ],
        "task_name": "drt/kqa_pro-train_val",
        "instruction": "Create a SPARQL query based on the question."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "drt/kqa_pro-train_val",
        "instruction": "Which choice is the correct answer to the question?"
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-sst2",
        "instruction": "Determine whether the given sentence has a positive or negative sentiment. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "severo/glue-sst2",
        "instruction": "Create a sentence with the given sentiment."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-mrpc",
        "instruction": "Determine whether two sentences are equivalent or not. Answers must be one of not_equivalent, equivalent."
    },
    {
        "input_fields": [
            "sentence2"
        ],
        "output_field": [
            "sentence1"
        ],
        "task_name": "severo/glue-mrpc",
        "instruction": "Rewrite the first sentence to make it equivalent to the second sentence."
    },
    {
        "input_fields": [
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "severo/glue-mrpc",
        "instruction": "Rewrite the second sentence to make it equivalent to the first sentence."
    },
    {
        "input_fields": [
            "question1",
            "question2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-qqp",
        "instruction": "Determine whether two questions are duplicates or not. Answers must be one of duplicate, not_duplicate."
    },
    {
        "input_fields": [
            "question1"
        ],
        "output_field": [
            "question2"
        ],
        "task_name": "severo/glue-qqp",
        "instruction": "Given a question, generate a similar question."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-stsb",
        "instruction": "Given two sentences, please predict the similarity score between them."
    },
    {
        "input_fields": [
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "severo/glue-stsb",
        "instruction": "Given a sentence, please generate a similar sentence with a different wording."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-mnli",
        "instruction": "Given a premise and a hypothesis, determine whether the hypothesis is entailment, contradiction, or neutral to the premise. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "severo/glue-mnli",
        "instruction": "Given a premise and a label, write a hypothesis that is entailment, contradiction, or neutral to the premise."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-mnli_mismatched",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of entailment, neutral, contradiction."
    },
    {
        "input_fields": [
            "hypothesis"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "severo/glue-mnli_mismatched",
        "instruction": "This task asks models to generate a premise that contradicts the hypothesis."
    },
    {
        "input_fields": [
            "premise"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "severo/glue-mnli_mismatched",
        "instruction": "This task asks models to generate a hypothesis that contradicts the premise."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-mnli_matched",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "severo/glue-mnli_matched",
        "instruction": "This task asks models to generate a hypothesis given a premise and a label."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "severo/glue-mnli_matched",
        "instruction": "This task asks models to generate a premise given a hypothesis and a label."
    },
    {
        "input_fields": [
            "question",
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-qnli",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "severo/glue-qnli",
        "instruction": "Create a sentence that is entailed by the question."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "severo/glue-qnli",
        "instruction": "Create a question that entails the sentence."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-rte",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "label",
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "severo/glue-rte",
        "instruction": "This task asks models to write a sentence that entails, contradicts to, or is neutral to the given sentence."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-wnli",
        "instruction": "Natural language inference (NLI) is the task of determining whether a hypothesis is entailment, contradiction, or neutral given a premise. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "label",
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "severo/glue-wnli",
        "instruction": "This task asks models to write a sentence that entails, contradicts to, or is neutral to the given sentence."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts-amazon",
        "instruction": "Create a question provided the Amazon review."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts-new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts-nyt",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_harvesting_from_wikipedia_pseudo-t5-base-squad",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_harvesting_from_wikipedia_pseudo-bart-base-squad",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_harvesting_from_wikipedia_pseudo-bart-large-squad",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_harvesting_from_wikipedia_pseudo-t5-large-squad",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_harvesting_from_wikipedia_pseudo-t5-small-squad",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "questions_answers"
        ],
        "output_field": [
            "paragraph"
        ],
        "task_name": "lmqg/qag_tweetqa",
        "instruction": "Given a set of questions and answers, generate a paragraph that contains the answers to the questions."
    },
    {
        "input_fields": [
            "smiles"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "zpn/bace_classification",
        "instruction": "Predict the target value based on the SMILES representation of the molecule. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "selfies"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "zpn/bace_classification",
        "instruction": "Predict the target value based on the SELFIES representation of the molecule. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "target"
        ],
        "output_field": [
            "smiles"
        ],
        "task_name": "zpn/bace_classification",
        "instruction": "Given the target value, can you predict the SMILES representation of the molecule?"
    },
    {
        "input_fields": [
            "target"
        ],
        "output_field": [
            "smiles"
        ],
        "task_name": "zpn/pcba_686978",
        "instruction": "Given the target value, can you predict the SMILES representation of the molecule?"
    },
    {
        "input_fields": [
            "smiles"
        ],
        "output_field": [
            "selfies"
        ],
        "task_name": "zpn/bbbp",
        "instruction": "Convert the SMILES representation of the molecule to SELFIES."
    },
    {
        "input_fields": [
            "paragraph"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qg_tweetqa",
        "instruction": "Please generate a question based on the given paragraph."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "paragraph"
        ],
        "task_name": "lmqg/qg_tweetqa",
        "instruction": "Please generate a paragraph based on the given question."
    },
    {
        "input_fields": [
            "paragraph",
            "answer"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qg_tweetqa",
        "instruction": "Please generate a question based on the given paragraph and answer."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.end2end.amazon",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.end2end.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.end2end.nyt",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.end2end.reddit",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.multitask.amazon",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.multitask.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.multitask.nyt",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.multitask.reddit",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.pipeline.amazon",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.pipeline.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.pipeline.nyt",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.pipeline.reddit",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.qg_reference.amazon",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.qg_reference.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.qg_reference.nyt",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.qg_reference.reddit",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.end2end.amazon",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.end2end.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.end2end.new_wiki",
        "instruction": "Given a question, generate a context that answers the question."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.end2end.nyt",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.end2end.reddit",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.multitask.amazon",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.multitask.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.multitask.nyt",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.multitask.reddit",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.pipeline.amazon",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.pipeline.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.pipeline.new_wiki",
        "instruction": "Given a question, please generate a context that answers the question."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.pipeline.nyt",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.pipeline.reddit",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.qg_reference.amazon",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.qg_reference.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.qg_reference.nyt",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.qg_reference.reddit",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.end2end.amazon",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.end2end.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.end2end.nyt",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.end2end.reddit",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.multitask.amazon",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.multitask.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.multitask.nyt",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.multitask.reddit",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.pipeline.amazon",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.pipeline.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.pipeline.new_wiki",
        "instruction": "Given a question, generate a context that answers the question."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.pipeline.nyt",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.pipeline.reddit",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.qg_reference.amazon",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.qg_reference.nyt",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.qg_reference.reddit",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.qg_reference.reddit",
        "instruction": "Can you identify the main concern of the author?"
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.end2end.amazon",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.end2end.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.end2end.nyt",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.end2end.reddit",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.multitask.amazon",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.multitask.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.multitask.new_wiki",
        "instruction": "Given a question, generate a context that answers the question."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.multitask.nyt",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.multitask.reddit",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.pipeline.amazon",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.pipeline.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.pipeline.new_wiki",
        "instruction": "Given a question, generate a context that answers the question."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.pipeline.nyt",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.pipeline.reddit",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.qg_reference.amazon",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.qg_reference.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.qg_reference.nyt",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.qg_reference.reddit",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.qg_reference.reddit",
        "instruction": "Determine the main concern of the author based on the given context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.end2end.amazon",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.end2end.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.end2end.nyt",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.end2end.reddit",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.multitask.amazon",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.multitask.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.multitask.new_wiki",
        "instruction": "Given a question, generate a context that answers the question."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.multitask.nyt",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.multitask.reddit",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.pipeline.amazon",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.pipeline.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.pipeline.nyt",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.pipeline.reddit",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.qg_reference.amazon",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.qg_reference.new_wiki",
        "instruction": "Create a question provided the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.qg_reference.nyt",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.qg_reference.reddit",
        "instruction": "Create a question provided the context."
    },
    {
        "input_fields": [
            "smiles"
        ],
        "output_field": [
            "selfies"
        ],
        "task_name": "zpn/tox21_srp53",
        "instruction": "Convert the SMILES representation of the molecule to SELFIES."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-sst2",
        "instruction": "Determine the sentiment of the given sentence. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "glue-sst2",
        "instruction": "Can you generate a sentence with the given sentiment?"
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-mrpc",
        "instruction": "Determine if two sentences are equivalent or not. Answers must be one of not_equivalent, equivalent."
    },
    {
        "input_fields": [
            "question1",
            "question2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-qqp",
        "instruction": "Determine if two questions are duplicates or not. Answers must be one of duplicate, not_duplicate."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-stsb",
        "instruction": "Determine the similarity score between two sentences."
    },
    {
        "input_fields": [
            "sentence1",
            "label"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "glue-stsb",
        "instruction": "Given a sentence and a similarity score, generate a similar sentence with a higher or lower score."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-mnli",
        "instruction": "Given a premise and a hypothesis, predict the relationship between them. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "glue-mnli",
        "instruction": "Given a premise and a label, generate a hypothesis that is consistent with the premise and label."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "glue-mnli",
        "instruction": "Given a hypothesis and a label, generate a premise that is consistent with the hypothesis and label."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-mnli_mismatched",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis contradicts the premise. Answers must be one of entailment, neutral, contradiction."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "glue-mnli_mismatched",
        "instruction": "Given a premise and a label, generate a hypothesis that contradicts the premise."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "glue-mnli_mismatched",
        "instruction": "Given a hypothesis and a label, generate a premise that contradicts the hypothesis."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-mnli_matched",
        "instruction": "Given a premise and a hypothesis, predict the relationship between them. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "glue-mnli_matched",
        "instruction": "Given a premise and a label, generate a hypothesis that is consistent with the premise and label."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "glue-mnli_matched",
        "instruction": "Given a hypothesis and a label, generate a premise that is consistent with the hypothesis and label."
    },
    {
        "input_fields": [
            "question",
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-qnli",
        "instruction": "Determine if the given sentence entails the answer to the question. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence",
            "label"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "glue-qnli",
        "instruction": "Given a sentence and a label, generate a question that can be answered by the sentence."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-rte",
        "instruction": "Determine whether the two sentences entail each other or not. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "glue-wnli",
        "instruction": "Given two sentences, determine if they are entailed or not. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence1",
            "label"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "glue-wnli",
        "instruction": "Given a sentence and a label, generate a new sentence that entails the original sentence."
    },
    {
        "input_fields": [
            "sentence1",
            "label"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "glue-wnli",
        "instruction": "Given a sentence and a label, generate a new sentence that contradicts the original sentence."
    },
    {
        "input_fields": [
            "sentence_good"
        ],
        "output_field": [
            "sentence_bad"
        ],
        "task_name": "blimp-only_npi_scope",
        "instruction": "Given a sentence with an NPI, provide an example of a sentence that violates the NPI licensing rule."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_dbert_generate_question",
        "instruction": "Given a passage, generate a question that tests the ability of students to identify a specific concept mentioned in the passage."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_dbidaf_answer_the_following_q",
        "instruction": "Given a passage, identify the year in which the research was conducted."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_dbidaf_answer_the_following_q",
        "instruction": "Given a passage, identify the name of the leader of a research group."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_dbidaf_based_on",
        "instruction": "Given a context and a person, determine their position or power."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_dbidaf_generate_question",
        "instruction": "Given a passage about research groups at Yale, generate a question that can be answered by reading the passage."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_dbidaf_generate_question",
        "instruction": "Given a passage about research groups at Yale, identify the year in which the groups were compared."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_dbidaf_generate_question",
        "instruction": "Given a passage about research groups at Yale, identify the group leader who lacked departmental power."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_droberta_generate_question",
        "instruction": "Given a passage, generate a question that tests the ability of students to read and comprehend the passage."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_droberta_generate_question",
        "instruction": "Given a question, identify the passage it is referring to."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_droberta_tell_what_it_is",
        "instruction": "Given a description of a person's attire, identify the type of uniform they wore."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_droberta_tell_what_it_is",
        "instruction": "Given a description of a person's attire, identify the decorations they wore."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-adversarial_qa_droberta_tell_what_it_is",
        "instruction": "Given a description of a person's attire, identify the type of clothing they wore."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-ag_news_classify",
        "instruction": "Given a label, generate a news article that fits the label."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-ag_news_classify_question_first",
        "instruction": "Given a label, generate a news article that fits the label."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Challenge_heres_a_problem",
        "instruction": "Given a problem statement and answer choices, choose the correct answer. Answers must be one of A, D, B, C, 2."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Challenge_heres_a_problem",
        "instruction": "Given a problem statement, identify the correct explanation. Answers must be one of A, D, B, C, 2."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Challenge_i_am_hesitating",
        "instruction": "Given a question and answer choices, choose the correct answer."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Challenge_i_am_hesitating",
        "instruction": "Given a correct answer, generate a question and answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Challenge_multiple_choice",
        "instruction": "Given a multiple choice question and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Challenge_multiple_choice",
        "instruction": "Given a statement and answer choices, select the correct answer that best explains the statement."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Challenge_pick_the_most_correct_option",
        "instruction": "Given a question and answer choices, choose the correct answer. Answers must be one of A, D, B, C, 2."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Challenge_pick_the_most_correct_option",
        "instruction": "Given a correct answer, generate a question and answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Challenge_qa_options",
        "instruction": "Given a question and answer choices, choose the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_heres_a_problem",
        "instruction": "Given a problem and answer choices, select the correct answer. Answers must be one of A, D, B, 3, C, 1."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_heres_a_problem",
        "instruction": "Given a problem and answer choices, provide the correct answer. Answers must be one of A, D, B, 3, C, 1."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_heres_a_problem",
        "instruction": "Given a correct answer, provide the problem and answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_i_am_hesitating",
        "instruction": "Choose the correct answer for the given question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_i_am_hesitating",
        "instruction": "Given the question and the answer choices, generate the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_multiple_choice",
        "instruction": "Choose the correct answer for the multiple choice question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_multiple_choice",
        "instruction": "Provide the correct answer for the multiple choice question."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_multiple_choice",
        "instruction": "Can you design a multiple choice question based on the correct answer?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_pick_false_options",
        "instruction": "Given a set of incorrect options, provide a plausible question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_pick_the_most_correct_option",
        "instruction": "Given a question and answer choices, select the correct answer. Answers must be one of A, D, B, 3, C, 1."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_pick_the_most_correct_option",
        "instruction": "Given a correct answer and answer choices, generate a question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ai2_arc_ARC_Easy_qa_options",
        "instruction": "Given a question and answer choices, choose the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-amazon_polarity_Is_this_product_review_positive",
        "instruction": "Given a product review, extract the title of the review. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-amazon_polarity_negative_or_positive_tone",
        "instruction": "Given a product review and its tone, extract the title and review text."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-amazon_polarity_would_you_buy",
        "instruction": "Given a product review and its title, predict whether the review would increase or decrease the chances of buying the product. Answers must be one of decrease, increase."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_GPT_3_style_r1_score_eval",
        "instruction": "Given a description of a public transport system and a question, determine whether the answer is True, False, or Neither. Answers must be one of True, Neither, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_GPT_3_style_r1_score_eval",
        "instruction": "Given a description of a public transport system and the correct answer to a question, determine whether the answer is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_GPT_3_style_r2",
        "instruction": "Given a movie description and the lead actors, identify the lead actor. True if Rajendra Prasad is the lead actor, False otherwise. Answers must be one of True, Neither, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_GPT_3_style_r2",
        "instruction": "Given a brief introduction of a Tanzanian actor and comedian, identify if he was born in Tanzania. True if Idris Sultan was born in Tanzania, False otherwise. Answers must be one of True, Neither, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_GPT_3_style_r3_score_eval",
        "instruction": "Given the article and the date, determine whether the statement is true, false, or neither. Answers must be one of True, Neither, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_GPT_3_style_r3_score_eval",
        "instruction": "Given the article and the statement, determine whether the statement is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_MNLI_crowdsource_r1_score_eval",
        "instruction": "Given a description of a public transport system and a statement about it, determine if the statement is definitely correct, incorrect, or inconclusive. Answers must be one of Correct, Incorrect, Inconclusive."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_MNLI_crowdsource_r1_score_eval",
        "instruction": "Given a description of a public transport system and a statement about it, determine the correctness of the statement. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_MNLI_crowdsource_r2",
        "instruction": "Given a description of a movie and the lead actors, determine whether a statement about the lead actor is correct, incorrect, or inconclusive. Answers must be one of Correct, Incorrect, Inconclusive."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_MNLI_crowdsource_r2",
        "instruction": "Given a description of a person and their achievements, determine whether a statement about their birthplace is correct, incorrect, or inconclusive. Answers must be one of Correct, Incorrect, Inconclusive."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_MNLI_crowdsource_r2_score_eval",
        "instruction": "Given a movie description and a statement, determine if the statement is correct, incorrect, or inconclusive based on the description. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_MNLI_crowdsource_r2_score_eval",
        "instruction": "Given a movie description and a statement, provide a label for the statement based on the description. Answers must be one of Correct, Incorrect, Inconclusive."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_MNLI_crowdsource_r3_score_eval",
        "instruction": "Given a news article and a statement, determine if the statement is definitely correct, incorrect, or inconclusive based on the article. Answers must be one of Correct, Incorrect, Inconclusive."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_MNLI_crowdsource_r3_score_eval",
        "instruction": "Given a news article and a statement, determine if the statement is correct or not based on the article. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_MNLI_crowdsource_r3_score_eval",
        "instruction": "Given a statement and its correctness, generate a news article that supports or refutes the statement."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_always_sometimes_never_r1_score_eval",
        "instruction": "Given a statement and a fact, determine if the statement is always, sometimes, or never true. Answers must be one of Never, Always, Sometimes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_always_sometimes_never_r1_score_eval",
        "instruction": "Given a statement and its truth value, determine if it is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_always_sometimes_never_r2",
        "instruction": "Given a statement and its context, determine if a specific fact is always, sometimes, or never true. Answers must be one of Always, Never, Sometimes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_always_sometimes_never_r2_score_eval",
        "instruction": "Given a statement and its truth value, determine whether a specific claim is always, sometimes, or never true. Answers must be one of Never, Always, Sometimes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_always_sometimes_never_r2_score_eval",
        "instruction": "Given a statement and its truth value, generate a new statement that is always true. Answers must be one of Never, Always, Sometimes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_always_sometimes_never_r2_score_eval",
        "instruction": "Given a statement and its truth value, generate a new statement that is sometimes true. Answers must be one of Never, Always, Sometimes."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_always_sometimes_never_r3",
        "instruction": "Given a statement and its truth value, determine the date on which the statement was made."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_always_sometimes_never_r3_score_eval",
        "instruction": "Given a statement and a date, determine if the statement is always, sometimes, or never true. Answers must be one of Never, Always, Sometimes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_always_sometimes_never_r3_score_eval",
        "instruction": "Given a statement and its truth value, determine if the date mentioned in the statement is correct or not. Answers must be one of Never, Always, Sometimes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_based_on_the_previous_passage_r1_score_eval",
        "instruction": "Given a passage and a statement, determine if the statement is true, false, or uncertain based on the information in the passage. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_based_on_the_previous_passage_r1_score_eval",
        "instruction": "Given a passage and a statement, provide a \"yes\", \"no\", or \"maybe\" answer based on the information in the passage. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_based_on_the_previous_passage_r1_score_eval",
        "instruction": "Given a statement and its correctness, generate a passage that supports or refutes the statement."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_based_on_the_previous_passage_r2",
        "instruction": "Given a passage and a statement, determine if the statement is true, false, or uncertain based on the passage. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_based_on_the_previous_passage_r2_score_eval",
        "instruction": "Given a passage and a statement, determine if the statement is true, false, or uncertain based on the information in the passage. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_based_on_the_previous_passage_r2_score_eval",
        "instruction": "Given a passage and a statement, determine if the statement is true or false based on the information in the passage. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_based_on_the_previous_passage_r2_score_eval",
        "instruction": "Given a passage and a statement, determine if the statement is uncertain based on the information in the passage. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_based_on_the_previous_passage_r3_score_eval",
        "instruction": "Given a passage and a statement, determine whether the statement is true, false, or uncertain based on the information in the passage. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_based_on_the_previous_passage_r3_score_eval",
        "instruction": "Given a passage and a statement, provide a \"yes\", \"no\", or \"maybe\" answer to the statement based on the information in the passage. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_can_we_infer_r1_score_eval",
        "instruction": "Given a statement and a question, determine if the answer is \"Yes\", \"No\", or \"Maybe\". Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_can_we_infer_r1_score_eval",
        "instruction": "Determine if the given answer is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_can_we_infer_r2",
        "instruction": "Given a sentence and a question, choose the correct answer. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_can_we_infer_r2_score_eval",
        "instruction": "Given a sentence and a statement, determine if the statement can be inferred from the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_can_we_infer_r3_score_eval",
        "instruction": "Given the inputs_pretokenized, can you infer the date of the article? Answer Yes, No or Maybe. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_can_we_infer_r3_score_eval",
        "instruction": "Given the inputs_pretokenized and targets_pretokenized, can you determine if the inference is correct? Answer True or False. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_claim_true_false_inconclusive_r2_score_eval",
        "instruction": "Given a movie description and a claim, determine if the claim is true, false, or inconclusive. Answers must be one of True, Inconclusive, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_claim_true_false_inconclusive_r2_score_eval",
        "instruction": "Given a movie description and a claim, determine if the claim is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_claim_true_false_inconclusive_r2_score_eval",
        "instruction": "Given a movie description and a claim, provide a rationale for why the claim is inconclusive. Answers must be one of True, Inconclusive, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_claim_true_false_inconclusive_r3_score_eval",
        "instruction": "Given the inputs_pretokenized and targets_pretokenized, determine whether the is_correct field is True, False, or Inconclusive. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_claim_true_false_inconclusive_r3_score_eval",
        "instruction": "Given the inputs_pretokenized and is_correct, determine whether the targets_pretokenized field is True, False, or Inconclusive. Answers must be one of True, Inconclusive, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_consider_always_sometimes_never_r1_score_eval",
        "instruction": "Given a statement and a choice of always, sometimes, or never, determine whether the statement is always, sometimes, or never correct. Answers must be one of Never, Always, Sometimes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_consider_always_sometimes_never_r1_score_eval",
        "instruction": "Given a statement and a choice of always, sometimes, or never, determine whether the statement is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_consider_always_sometimes_never_r2_score_eval",
        "instruction": "Given a movie description and a statement about the movie, determine if the statement is always, sometimes, or never correct. Answers must be one of Never, Always, Sometimes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_consider_always_sometimes_never_r2_score_eval",
        "instruction": "Given a movie description and a statement about the movie, determine if the statement is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_consider_always_sometimes_never_r3_score_eval",
        "instruction": "Given a piece of news and a date, determine whether the statement \"The article was written on [date]\" is always, sometimes, or never correct. Answers must be one of Never, Always, Sometimes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_consider_always_sometimes_never_r3_score_eval",
        "instruction": "Given a piece of news and a statement about the date it was written, determine whether the statement is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_does_it_follow_that_r1_score_eval",
        "instruction": "Given a statement and a question, determine whether the answer is yes, no, or maybe. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_does_it_follow_that_r1_score_eval",
        "instruction": "Determine whether the given answer is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_does_it_follow_that_r1_score_eval",
        "instruction": "Determine the number of urban routes in the Parma trolleybus system. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_does_it_follow_that_r2",
        "instruction": "Given a sentence and answer choices, determine whether the statement in the sentence is true or false. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_does_it_follow_that_r2",
        "instruction": "Given a sentence and answer choices, choose the most likely answer choice. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_does_it_follow_that_r2_score_eval",
        "instruction": "Given a movie description and a statement, determine if the statement follows from the description. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_does_it_follow_that_r2_score_eval",
        "instruction": "Given a movie description and a statement, predict whether the statement is true, false, or uncertain. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_does_it_follow_that_r3_score_eval",
        "instruction": "Given the inputs_pretokenized and targets_pretokenized, predict whether is_correct is True, False, or Maybe. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_does_it_follow_that_r3_score_eval",
        "instruction": "Given the inputs_pretokenized and is_correct, predict whether targets_pretokenized is Yes, No, or Maybe. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_does_this_imply_r1_score_eval",
        "instruction": "Given a statement and a question, determine whether the statement implies the answer to the question. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_does_this_imply_r2",
        "instruction": "Given a sentence and the answer, determine the statement that is implied by the sentence. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_does_this_imply_r2_score_eval",
        "instruction": "Given a sentence and a question, determine whether the question can be answered based on the information in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_does_this_imply_r3_score_eval",
        "instruction": "Given a sentence and a question, determine whether the answer to the question is \"Yes\", \"No\", or \"Maybe\". Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_does_this_imply_r3_score_eval",
        "instruction": "Given a sentence and an answer, determine whether the answer is \"Yes\", \"No\", or \"Maybe\". Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_possible_impossible_r1_score_eval",
        "instruction": "Given a statement and a question about the statement, determine if the answer is guaranteed, possible, or impossible based on the statement. Answers must be one of Impossible, Guaranteed, Possible."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_possible_impossible_r1_score_eval",
        "instruction": "Given a statement and a question about the statement, determine if the answer is correct or incorrect based on the statement. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_possible_impossible_r2",
        "instruction": "Given a sentence and a statement, determine whether the statement is guaranteed, possible, or impossible based on the information in the sentence. Answers must be one of Impossible, Guaranteed, Possible."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_possible_impossible_r2",
        "instruction": "Given a sentence and a statement, generate a statement that is guaranteed based on the information in the sentence. Answers must be one of Impossible, Guaranteed, Possible."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_possible_impossible_r2",
        "instruction": "Given a sentence and a statement, generate a statement that is impossible based on the information in the sentence. Answers must be one of Impossible, Guaranteed, Possible."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_possible_impossible_r2_score_eval",
        "instruction": "Given a statement and a fact, determine whether the fact is guaranteed, possible, or impossible based on the statement. Answers must be one of Impossible, Guaranteed, Possible."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_possible_impossible_r2_score_eval",
        "instruction": "Given a statement and a fact, determine whether the fact is correct or incorrect based on the statement. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_possible_impossible_r3_score_eval",
        "instruction": "Given a statement and a date, determine whether the statement is guaranteed, possible, or impossible based on the date. Answers must be one of Impossible, Guaranteed, Possible."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_possible_impossible_r3_score_eval",
        "instruction": "Given a statement and its guarantee status, determine whether the statement is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_true_r1_score_eval",
        "instruction": "Given the description of a trolleybus system and a statement, determine if the statement is guaranteed true or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_true_r1_score_eval",
        "instruction": "Given the description of a trolleybus system and a statement, determine if the statement is guaranteed true, false, or maybe. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_true_r2",
        "instruction": "Given a sentence and a statement, determine if the statement is guaranteed true based on the information in the sentence. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_true_r2",
        "instruction": "Given a sentence and a statement, determine if the statement is possibly true based on the information in the sentence. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_true_r2",
        "instruction": "Given a sentence and a statement, determine if the statement is not true based on the information in the sentence. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_true_r2_score_eval",
        "instruction": "Given a sentence and a statement, determine if the statement is guaranteed true based on the information in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_true_r2_score_eval",
        "instruction": "Given a sentence and a statement, provide a response of \"Yes\", \"No\", or \"Maybe\" to indicate whether the statement is guaranteed true based on the information in the sentence. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_true_r2_score_eval",
        "instruction": "Given a sentence and a response of \"Yes\", \"No\", or \"Maybe\", determine whether the statement is guaranteed true based on the information in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_true_r3_score_eval",
        "instruction": "Given the article and the date, determine whether the article was written on the given date. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_guaranteed_true_r3_score_eval",
        "instruction": "Determine whether the statement is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_justified_in_saying_r1_score_eval",
        "instruction": "Determine whether the statement is true, false, or uncertain based on the given information. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_justified_in_saying_r1_score_eval",
        "instruction": "Given the statement and the answer, determine whether the answer is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_justified_in_saying_r2",
        "instruction": "Given a sentence and a statement, determine if the statement is justified based on the information in the sentence. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_justified_in_saying_r2_score_eval",
        "instruction": "Given a sentence and a statement, determine if the statement is justified based on the information in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_justified_in_saying_r3_score_eval",
        "instruction": "Given the article and the date mentioned in the article, determine whether the statement \"The article was written on December 18th.\" is justified or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_justified_in_saying_r3_score_eval",
        "instruction": "Given the article and the date mentioned in the article, determine whether it is possible to determine if the statement \"The article was written on December 18th.\" is justified or not. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_must_be_true_r1_score_eval",
        "instruction": "Given the description of a trolleybus system, determine whether the statement \"The trolleybus system has over 2 urban routes\" is true, false, or uncertain. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_must_be_true_r1_score_eval",
        "instruction": "Given the description of a trolleybus system and a statement, determine whether the statement is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_must_be_true_r2_score_eval",
        "instruction": "Given a statement and a question, determine whether the answer is \"Yes\", \"No\", or \"Maybe\". Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_must_be_true_r2_score_eval",
        "instruction": "Given a statement and a question, determine whether the answer is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_must_be_true_r3_score_eval",
        "instruction": "Given the article and a statement, determine whether the statement is true, false, or uncertain based on the article. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_must_be_true_r3_score_eval",
        "instruction": "Given the article and a statement, determine whether the statement is true or false based on the article. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_should_assume_r1_score_eval",
        "instruction": "Given a statement and its context, determine whether the statement is true, false, or uncertain. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_should_assume_r1_score_eval",
        "instruction": "Given a statement and its context, determine whether the statement is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_should_assume_r1_score_eval",
        "instruction": "Given a statement and its context, determine the level of certainty of the statement. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_should_assume_r2",
        "instruction": "Given a sentence and the answer choices, determine the statement that is true. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_should_assume_r2_score_eval",
        "instruction": "Given a sentence and a statement, determine whether the statement is true, false, or uncertain based on the information in the sentence. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_should_assume_r2_score_eval",
        "instruction": "Given a sentence and a statement, determine whether the statement is correct or incorrect based on the information in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_should_assume_r3_score_eval",
        "instruction": "Given the inputs_pretokenized, predict whether the is_correct is True, False, or Maybe. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_should_assume_r3_score_eval",
        "instruction": "Given the inputs_pretokenized and targets_pretokenized, predict whether the is_correct is True or False. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_take_the_following_as_truth_r1_score_eval",
        "instruction": "Determine whether a given statement is true, false, or inconclusive based on the provided information about the Parma trolleybus system. Answers must be one of True, Inconclusive, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_take_the_following_as_truth_r1_score_eval",
        "instruction": "Given a statement and the information about the Parma trolleybus system, determine whether the statement is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_take_the_following_as_truth_r2",
        "instruction": "Given a statement and a set of facts, determine whether the statement is true, false, or inconclusive. Answers must be one of True, Inconclusive, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_take_the_following_as_truth_r2_score_eval",
        "instruction": "Given a movie description and a statement, determine if the statement is true, false, or inconclusive. Answers must be one of True, Inconclusive, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_take_the_following_as_truth_r2_score_eval",
        "instruction": "Given a movie description and a statement, determine if the statement is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_take_the_following_as_truth_r3_score_eval",
        "instruction": "Given a statement and a date, determine whether the statement is true, false, or inconclusive based on the date. Answers must be one of True, Inconclusive, False."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-anli_take_the_following_as_truth_r3_score_eval",
        "instruction": "Given a statement and its truth value, determine whether the statement is true, false, or inconclusive based on the date. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-anli_take_the_following_as_truth_r3_score_eval",
        "instruction": "Determine the date of the article based on the statement and the given information. Answers must be one of True, Inconclusive, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-app_reviews_generate_review",
        "instruction": "Generate a 4-star review for an app with a given package name."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_2_or_3_sentences",
        "instruction": "Given a news article, summarize the main points in 2-3 sentences."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_2_or_3_sentences",
        "instruction": "Given a news article summary, provide the original article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_generate_story",
        "instruction": "Given a news article summary, generate a full news article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_generate_story",
        "instruction": "Given a news article, summarize it into a few key points."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_news_card_view",
        "instruction": "Given a news article, summarize it into short cards for mobile news apps."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_news_card_view",
        "instruction": "Given a news summary, extract the key events and details from it."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_news_card_view",
        "instruction": "Given a news article, identify the key questions that need to be answered."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_news_stock",
        "instruction": "Given the article, predict how the stock market will react based on the key points extracted from the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_news_summary",
        "instruction": "Given a news article, generate a summary of the article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_news_summary",
        "instruction": "Given a news summary, generate a headline for the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_sum_in_brief",
        "instruction": "Given an article, summarize it in brief."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_tldr_summary",
        "instruction": "Given a news article, generate a TLDR (Too Long Didn't Read) summary of the article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_tldr_summary",
        "instruction": "Given a TLDR summary of a news article, generate the original news article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_write_an_outline",
        "instruction": "Given an article, can you summarize it in a few points?"
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_write_an_outline",
        "instruction": "Given a summary, can you provide the article it summarizes?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cnn_dailymail_3.0.0_write_an_outline",
        "instruction": "Given an article, can you identify the key questions it raises?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-common_gen_sentence_to_concepts",
        "instruction": "Given a sentence, extract all the key concepts."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-common_gen_topic_to_sentence",
        "instruction": "Given a topic, generate a sentence describing an everyday scenario using this topic."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-common_gen_topic_to_sentence",
        "instruction": "Given a sentence, identify the topic."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-common_gen_topics_from_the_sentence",
        "instruction": "Given a sentence, generate a set of related concepts."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-common_gen_topics_from_the_sentence",
        "instruction": "Given a set of concepts, generate a sentence describing an everyday scenario using these concepts."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_description_question_option_id",
        "instruction": "Given a question and answer choices, select the correct answer. Answers must be one of A, C, D, B, E."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_description_question_option_text",
        "instruction": "Given a question and answer choices, select the correct answer choice."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_description_question_option_text",
        "instruction": "Given a correct answer and a set of answer choices, generate a question."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_generate_explanation_given_text",
        "instruction": "Given a question and options, choose the correct answer and provide an explanation."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_generate_explanation_given_text",
        "instruction": "Given a sentence describing a stereotype, identify the location where the person lives."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_question_description_option_id",
        "instruction": "Given a sentence with missing information, fill in the blank with the correct number or word. Answers must be one of A, C, D, B, E."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_question_description_option_id",
        "instruction": "Choose the correct answer for the question. Answers must be one of A, C, D, B, E."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_question_description_option_id",
        "instruction": "Identify the type of question being asked. Answers must be one of A, C, D, B, E."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_question_description_option_text",
        "instruction": "Given a sentence with a missing value, fill in the blank with the correct value."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_question_description_option_text",
        "instruction": "Choose the correct answer for the given question."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_question_description_option_text",
        "instruction": "Identify the missing value in the given sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_question_option_description_id",
        "instruction": "Given a sentence with a missing word or phrase, choose the correct answer from a list of options. Answers must be one of A, C, D, B, E."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_question_option_description_id",
        "instruction": "Given a sentence and a list of options, choose the correct option that best describes the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_question_option_description_text",
        "instruction": "Given a sentence with a missing value, choose the correct option to fill in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_question_option_description_text",
        "instruction": "Given a sentence and a list of options, choose the option that best describes the location of a person or object in the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_rationale",
        "instruction": "Given a question and choices, select the correct answer based on the rationale provided."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cos_e_v1.11_rationale",
        "instruction": "Given a sentence describing a stereotype, identify the location where the person lives."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_context_description_question_answer_id",
        "instruction": "Choose the correct answer for the given question based on the context. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_context_description_question_answer_text",
        "instruction": "Given a sentence and answer choices, choose the best option to answer the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_context_description_question_text",
        "instruction": "Choose the correct answer for the given question."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_context_description_question_text",
        "instruction": "Given the context, can you provide the question?"
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_context_question_description_answer_id",
        "instruction": "Given the answer choices and the context, choose the best answer for the question. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_context_question_description_answer_text",
        "instruction": "Given a sentence and answer choices, choose the best answer that matches the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_context_question_description_answer_text",
        "instruction": "Given a sentence and answer choices, choose the best reason for a certain event."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_context_question_description_text",
        "instruction": "Given a sentence and answer choices, choose the correct answer to a question based on the information in the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_context_question_description_text",
        "instruction": "Given a sentence and answer choices, choose the incorrect answer to a question based on the information in the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_description_context_question_answer_id",
        "instruction": "Given a context and a question, choose the best option to answer the question. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_description_context_question_answer_text",
        "instruction": "Choose the best option to answer the question based on the given context."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_description_context_question_answer_text",
        "instruction": "Given the context and the options, identify the question that can be answered."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_description_context_question_text",
        "instruction": "Given a context and answer choices, choose the correct answer for the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_no_prompt_id",
        "instruction": "Given a sentence and answer choices, choose the correct answer that corresponds to the question asked in the sentence. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_no_prompt_id",
        "instruction": "Given a sentence and answer choices, choose the incorrect answer that corresponds to the question asked in the sentence. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_no_prompt_id",
        "instruction": "Given a sentence and answer choices, choose the answer that is not mentioned in the sentence. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_no_prompt_text",
        "instruction": "Given a sentence and answer choices, choose the correct answer that best describes the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_no_prompt_text",
        "instruction": "Given a sentence and answer choices, choose the incorrect answer that does not describe the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_no_prompt_text",
        "instruction": "Given a sentence and answer choices, choose the answer that is not mentioned in the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-duorc_SelfRC_movie_director",
        "instruction": "Given a movie plot, identify the main character(s) and their role(s)."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-duorc_SelfRC_movie_director",
        "instruction": "Given a movie plot, identify the key events that occur."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-duorc_SelfRC_movie_director",
        "instruction": "Given a movie plot, identify the setting and time period."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-duorc_SelfRC_question_answering",
        "instruction": "Given a movie title and context, answer a specific question related to the movie."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-duorc_SelfRC_title_generation",
        "instruction": "Given a movie plot, generate a movie title."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-duorc_SelfRC_title_generation",
        "instruction": "Given a movie title, generate a movie plot."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_TLDR",
        "instruction": "Given a news article summary, generate a headline that captures the essence of the article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_TLDR",
        "instruction": "Given a headline, generate a news article summary."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_generate_summary_for_this",
        "instruction": "Given a news article, generate a summary of the article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_generate_summary_for_this",
        "instruction": "Given a news summary, generate a title for the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_make_a_title",
        "instruction": "Given a news article, generate a short summary of the article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_make_a_title",
        "instruction": "Given a short summary of a news article, generate a headline for the article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_reverse_writing",
        "instruction": "Given a sentence, predict the title of the news article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_reverse_writing",
        "instruction": "Given a news article title, generate a summary of the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_write_a_title_for_this_sentence",
        "instruction": "Given a sentence, write a summary of the key information in the sentence."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_write_a_title_for_this_sentence",
        "instruction": "Given a summary, generate a sentence that contains the key information."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_write_an_article",
        "instruction": "Given a title, write an article about the topic."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-gigaword_write_an_article",
        "instruction": "Extract the title from the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_equivalent",
        "instruction": "Determine whether two sentences are equivalent or not. Answers must be one of not equivalent, equivalent."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_equivalent",
        "instruction": "Select the correct answer choice for a given pair of sentences. Answers must be one of not equivalent, equivalent."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_equivalent",
        "instruction": "Rewrite a sentence to make it equivalent to another sentence. Answers must be one of not equivalent, equivalent."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_generate_paraphrase",
        "instruction": "Paraphrase the given sentence."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_generate_paraphrase",
        "instruction": "Identify the sentence that is a paraphrase of the given sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_generate_sentence",
        "instruction": "Generate a sentence that means the same thing as the input sentence, but with a different word choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_generate_sentence",
        "instruction": "Generate a sentence that means the opposite of the input sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_generate_sentence",
        "instruction": "Rearrange the words in the input sentence to form a new sentence with the same meaning."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_paraphrase",
        "instruction": "Determine whether the paraphrase is correct or not. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_paraphrase",
        "instruction": "Provide the original sentence given a paraphrase."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_replace",
        "instruction": "Determine if two sentences have the same meaning. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_replace",
        "instruction": "Given two sentences, replace a specific phrase in one sentence with a different phrase and determine if the resulting sentence has the same meaning as the original sentence. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_replace",
        "instruction": "Given a sentence and a replacement phrase, generate a new sentence by replacing the specific phrase in the original sentence with the replacement phrase. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-glue_mrpc_same_thing",
        "instruction": "Determine if two sentences mean the same thing. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-glue_mrpc_same_thing",
        "instruction": "Identify the sentence that contains incorrect information. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_want_to_know",
        "instruction": "Determine whether two sentences mean the same thing. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_mrpc_want_to_know",
        "instruction": "Choose the correct sentence that matches the given information. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_qqp_answer",
        "instruction": "Given a question and answer choices, determine if the answer to one question can be used to answer another question. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_qqp_duplicate_or_not",
        "instruction": "Given a question, determine how to control horny emotions. Answers must be one of not duplicates, duplicates."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_qqp_quora",
        "instruction": "Given two Quora posts, merge them if they are asking the same thing. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_qqp_same_thing",
        "instruction": "Determine whether two questions are asking the same thing. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-glue_qqp_same_thing",
        "instruction": "Given a question and its answer choices, determine whether the answer is \"yes\" or \"no\". Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Appropriate_continuation_Yes_or_No",
        "instruction": "Given a description and a potential continuation, determine if the continuation is appropriate. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Appropriate_continuation_Yes_or_No",
        "instruction": "Given a description and a potential continuation, generate a plausible continuation. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Open_ended_completion",
        "instruction": "Given a sentence with missing information, choose the correct completion from a list of options."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Open_ended_completion",
        "instruction": "Given a set of baking pans, describe what is done with them in a kitchen."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Open_ended_start",
        "instruction": "Given a sentence starting with a phrase, complete the sentence with a coherent ending."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Open_ended_start",
        "instruction": "Given a sentence ending with a phrase, complete the sentence with a coherent beginning."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Predict_ending_with_hint",
        "instruction": "Given a sentence and answer choices, choose the correct ending of the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Randomized_prompts_template",
        "instruction": "Choose the correct ending for the given sentence."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Randomized_prompts_template",
        "instruction": "Generate a sentence based on the given ending."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Topic_without_the_ending_answer",
        "instruction": "Given a sentence, identify the action being performed."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Topic_without_the_ending_answer",
        "instruction": "Given a sentence, identify the object being presented."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_Topic_without_the_ending_answer",
        "instruction": "Given a sentence, identify the location where the action is taking place."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_complete_first_then",
        "instruction": "Choose the correct ending for the given incomplete description."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_complete_first_then_score_eval",
        "instruction": "Given a sentence with a sequence of events, choose the correct ending to complete the story."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_how_ends",
        "instruction": "Given a sentence with a set of answer choices, the task is to choose the correct ending that completes the sentence. Answers must be one of Ending 1, Ending 4, Ending 2, Ending 3."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_if_begins_how_continues",
        "instruction": "Given a sentence that starts with a description of a situation and ends with multiple choices, choose the correct ending. Answers must be one of Ending 1, Ending 4, Ending 2, Ending 3."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-hellaswag_if_begins_how_continues_score_eval",
        "instruction": "Given a sentence with a starting scenario and multiple possible endings, generate a new ending. Answers must be one of Ending 1, Ending 4, Ending 2, Ending 3."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-imdb_Negation_template_for_positive_and_negative",
        "instruction": "Given a review, provide a summary of the main points."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-kilt_tasks_hotpotqa_combining_facts",
        "instruction": "Given a set of facts, determine which magazine was started first."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-kilt_tasks_hotpotqa_combining_facts",
        "instruction": "Given a set of facts, determine the head office city of a hotel company."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-kilt_tasks_hotpotqa_complex_question",
        "instruction": "Given a complex question, identify the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-kilt_tasks_hotpotqa_formulate",
        "instruction": "Given a question with two options, choose the option that was started first."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-kilt_tasks_hotpotqa_formulate",
        "instruction": "Given a question about a hotel company, provide the city where the head office is located."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-multi_news_synthesize",
        "instruction": "Given a set of documents, synthesize a summary of the key points."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-multi_news_synthesize",
        "instruction": "Given a summary of key points, identify the original documents that were synthesized."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-multi_news_synthesize",
        "instruction": "Given a document, extract the expected job creation and unemployment rate."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-multi_news_synthesize",
        "instruction": "Given a document, extract the name of the person being interviewed and the topic of the interview."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_choices",
        "instruction": "Given a sentence and a list of answer choices, choose the correct answer that completes the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_choose_an_answer_with_options",
        "instruction": "Given a question and a list of answer choices, choose the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_choose_an_answer_with_options",
        "instruction": "Given a sentence and a list of answer choices, choose the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_only_options",
        "instruction": "Given a list of options and a statement, choose the correct option that matches the statement."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_only_options",
        "instruction": "Given a list of options and a correct option, generate a statement that matches the correct option."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_pick_answer_with_options",
        "instruction": "Given a scenario and a list of options, choose the correct option that matches the scenario."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_pick_using_id",
        "instruction": "Given a question and answer choices, select the correct answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_pick_using_id",
        "instruction": "Given a correct answer and answer choices, generate a question that fits the answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_which_correct",
        "instruction": "Given a statement and answer choices, choose the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_which_correct",
        "instruction": "Given a statement and answer choices, choose the incorrect answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_which_correct_inverse",
        "instruction": "Given a list of options and a statement, choose the correct answer."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-openbookqa_main_which_correct_inverse",
        "instruction": "Given a list of options and a correct answer, generate a statement."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_Concatenation",
        "instruction": "Given two sentences and a question, determine if the first sentence paraphrases the second sentence. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_Concatenation",
        "instruction": "Given a sentence and a question, determine if the sentence is a paraphrase of another sentence. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_Concatenation_no_label",
        "instruction": "Given two sentences, determine if one is a paraphrase of the other. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_Meaning",
        "instruction": "Determine if two sentences express the same meaning. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_Meaning",
        "instruction": "Choose the correct sentence that describes a given event. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_Meaning",
        "instruction": "Given a sentence, provide a paraphrased version of the same sentence. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_Meaning_no_label",
        "instruction": "Determine if two sentences express the same meaning. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_Meaning_no_label",
        "instruction": "Given two sentences, identify the difference in meaning between them. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_Meaning_no_label",
        "instruction": "Given a sentence and a set of possible rephrased sentences, identify the sentence that expresses the same meaning as the original sentence. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_PAWS_ANLI_GPT3",
        "instruction": "Given a sentence and a question, determine if the question is true or false based on the information in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_Rewrite",
        "instruction": "Determine the season number of the NBA season. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_context_question",
        "instruction": "Provide a sentence given a target sentence and whether it is a paraphrase or not."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_context_question_no_label",
        "instruction": "Given a sentence and a paraphrased version of the sentence, determine if the paraphrased version is correct. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_context_question_no_label",
        "instruction": "Given a sentence and a paraphrased version of the sentence, generate a new paraphrased version of the sentence. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_paraphrase_task",
        "instruction": "Paraphrase the given sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_paraphrase_task",
        "instruction": "Reverse the order of clauses in the given sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-paws_labeled_final_task_description_no_label",
        "instruction": "Determine if two given sentences paraphrase each other or not. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Correct_the_solution",
        "instruction": "Given a goal and a wrong solution, rewrite it to give a correct solution."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Correct_the_solution_if_false_from_sol_1",
        "instruction": "Correct the sentence if it does not make sense."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Correct_the_solution_if_false_from_sol_1",
        "instruction": "Identify the common pattern in the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Correct_the_solution_if_false_from_sol_2",
        "instruction": "Correct the sentence if it does not make sense."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Correct_the_solution_if_false_from_sol_2",
        "instruction": "Identify the error in the sentence and provide a corrected version."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Correct_the_solution_if_false_from_sol_2",
        "instruction": "Identify the type of error in the sentence and provide a corrected version."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Does_this_solution_make_sense_sol1",
        "instruction": "Determine if a given phrase makes sense or not. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Does_this_solution_make_sense_sol1",
        "instruction": "Provide a better alternative phrase for a given phrase that does not make sense. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Does_this_solution_make_sense_sol2",
        "instruction": "Given a phrase and a question asking if it makes sense, choose the correct answer. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_Does_this_solution_make_sense_sol2",
        "instruction": "Given a phrase, determine if it makes sense or not. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_choose_the_most_appropriate_solution",
        "instruction": "Given a goal and 2 solutions, choose the most appropriate solution. Answers must be one of Solution 1, Solution 2."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_choose_the_most_appropriate_solution",
        "instruction": "Provide the goal and the correct solution."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_finish_sentence_with_correct_choice",
        "instruction": "Given a sentence with a blank, choose the best option to fill in the blank."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_finish_sentence_with_correct_choice",
        "instruction": "Given a sentence and two options, choose the best option to complete the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_no_prompt_needed",
        "instruction": "Given a sentence with a missing word, choose the correct word to complete the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_no_prompt_needed",
        "instruction": "Given a sentence, identify the action being described."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_no_prompt_needed",
        "instruction": "Given a sentence, identify the tool or method being described."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_pick_correct_choice_index",
        "instruction": "Given a sentence with two possible endings, choose the correct ending. Answers must be one of 2, 1."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_pick_correct_choice_with_choice_given_before_goal",
        "instruction": "Given a set of answer choices and a goal, choose the correct solution."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-piqa_what_is_the_correct_ending",
        "instruction": "Given a goal and two options, choose the correct ending."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_is_correct_2",
        "instruction": "Given a statement and a correct answer, determine if the statement is a correct answer to the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_combined_facts_1",
        "instruction": "Given a question and answer choices, choose the correct answer."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_combined_facts_1",
        "instruction": "Given a correct answer, generate a question and answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_1",
        "instruction": "Given a set of facts and answer choices, choose the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_2",
        "instruction": "Given a set of facts and answer choices, answer the question."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_2",
        "instruction": "Given a question and answer choices, identify the correct fact."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_2",
        "instruction": "Given a fact and answer choices, identify the correct question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_3",
        "instruction": "Given a set of facts, choose the correct answer from the given options."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_3",
        "instruction": "Given a set of facts, identify the correct answer from the given options."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_3",
        "instruction": "Given a set of facts, generate a question based on the information provided."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_4",
        "instruction": "Choose the correct answer for the given question."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_4",
        "instruction": "Given the answer, choose the correct question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_4",
        "instruction": "Given the question and answer choices, identify the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_5",
        "instruction": "Choose the correct answer for the given question."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_5",
        "instruction": "Given a question and hints, identify the correct answer."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-qasc_qa_with_separated_facts_5",
        "instruction": "Given a correct answer and hints, generate a question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_description_question_answer_id",
        "instruction": "Given a sentence, choose the correct answer from the given options. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_description_question_answer_id",
        "instruction": "Given a sentence, generate a question based on the context. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_description_question_answer_text",
        "instruction": "Given a sentence, choose the correct answer choice that best fits the context."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_description_question_answer_text",
        "instruction": "Given a sentence, generate a question that can be answered based on the context."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_description_question_text",
        "instruction": "Given a sentence, choose the correct answer choice that best fits the context."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_description_question_text",
        "instruction": "Given a sentence, generate a question that can be answered based on the information provided."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_answer_description_id",
        "instruction": "Given a sentence, choose the correct answer from the given options. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_answer_description_id",
        "instruction": "Given a sentence, provide the correct answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_answer_description_id",
        "instruction": "Given an answer, provide the corresponding sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_answer_description_text",
        "instruction": "Given a sentence and answer choices, choose the correct answer that best fits the context of the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_answer_description_text",
        "instruction": "Given a sentence, provide a description of the context of the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_description_answer_id",
        "instruction": "Given a sentence, choose the correct answer from the given options. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_description_answer_text",
        "instruction": "Given a sentence, choose the correct answer choice that best fits the context."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_description_answer_text",
        "instruction": "Given a sentence, provide a possible reason or explanation for a certain situation."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_description_text",
        "instruction": "Given a sentence and answer choices, choose the correct answer that best fits the context of the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_context_question_description_text",
        "instruction": "Given a sentence, provide additional information that is not explicitly stated in the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_description_context_question_answer_id",
        "instruction": "Given the context and answer choices, choose the correct option to answer the question. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_description_context_question_answer_text",
        "instruction": "Given a context and answer choices, choose the correct option to answer the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_description_context_question_answer_text",
        "instruction": "Given a context, determine if a certain statement is true or false."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_description_context_question_answer_text",
        "instruction": "Given a context, provide additional information that is not explicitly stated."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_description_context_question_text",
        "instruction": "Given a context and answer choices, choose the correct answer for the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_no_prompt_id",
        "instruction": "Given a set of answer choices and a passage, choose the correct answer for a multiple-choice question. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quail_no_prompt_text",
        "instruction": "Given a sentence, choose the correct answer choice that best fits the context."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quarel_choose_between",
        "instruction": "Choose the correct answer for the given question."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-quarel_choose_between",
        "instruction": "Given the correct answer, generate a question with two answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quarel_do_not_use",
        "instruction": "Given a question and answer choices, choose the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quarel_logic_test",
        "instruction": "Given a logic test question with two answer choices, choose the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quarel_logic_test",
        "instruction": "Given a logic test question with two answer choices, extract the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quarel_testing_students",
        "instruction": "Given a logic test question with answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_answer_question_based_on",
        "instruction": "Choose the correct answer based on the given question and text."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_answer_question_based_on",
        "instruction": "Fill in the blank with the correct word based on the given question and text."
    },
    {
        "input_fields": [
            "answer_choices",
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_answer_question_based_on",
        "instruction": "Given the answer choices and the correct answer, generate a question and text."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_answer_question_below",
        "instruction": "Answer the question based on the given context."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_answer_question_below",
        "instruction": "Given a statement about electrons, determine whether the energy level is higher or lower."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_given_the_fact_answer_the_q",
        "instruction": "Given a statement and answer choices, determine the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_having_read_above_passage",
        "instruction": "Given a sentence and two answer choices, choose the correct answer based on the information in the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_paragraph_question_plain_concat",
        "instruction": "Given a sentence about water scarcity, choose the correct word to complete the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_paragraph_question_plain_concat",
        "instruction": "Given a sentence about electron energy levels, choose the correct word to complete the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_paragraph_question_plain_concat",
        "instruction": "Can you provide an explanation for the given sentence?"
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_read_passage_below_choose",
        "instruction": "Choose the right answer based on the given passage."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_read_passage_below_choose",
        "instruction": "Fill in the blank with the right word based on the given passage."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_read_passage_below_choose",
        "instruction": "Determine whether the given statement is true or false based on the given passage."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_use_info_from_paragraph_question",
        "instruction": "Given a paragraph and answer choices, determine the correct answer to a question that requires information from the paragraph."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quartz_use_info_from_question_paragraph",
        "instruction": "Choose the correct answer for the question based on the given paragraph."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Answer_Friend_Question",
        "instruction": "Given a question and an article, identify the name of a person mentioned in the article who is associated with a specific field."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Answer_Friend_Question",
        "instruction": "Given a name of a person and an article, identify the field that the person is associated with."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Context_Contains_Answer",
        "instruction": "Given a sentence, identify the scientist mentioned in the report."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Context_Contains_Answer",
        "instruction": "Given a sentence, identify the pseudonym used by the composer."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Context_Contains_Answer",
        "instruction": "Given a name, find the sentence that mentions the person."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Find_Answer",
        "instruction": "Given a text, find the name of the person who made a statement."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Found_Context_Online",
        "instruction": "Given a paragraph, identify the name of the seismologist mentioned in the report."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Found_Context_Online",
        "instruction": "Given a paragraph, identify the pseudonym used by Philip Arnold Heseltine."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Found_Context_Online",
        "instruction": "Given a name, provide a brief description of the person."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Guess_Answer",
        "instruction": "Given a question and an article, identify the name of the seismologist mentioned in the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Guess_Answer",
        "instruction": "Given a question and an article, identify the last name of the composer of songs and other vocal music mentioned in the article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Guess_Title_For_Context",
        "instruction": "Given a title, provide a context that matches the title."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Read_And_Extract_",
        "instruction": "Given a paragraph and a question, extract the answer for the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Read_And_Extract_",
        "instruction": "Given a paragraph, extract the name of a person who is mentioned in the paragraph."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-quoref_Read_And_Extract_",
        "instruction": "Given a name of a person, extract the paragraph where the person is mentioned."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_high_Select_the_best_answer",
        "instruction": "Given the article, select the correct answer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_high_Select_the_best_answer_generate_span_",
        "instruction": "Given a sentence, generate a span that best describes the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_high_Select_the_best_answer_no_instructions_",
        "instruction": "Given a sentence, choose the correct answer that best describes the scenario. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_high_Taking_a_test",
        "instruction": "Given a sentence with a missing word, choose the correct answer choice to complete the sentence. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_high_Taking_a_test",
        "instruction": "Given a sentence with a missing word, provide the missing word. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-race_high_Taking_a_test",
        "instruction": "Given a sentence, identify the correct answer choice."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_middle_Read_the_article_and_answer_the_question_no_option_",
        "instruction": "Given a sentence, choose the correct answer from the given options."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_middle_Read_the_article_and_answer_the_question_no_option_",
        "instruction": "Given a sentence, identify the emotion being expressed."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_middle_Select_the_best_answer",
        "instruction": "Given an article and a question with answer choices, select the best answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_middle_Select_the_best_answer",
        "instruction": "Given an article and a question with answer choices, extract the correct answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_middle_Select_the_best_answer_generate_span_",
        "instruction": "Given a short article and a question with answer choices, select the best answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_middle_Select_the_best_answer_generate_span_",
        "instruction": "Given a short article and a question with answer choices, generate a span that answers the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_middle_Select_the_best_answer_generate_span_",
        "instruction": "Given a short article and a question with answer choices, determine the sentiment of the answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_middle_Select_the_best_answer_no_instructions_",
        "instruction": "Given a short story and a question, choose the correct answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-race_middle_Write_a_multi_choice_question_for_the_following_article",
        "instruction": "Write a multi-choice question for the following article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_background_new_situation_answer",
        "instruction": "Given a situation where a substance is added to two cups of water, determine which cup has a higher concentration of the substance based on the amount added."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_background_new_situation_answer",
        "instruction": "Given a situation where a substance is added to two cups of water, determine which cup has a lower concentration of the substance based on the amount added."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_background_situation_middle",
        "instruction": "Given a situation and a hint, determine which cup has a higher concentration of sugar."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_background_situation_middle",
        "instruction": "Given a situation and a hint, determine which cup has a lower concentration of sugar."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_given_background_situation",
        "instruction": "Given the background and situation, determine the amount of sugar needed to make a drink with a certain concentration."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_given_background_situation",
        "instruction": "Given the background and situation, determine the amount of water needed to make a drink with a certain concentration."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_given_background_situation",
        "instruction": "Given the background and situation, determine the concentration of a drink with a certain amount of sugar."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_new_situation_background_answer",
        "instruction": "Given a situation where a substance is added to two cups of water, determine which cup has a higher concentration of the substance based on the amount added."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_new_situation_background_answer",
        "instruction": "Given a situation where a substance is added to two cups of water, determine which cup has a lower concentration of the substance based on the amount added."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_new_situation_background_answer",
        "instruction": "Given a situation where a substance is added to two cups of water, determine the difference in concentration between the two cups based on the amount added."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_plain_background_situation",
        "instruction": "Given a scenario where a substance is dissolved in water, determine which solution has a higher concentration of the substance."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_plain_background_situation",
        "instruction": "Given a scenario where a substance is dissolved in water and sugar is added, determine which solution has a lower concentration of sugar."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_plain_bottom_hint",
        "instruction": "Given a scenario where a child adds sugar to two cups of water, determine which cup has a higher concentration of sugar."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_plain_bottom_hint",
        "instruction": "Given a scenario where a child adds sugar to two cups of water, determine which cup has a lower concentration of sugar."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_plain_no_background",
        "instruction": "Given the amount of sugar added to each cup, determine which cup has a higher concentration of sugar."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_plain_no_background",
        "instruction": "Given the amount of sugar added to each cup, determine which cup has a lower concentration of sugar."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_plain_no_background",
        "instruction": "Given the concentration of sugar in each cup, determine how much sugar was added to each cup."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_prompt_bottom_hint_beginning",
        "instruction": "Calculate the concentration of sugar in each cup after the child added sugar."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_prompt_bottom_hint_beginning",
        "instruction": "What would happen to the concentration of sugar in cup A if the child added one more spoonful of sugar?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_prompt_bottom_no_hint",
        "instruction": "Given the amount of sugar poured into two cups, determine which cup has a higher concentration of sugar."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_prompt_bottom_no_hint",
        "instruction": "Given the amount of sugar poured into two cups, determine which cup has a lower concentration of sugar."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_prompt_bottom_no_hint",
        "instruction": "Given the amount of water and sugar poured into two cups, calculate the concentration of sugar in each cup."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_read_background_situation",
        "instruction": "Calculate the concentration of sugar in cup A and cup B after the child added sugar."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-ropes_read_background_situation",
        "instruction": "What would happen if the child added more sugar to cup A and cup B?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-samsum_Generate_a_summary_for_this_dialogue",
        "instruction": "Given a dialogue, generate a summary of the conversation."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-samsum_Given_the_above_dialogue_write_a_summary",
        "instruction": "Given a sentence with a question and an answer, generate a summary of the conversation."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-samsum_Given_the_above_dialogue_write_a_summary",
        "instruction": "Given a summary of a conversation, generate the question and answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-samsum_Sum_up_the_following_dialogue",
        "instruction": "Summarize the given dialogue into a single sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-samsum_Summarize_",
        "instruction": "Given a conversation between two people, summarize the key information in a coherent sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-samsum_Summarize_this_dialogue_",
        "instruction": "Summarize the conversation into a single sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-samsum_To_sum_up_this_dialog",
        "instruction": "Summarize the conversation in one sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-samsum_Write_a_dialogue_that_match_this_summary",
        "instruction": "Write a dialogue that matches this summary."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-sciq_Direct_Question",
        "instruction": "Given a paragraph and a question, choose the correct answer from the provided answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-sciq_Direct_Question_Closed_Book_",
        "instruction": "Given a set of answer choices and a question, choose the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-sciq_Direct_Question_Closed_Book_",
        "instruction": "Given a question, identify the phenomenon described in the question."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-sciq_Direct_Question_Closed_Book_",
        "instruction": "Given a phenomenon, provide a question that describes the phenomenon."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-sciq_Multiple_Choice",
        "instruction": "Given a paragraph and a question, choose the correct answer from the provided options."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-sciq_Multiple_Choice_Closed_Book_",
        "instruction": "Given the question and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-sciq_Multiple_Choice_Closed_Book_",
        "instruction": "Given the question, identify the phenomenon described."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "task_name": "bigscience/P3-sciq_Multiple_Choice_Closed_Book_",
        "instruction": "Given the correct answer, provide the question and answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-sciq_Multiple_Choice_Question_First",
        "instruction": "Choose the correct answer for the given question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-social_i_qa_Check_if_a_random_answer_is_valid_or_not",
        "instruction": "Given a question and a potential answer, determine if the answer is valid or not. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-social_i_qa_Generate_answer",
        "instruction": "Given a context and answer choices, predict how others would feel as a result of the context."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-social_i_qa_Generate_answer",
        "instruction": "Given a context and answer choices, predict what others would want to do next."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-social_i_qa_Generate_the_question_from_the_answer",
        "instruction": "Given a scenario and an answer, generate a question that would lead to that answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-social_i_qa_Generate_the_question_from_the_answer",
        "instruction": "Given a scenario and a question, generate an answer that would fit the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-social_i_qa_I_was_wondering",
        "instruction": "Given a scenario and answer choices, predict what others would prefer to do."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-social_i_qa_Show_choices_and_generate_answer",
        "instruction": "Given a context and answer choices, select the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-social_i_qa_Show_choices_and_generate_answer",
        "instruction": "Given a context and answer choices, identify the incorrect answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-social_i_qa_Show_choices_and_generate_index",
        "instruction": "Given a context and a question, choose the best answer from the provided options. Answers must be one of A, C, B."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-squad_v2_Questions_with_Context",
        "instruction": "Given a context, extract the name of Beyonce's debut album."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-squad_v2_Questions_with_Context_Without_Prompt_Keywords_unanswerable",
        "instruction": "Given a brief introduction of Beyonce, the task is to answer a specific question about her."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-squad_v2_Questions_with_Context_Without_Prompt_Keywords_unanswerable",
        "instruction": "Given a specific question about Beyonce, the task is to provide the answer based on the given information."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-squad_v2_Questions_with_Context_Without_Prompt_Keywords_unanswerable",
        "instruction": "Can you identify the number of Grammy Awards Beyonce won?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-squad_v2_Questions_with_Context_unanswerable",
        "instruction": "Answer the question based on the given context."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-squad_v2_Topic_Prediction_Context",
        "instruction": "Given a name, provide a brief description of the person."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-squad_v2_Topic_Prediction_Context_with_randomized_prompt_options",
        "instruction": "Given a topic, generate a paragraph about it."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-squad_v2_Topic_Prediction_Context_with_randomized_prompt_options_placed_in_the_end",
        "instruction": "Given a topic, generate a paragraph about it."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_GPT_3_Style",
        "instruction": "Given a sentence, identify the language it is written in."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_I_wonder_",
        "instruction": "Given a statement, determine if the answer is Yes or No. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_I_wonder_",
        "instruction": "Given a language, identify the countries where it is spoken. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_after_reading",
        "instruction": "Given a passage and a question, choose the correct answer. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_after_reading",
        "instruction": "Given a passage, generate a question that can be answered with True or False. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_based_on_the_following_passage",
        "instruction": "Given a passage and answer choices, determine whether the answer is \"Yes\" or \"No\". Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_based_on_the_following_passage",
        "instruction": "Given a passage, determine the meaning of a legal term or concept. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_based_on_the_previous_passage",
        "instruction": "Given a passage, determine the language spoken in a particular country. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_based_on_the_previous_passage",
        "instruction": "Given a description of a law, determine whether it offers legal protection to those who provide assistance. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_could_you_tell_me_",
        "instruction": "Given a piece of information, determine whether a certain statement is true or false. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_could_you_tell_me_",
        "instruction": "Given a statement, determine the piece of information it refers to."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_exercise",
        "instruction": "Given a text and a question, choose the correct answer. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_valid_binary",
        "instruction": "Given a statement and answer choices, determine whether the statement is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_valid_binary",
        "instruction": "Given a statement and answer choices, select the correct answer choice. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_yes_no_question",
        "instruction": "Given a text and a yes/no question, answer the question. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_boolq_yes_no_question",
        "instruction": "Given a text and an answer choice, determine if it is correct or not. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_GPT_3_style_score_eval",
        "instruction": "Given a sentence and a statement, determine whether the statement is true, false, or neither based on the information in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_GPT_3_style_score_eval",
        "instruction": "Given a sentence and a statement, generate a statement that is true based on the information in the sentence. Answers must be one of True, Neither, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_GPT_3_style_score_eval",
        "instruction": "Given a sentence and a statement, generate a statement that is false based on the information in the sentence. Answers must be one of True, Neither, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_MNLI_crowdsource_score_eval",
        "instruction": "Given a sentence and a statement, determine if the statement is correct, incorrect, or inconclusive based on the information in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_MNLI_crowdsource_score_eval",
        "instruction": "Given a sentence and a statement, provide the correct or incorrect label for the statement based on the information in the sentence. Answers must be one of Correct, Incorrect, Inconclusive."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_always_sometimes_never_score_eval",
        "instruction": "Given a statement and a condition, determine whether the statement is always, sometimes, or never true. Answers must be one of Always, Never, Sometimes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_always_sometimes_never_score_eval",
        "instruction": "Given a statement and its truth value, determine whether the statement is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_based_on_the_previous_passage_score_eval",
        "instruction": "Given a statement and a question, determine if the answer is \"Yes\", \"No\", or \"Maybe\". Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_based_on_the_previous_passage_score_eval",
        "instruction": "Given a statement and an answer, determine if the answer is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_can_we_infer_score_eval",
        "instruction": "Given a statement and a question, determine if the answer is \"Yes\", \"No\", or \"Maybe\". Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_can_we_infer_score_eval",
        "instruction": "Determine if the answer to the question is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_claim_true_false_inconclusive_score_eval",
        "instruction": "Given a statement and a claim, determine whether the claim is true, false, or inconclusive based on the information provided. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_claim_true_false_inconclusive_score_eval",
        "instruction": "Given a statement and a claim, provide the correct evaluation (true, false, or inconclusive). Answers must be one of True, Inconclusive, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_consider_always_sometimes_never_score_eval",
        "instruction": "Given a sentence and a statement, determine if the statement is always, sometimes, or never correct based on the information in the sentence. Answers must be one of Always, Never, Sometimes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_consider_always_sometimes_never_score_eval",
        "instruction": "Given a sentence and a statement, determine if the statement is true or false based on the information in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_consider_always_sometimes_never_score_eval",
        "instruction": "Given a statement and its correctness, generate a sentence that supports the statement."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_does_it_follow_that_score_eval",
        "instruction": "Given a statement and a conclusion, determine whether the conclusion follows logically from the statement. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_does_it_follow_that_score_eval",
        "instruction": "Given a statement and a conclusion, provide the correct conclusion that follows logically from the statement. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_does_this_imply_score_eval",
        "instruction": "Given a sentence and a question, determine whether the answer is \"Yes\", \"No\", or \"Maybe\". Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_does_this_imply_score_eval",
        "instruction": "Given a sentence and an answer, determine whether the answer is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_guaranteed_possible_impossible_score_eval",
        "instruction": "Given a sentence and a target word, determine whether the target word is guaranteed, possible, or impossible based on the information in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_guaranteed_true_score_eval",
        "instruction": "Given a statement and a question, determine if the answer is guaranteed true, false, or uncertain. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_guaranteed_true_score_eval",
        "instruction": "Given a statement and a question, provide the correct answer. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_justified_in_saying_score_eval",
        "instruction": "Given a sentence and a statement, determine if the statement is justified or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_must_be_true_score_eval",
        "instruction": "Given the input sentence, determine whether the statement is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_must_be_true_score_eval",
        "instruction": "Given the input sentence and the correctness of the statement, determine the correct answer. Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_should_assume_score_eval",
        "instruction": "Given a statement and a question, determine whether the answer is \"Yes\", \"No\", or \"Maybe\". Answers must be one of Yes, Maybe, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_should_assume_score_eval",
        "instruction": "Given a statement and an answer, determine whether the answer is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_cb_take_the_following_as_truth_score_eval",
        "instruction": "Given a statement and a description of a language, determine whether a given statement is true, false, or inconclusive. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_cb_take_the_following_as_truth_score_eval",
        "instruction": "Given a statement and a description of a language, provide the correct answer. Answers must be one of True, Inconclusive, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_C1_or_C2_premise_so_because_",
        "instruction": "Choose the correct answer based on the premise and the reason given."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_C1_or_C2_premise_so_because__score_eval",
        "instruction": "Choose the correct sentence that matches the given context. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa__As_a_result_C1_or_C2_",
        "instruction": "Given a sentence with a result, choose the correct cause from the given options."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa__As_a_result_C1_or_C2_",
        "instruction": "Given a sentence with a result and two possible causes, identify the correct cause."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa__As_a_result_C1_or_C2_",
        "instruction": "Given a sentence with a cause and two possible results, identify the correct result."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized",
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa__As_a_result_C1_or_C2__score_eval",
        "instruction": "Given a sentence with a result, choose the correct outcome from two options."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa__As_a_result_C1_or_C2__score_eval",
        "instruction": "Given a sentence with a result, generate two possible outcomes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa__As_a_result_C1_or_C2__score_eval",
        "instruction": "Given a sentence with a result, predict whether the outcome is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa__What_could_happen_next_C1_or_C2_",
        "instruction": "Given a scenario and two possible outcomes, choose the more likely outcome."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa__which_may_be_caused_by",
        "instruction": "Given a sentence with a shadow, choose the correct cause of the shadow."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa__which_may_be_caused_by",
        "instruction": "Given a sentence about a woman tolerating difficult behavior, choose the correct reason for her behavior."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa__which_may_be_caused_by_score_eval",
        "instruction": "Given a sentence and two possible causes, choose the correct cause. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa__which_may_be_caused_by_score_eval",
        "instruction": "Given a sentence and the correct cause, generate the corresponding sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa__why_C1_or_C2",
        "instruction": "Given a sentence and two answer choices, choose the correct answer that explains the reason for the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_best_option",
        "instruction": "Given a sentence and two options, choose the option that best explains the cause of the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_best_option_score_eval",
        "instruction": "Given a sentence and two options, choose the best option that explains the cause of the situation described in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_best_option_score_eval",
        "instruction": "Given a sentence and the correct option, generate the best option that explains the cause of the situation described in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_cause_effect",
        "instruction": "Given a sentence and two possible causes, choose the most plausible cause."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_choose",
        "instruction": "Given a sentence and two answer choices, choose the correct answer that best completes the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_choose_score_eval",
        "instruction": "Given a sentence and two options, choose the correct option that completes the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_choose_score_eval",
        "instruction": "Given a sentence and the correct option, generate the missing part of the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_exercise",
        "instruction": "Choose the most plausible alternative based on the given context."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_exercise",
        "instruction": "Rewrite the given context in a different way."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_exercise_score_eval",
        "instruction": "Given a sentence and two options, choose the most plausible alternative. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_i_am_hesitating",
        "instruction": "Given a sentence and two possible causes, choose the more likely cause."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_i_am_hesitating_score_eval",
        "instruction": "Given a sentence and the correct cause of a certain phenomenon, determine whether the cause is correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_more_likely_score_eval",
        "instruction": "Given a sentence and two possible reasons for the shadow cast over the grass, choose the correct reason. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_more_likely_score_eval",
        "instruction": "Given a sentence and the correct reason for the shadow cast over the grass, generate the sentence with the correct reason."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_copa_plausible_alternatives",
        "instruction": "Choose the more plausible option based on the given context."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_copa_plausible_alternatives_score_eval",
        "instruction": "Given a sentence and two options, choose the more plausible option. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_I_was_going_to_say_",
        "instruction": "Given a paragraph and a predicted answer, extract the reason why the answer is Yes or No. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_Would_it_be_good_to_answer_",
        "instruction": "Given a paragraph and the answer choices, predict the correct answer. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_confirm",
        "instruction": "Given a paragraph, identify the country that the paragraph is referring to. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_correct",
        "instruction": "Given a question and a text, choose the correct answer from the answer choices. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_found_this_answer",
        "instruction": "Given a paragraph and answer choices, predict the correct answer. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_grading",
        "instruction": "Given a paragraph and answer choices, predict the correct answer. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_is_a_correct_answer_",
        "instruction": "Given a sentence and answer choices, predict the correct answer. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_multirc_is_the_correct_answer_",
        "instruction": "Given a paragraph and the answer choices, predict the correct answer. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_Add_sentence_after_after_continuation_choices_",
        "instruction": "Given a sentence and a list of answer choices, choose the correct answer choice that matches the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_Add_sentence_after_after_continuation_choices_",
        "instruction": "Given a sentence, extract the name of the country mentioned in the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_Add_sentence_after_continuation_choices_",
        "instruction": "Given a sentence, choose the correct continuation sentence from the given answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_Can_you_figure_out_",
        "instruction": "Given a sentence and answer choices, choose the correct answer."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_Can_you_figure_out_",
        "instruction": "Given a name of a person or place, find a sentence that mentions it."
    },
    {
        "input_fields": [
            "answer_choices",
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_GPT_3_style_continuation_choices_",
        "instruction": "Given a set of answer choices and a context, choose the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_GPT_3_style_summary_only_continuation_choices_",
        "instruction": "Given a summary of a news article and answer choices, choose the correct answer."
    },
    {
        "input_fields": [
            "answer_choices",
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_GPT_3_style_with_labels_continuation_choices_",
        "instruction": "Given a set of answer choices and a paragraph, identify the correct answer choice that matches the paragraph."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_GPT_3_style_with_labels_without_hyphens_continuation_choices_",
        "instruction": "Given a sentence and a list of answer choices, choose the correct answer choice that matches the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_GPT_3_style_without_hyphens_continuation_choices_",
        "instruction": "Given a news article and a sentence from it, choose the correct sentence from a list of options."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_GPT_3_style_without_hyphens_continuation_choices_",
        "instruction": "Given a news article and a sentence from it, generate a plausible sentence that could appear in the article."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_GPT_3_style_without_hyphens_continuation_choices_",
        "instruction": "Given a sentence and a list of options, choose the correct option that completes the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_In_the_question_above_the_placeholder_stands_for",
        "instruction": "Given a sentence with a blank, choose the correct answer to fill in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_News_article_continuation_choices_",
        "instruction": "Given a news article and a sentence, choose the correct sentence to add to the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_News_article_continuation_choices_",
        "instruction": "Given a news article and a sentence, generate a new sentence that can be added to the article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_News_article_continuation_choices_",
        "instruction": "Given a sentence, identify the country or city mentioned in the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_Summary_first_continuation_choices_",
        "instruction": "Given a sentence, provide the corresponding target sentence."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_Summary_first_continuation_choices_",
        "instruction": "Given a target sentence, provide the corresponding input sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_What_could_the_placeholder_be_",
        "instruction": "Given a sentence with a missing word, choose the correct word from the answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_Which_one_is_the_placeholder_",
        "instruction": "Given a sentence with a missing word, choose the correct answer from the answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_choose_between",
        "instruction": "Given a sentence and a list of answer choices, choose the correct answer that completes the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_corrupted",
        "instruction": "Given a sentence with a placeholder, choose the correct word to fill in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_exercise",
        "instruction": "Given a sentence with a placeholder, choose the correct entity that the placeholder is referring to."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_pick_one_option",
        "instruction": "Given a sentence and a list of answer choices, choose the correct answer choice that completes the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_pick_one_option",
        "instruction": "Given a sentence, extract the reason why a person is in prison."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_the_placeholder_refers_to_",
        "instruction": "Given a sentence with a blank, choose the correct answer choice to fill in the blank."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_the_placeholder_refers_to_",
        "instruction": "Given a name of a person or place, provide a sentence that includes this name."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_record_trying_to_decide",
        "instruction": "Given a sentence and answer choices, choose the correct answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_GPT_3_style",
        "instruction": "Given a statement and a question, choose the correct answer. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_MNLI_crowdsource",
        "instruction": "Given a statement and answer choices, determine if the statement is definitely correct based on the information provided. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_MNLI_crowdsource",
        "instruction": "Given a statement and answer choices, determine if the statement is definitely incorrect based on the information provided. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_MNLI_crowdsource",
        "instruction": "Given a statement and answer choices, determine if the statement is possibly correct based on the information provided. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_MNLI_crowdsource_score_eval",
        "instruction": "Given a statement and a target answer, determine whether the statement is definitely correct or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_MNLI_crowdsource_score_eval",
        "instruction": "Given a statement and whether it is definitely correct or not, provide the target answer. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_based_on_the_previous_passage",
        "instruction": "Given a statement and answer choices, determine if the statement is true based on the previous passage. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_based_on_the_previous_passage",
        "instruction": "Given a statement and answer choices, determine if the statement is false based on the previous passage. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_based_on_the_previous_passage",
        "instruction": "Given a statement and answer choices, determine if the statement is ambiguous based on the previous passage. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_based_on_the_previous_passage_score_eval",
        "instruction": "Given a statement and a question based on the statement, determine whether the answer is \"Yes\" or \"No\". Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_based_on_the_previous_passage_score_eval",
        "instruction": "Given a statement and a question based on the statement, determine whether the answer is correct or incorrect. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_based_on_the_previous_passage_score_eval",
        "instruction": "Given a statement and a correct/incorrect answer, generate a question based on the statement. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_can_we_infer",
        "instruction": "Given a statement and a question, choose the correct answer. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_can_we_infer_score_eval",
        "instruction": "Given a statement and a question, determine whether the answer is yes or no based on the statement. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_can_we_infer_score_eval",
        "instruction": "Given a statement and the answer, determine whether the statement is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_does_it_follow_that",
        "instruction": "Given a statement and a conclusion, determine if the conclusion follows logically from the statement. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_does_it_follow_that_score_eval",
        "instruction": "Given a statement and a target, determine whether the target follows from the statement. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_does_it_follow_that_score_eval",
        "instruction": "Given a statement and a target, predict the target that does not follow from the statement. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_does_this_imply",
        "instruction": "Given a statement and a question, determine if the answer to the question can be inferred from the statement. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_does_this_imply_score_eval",
        "instruction": "Given a statement and a question, determine whether the answer is yes or no. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_does_this_imply_score_eval",
        "instruction": "Given a statement and a question, determine whether the answer is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_does_this_imply_score_eval",
        "instruction": "Given a question and an answer, determine whether the statement is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_guaranteed_true",
        "instruction": "Given a statement and a question, choose the correct answer from the answer choices. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_guaranteed_true",
        "instruction": "Given a statement and a question, determine whether the statement is guaranteed true or not. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_justified_in_saying",
        "instruction": "Given a statement and a question, choose the correct answer. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_justified_in_saying",
        "instruction": "Given a statement and a question, provide a justification for the statement. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_justified_in_saying_score_eval",
        "instruction": "Given a statement and a question, determine if the answer is \"Yes\" or \"No\". Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_justified_in_saying_score_eval",
        "instruction": "Determine if the statement is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_justified_in_saying_score_eval",
        "instruction": "Given a statement and a question, provide a justification for the answer. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_must_be_true",
        "instruction": "Given a statement and a conclusion, determine whether the conclusion must be true based on the statement. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_must_be_true_score_eval",
        "instruction": "Given a statement and a target answer, determine whether the statement implies the target answer. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_must_be_true_score_eval",
        "instruction": "Given a statement and a target answer, determine the target answer that is not implied by the statement. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_should_assume",
        "instruction": "Given a statement and a question, choose the correct answer from the given options. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_should_assume",
        "instruction": "Given a statement and a question, determine whether the statement is true or false. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_rte_should_assume_score_eval",
        "instruction": "Given a statement and a question, determine the correct answer. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_rte_should_assume_score_eval",
        "instruction": "Given a statement and a question, provide the correct answer. Answers must be one of Yes, No."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_GPT_3_prompt",
        "instruction": "Identify the word that is used in different senses in two given sentences. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_GPT_3_prompt_score_eval",
        "instruction": "Given a sentence and a question, determine if a specific word is used in the same sense in both the sentence and the question. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_GPT_3_prompt_with_label",
        "instruction": "Determine if two sentences use a specific word in the same sense. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_GPT_3_prompt_with_label_score_eval",
        "instruction": "Given a sentence and a question about the usage of a specific word, determine whether the word is used in the same sense in both sentences. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_GPT_3_prompt_with_label_score_eval",
        "instruction": "Given a sentence and a question about the usage of a specific word, provide the correct answer to the question. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_affirmation_true_or_false",
        "instruction": "Determine whether the two sentences have a word with similar meaning. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_affirmation_true_or_false_score_eval",
        "instruction": "Given two sentences and a word, determine if the word has a similar meaning in both sentences. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_affirmation_true_or_false_score_eval",
        "instruction": "Given two sentences and a word, determine if the word has a different meaning in both sentences. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_grammar_homework",
        "instruction": "Determine whether two sentences use a given word with the same meaning. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_grammar_homework_score_eval",
        "instruction": "Given two sentences, decide whether a specific word is used with the same meaning in both sentences. Answer by yes or no. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_grammar_homework_score_eval",
        "instruction": "Given two sentences and the answer to whether a specific word is used with the same meaning in both sentences, identify which answer is correct. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_polysemous",
        "instruction": "Determine if a word has the same meaning in two different sentences. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_polysemous_score_eval",
        "instruction": "Given a word with multiple meanings and two sentences containing the word, determine if the word has the same meaning in both sentences. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_polysemous_score_eval",
        "instruction": "Given a word with multiple meanings and two sentences containing the word, identify if the word has the same meaning in both sentences. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context",
        "instruction": "Determine if the given word is used in the same way in the two sentences. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context",
        "instruction": "Choose the correct usage of the given word in the sentence. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context_meaning",
        "instruction": "Determine if two sentences use the word \"place\" with the same meaning. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context_meaning",
        "instruction": "Determine if two sentences use the word \"approach\" with the same meaning. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context_meaning_with_label",
        "instruction": "Determine if two sentences use the same meaning of a given word. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_question_context_meaning_with_label",
        "instruction": "Identify the sentence that uses a different meaning of a given word. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_same_sense",
        "instruction": "Determine whether the given word is used in the same sense in both sentences. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_same_sense",
        "instruction": "Choose the correct word that is used in the same sense in both sentences. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_same_sense_score_eval",
        "instruction": "Given two sentences and a word, determine whether the word is used in the same sense in both sentences. Yes or no? Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_same_sense_score_eval",
        "instruction": "Given two sentences and a word, determine whether the word is used in the opposite sense in both sentences. Yes or no? Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_same_sense_score_eval",
        "instruction": "Given a sentence and a target word, determine whether the target word is used in the same sense as in the sentence. Yes or no? Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_similar_sense",
        "instruction": "Given a sentence and a list of answer choices, determine whether the sentence has a similar sense of place as the answer choice. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wic_similar_sense",
        "instruction": "Given a sentence and a list of answer choices, determine whether the sentence has a similar sense of approach as the answer choice. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_similar_sense_score_eval",
        "instruction": "Given a sentence and a word, determine if the word has a similar sense as the context of the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wic_similar_sense_score_eval",
        "instruction": "Given a sentence and a word, determine if the word has a different sense as the context of the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_GPT_3_Style",
        "instruction": "Given a passage and a question, choose the correct answer from the answer choices. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_GPT_3_Style",
        "instruction": "Given a passage and a pronoun, determine whether the pronoun refers to a specific noun in the passage. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_GPT_3_Style_score_eval",
        "instruction": "Given a passage and a question, determine if the pronoun \"He\" refers to the subject mentioned in the passage. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_GPT_3_Style_score_eval",
        "instruction": "Given a passage and a question, determine if the answer to the question is \"Yes\" or \"No\". Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_I_think_they_mean",
        "instruction": "Determine whether the given statement is correct or not. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_I_think_they_mean_score_eval",
        "instruction": "Given a sentence with a quoted statement, determine whether the quoted statement accurately reflects the intended meaning of the speaker. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_I_think_they_mean_score_eval",
        "instruction": "Given a sentence with a quoted statement, identify the intended meaning of the speaker. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_Who_or_what_is_are",
        "instruction": "Given a sentence and a question, determine the answer from the answer choices. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_by_p_they_mean_score_eval",
        "instruction": "Given a sentence with a pronoun and its antecedent, determine if the pronoun refers to the correct antecedent. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_by_p_they_mean_score_eval",
        "instruction": "Given a sentence with a pronoun and its antecedent, identify the antecedent of the pronoun. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_does_p_stand_for",
        "instruction": "Given a sentence and a pronoun, determine whether the pronoun refers to the correct antecedent. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_does_p_stand_for_score_eval",
        "instruction": "Given a sentence and a pronoun, determine whether the pronoun refers to a specific entity in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_does_p_stand_for_score_eval",
        "instruction": "Given a sentence and a pronoun, identify the entity that the pronoun refers to. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_does_the_pronoun_refer_to",
        "instruction": "Determine whether the pronoun refers to the correct antecedent in a given sentence. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_does_the_pronoun_refer_to",
        "instruction": "Identify the antecedent of a given pronoun in a sentence. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_does_the_pronoun_refer_to_score_eval",
        "instruction": "Given a sentence with a pronoun, determine whether the pronoun refers to the correct antecedent. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_does_the_pronoun_refer_to_score_eval",
        "instruction": "Given a sentence with a pronoun and a question about the antecedent, provide the correct answer. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_in_other_words",
        "instruction": "Rewrite the sentence in other words. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_in_other_words_score_eval",
        "instruction": "Given a sentence and its paraphrase, determine whether the paraphrase is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "is_correct"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_in_other_words_score_eval",
        "instruction": "Given a sentence and its paraphrase, provide the correct paraphrase. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_p_is_are_r",
        "instruction": "Given a context and a question, determine if the pronoun refers to the correct antecedent. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_p_is_are_r",
        "instruction": "Given a context and a question, identify the antecedent of the pronoun. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_replaced_with",
        "instruction": "Given a sentence with a pronoun, determine if the pronoun can be replaced with a specific noun. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_replaced_with",
        "instruction": "Given a sentence with a pronoun and a possible replacement noun, determine if the replacement is correct. Answers must be one of No, Yes."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_replaced_with_score_eval",
        "instruction": "Given a sentence with a pronoun, determine whether the pronoun can be replaced with a specific noun. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_replaced_with_score_eval",
        "instruction": "Given a sentence with a pronoun and a specific noun, determine whether the pronoun can be replaced with the noun. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_the_pronoun_refers_to",
        "instruction": "Determine whether the pronoun in the given sentence refers to the correct antecedent. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_the_pronoun_refers_to",
        "instruction": "Identify the antecedent of the pronoun in the given sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_the_pronoun_refers_to_score_eval",
        "instruction": "Given a sentence with a pronoun, determine whether the pronoun refers to the correct antecedent. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-super_glue_wsc.fixed_the_pronoun_refers_to_score_eval",
        "instruction": "Given a sentence with a pronoun and a statement about the antecedent, determine whether the statement is true or false. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_ABBR",
        "instruction": "Given a full form, choose the correct abbreviation."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_ABBR",
        "instruction": "Given an abbreviation, provide the full form."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_ABBR_context_first",
        "instruction": "Given an abbreviation, provide the full form."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_DESC_context_first",
        "instruction": "Given a question and answer choices, identify the type of information being asked for."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_ENTY",
        "instruction": "Given a question and a list of possible entity types, identify the correct entity type being asked for."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_ENTY",
        "instruction": "Given a film title, identify the main character."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_ENTY",
        "instruction": "Given a question and a list of possible animals, identify the correct animal being asked for."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_LOC",
        "instruction": "Given a question and answer choices, predict the correct answer choice. Answers must be one of state, mountain, country, city, other location."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_LOC_context_first",
        "instruction": "Given a question and answer choices, identify the type of location being asked for. Answers must be one of state, mountain, country, city, other location."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_LOC_context_first",
        "instruction": "Given a location, provide a question that could be asked about it."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_NUM",
        "instruction": "Given a question, identify the type of answer it is asking for."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_NUM_context_first",
        "instruction": "Given a question, provide the answer type."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_fine_grained_open",
        "instruction": "Given a question, predict the type of creative piece it is asking for."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_pick_the_best_descriptor",
        "instruction": "Given a question and a list of possible descriptors, choose the best descriptor for the question. Answers must be one of Entity, Description, Quantity, Location, Abbreviation, Person."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_trec1",
        "instruction": "Given a question and a list of answer choices, identify the type of information being asked for. Answers must be one of Entity, Description, Quantity, Location, Abbreviation, Person."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_trec1",
        "instruction": "Given a question, identify the entity being asked for. Answers must be one of Entity, Description, Quantity, Location, Abbreviation, Person."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_what_category_best_describe",
        "instruction": "Given a category and a question, choose the best category that describes the question. Answers must be one of Entity, Description, Quantity, Location, Abbreviation, Person."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_which_category_best_describes",
        "instruction": "Given a question, choose the correct category that best describes it. Answers must be one of Entity, Description, Quantity, Location, Abbreviation, Person."
    },
    {
        "input_fields": [
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-trec_which_category_best_describes",
        "instruction": "Given a category, generate a question that fits into this category."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trivia_qa_unfiltered_first_person_context",
        "instruction": "Given a question about a historical event or fact, provide the correct answer."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-trivia_qa_unfiltered_first_person_context",
        "instruction": "Given an answer to a historical question, provide the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trivia_qa_unfiltered_formal_description",
        "instruction": "Given an English question, predict the corresponding English answer string."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-trivia_qa_unfiltered_formal_description",
        "instruction": "Given an English answer string, predict the corresponding English question."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-trivia_qa_unfiltered_guess_question",
        "instruction": "Guess the answer based on the given question."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trivia_qa_unfiltered_guess_question",
        "instruction": "Guess the question based on the given answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trivia_qa_unfiltered_question_answer",
        "instruction": "Given a question, provide the corresponding answer."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-trivia_qa_unfiltered_question_answer",
        "instruction": "Given an answer, provide the corresponding question."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-trivia_qa_unfiltered_question_with_instruction",
        "instruction": "Answer the trivia question."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-trivia_qa_unfiltered_question_with_instruction",
        "instruction": "Rewrite the trivia question in a different format."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-web_questions_get_the_answer",
        "instruction": "Given a question, provide the correct answer."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-web_questions_get_the_answer",
        "instruction": "Given an answer, provide the question that corresponds to it."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-web_questions_potential_correct_answer",
        "instruction": "Given a question, provide a possible correct answer."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-web_questions_potential_correct_answer",
        "instruction": "Given a possible correct answer, provide the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-web_questions_question_answer",
        "instruction": "Given a name, provide the name of Justin Bieber's brother."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-web_questions_question_answer",
        "instruction": "Given a character name from Star Wars, provide the name of the actor/actress who played the character."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-web_questions_short_general_knowledge_q",
        "instruction": "Given a question, provide the correct answer."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-web_questions_short_general_knowledge_q",
        "instruction": "Given an answer, provide the corresponding question."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-web_questions_whats_the_answer",
        "instruction": "Given a question, provide the correct answer."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-web_questions_whats_the_answer",
        "instruction": "Given an answer, provide the question that was asked."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_bio_comprehension",
        "instruction": "Given a bio, extract the name, birth date, nationality, and occupation of the person."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_bio_comprehension",
        "instruction": "Given a bio, extract the clubs, caps, position, years, height, youthclubs, youthyears, pcupdate, fullname, birth place, and goals of the person."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_bio_guess_person",
        "instruction": "Given the birth date, nationality, and occupation of a person, guess who this information could be about."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_bio_guess_person",
        "instruction": "Given the clubs, caps, position, years, height, youthclubs, youthyears, pcupdate, birth date, fullname, birth place, and goals of a person, guess who this information could be about."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_bio_key_content",
        "instruction": "Extract the birth date, name, nationality, and occupation of a person from their bio."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_bio_key_content",
        "instruction": "Extract the clubs, caps, position, years, height, youthclubs, youthyears, pcupdate, fullname, birth place, and goals of a soccer player from their bio."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_bio_who",
        "instruction": "Given a set of bullet points about a person, write a short biography describing their life."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_bio_who",
        "instruction": "Extract the birth date, name, nationality, occupation, and other relevant information from the biography."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_choose_best_object_affirmative_1",
        "instruction": "Choose the correct answer based on the given information."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_choose_best_object_affirmative_1",
        "instruction": "Given a target answer, provide the information that supports it."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_choose_best_object_affirmative_2",
        "instruction": "Choose the correct answer based on the given information."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_choose_best_object_affirmative_2",
        "instruction": "Given a target answer, provide the information that supports it."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_choose_best_object_affirmative_3",
        "instruction": "Choose the correct answer based on the given information."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_choose_best_object_affirmative_3",
        "instruction": "Given a target answer, provide the information that supports it."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_choose_best_object_interrogative_1",
        "instruction": "Given a set of answer choices and information, choose the correct answer for the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_choose_best_object_interrogative_1",
        "instruction": "Given a set of information, generate a question that can be answered by one of the answer choices."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_choose_best_object_interrogative_2",
        "instruction": "Given a set of answer choices and information, choose the correct answer for the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_generate_object",
        "instruction": "Given a set of information about a major sporting event, identify the year of the Summer Olympics that preceded it."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_generate_object",
        "instruction": "Given a set of information about a Christian megachurch, identify the senior pastor and their role."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_hop_original_generate_object",
        "instruction": "Given a set of information about a country, identify its official language."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_qa_Direct_Answer_to_Question",
        "instruction": "Given a sentence describing a phenomenon, the task is to provide a question that can be answered by the sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_qa_Direct_Answer_to_Question",
        "instruction": "Given a question, the task is to provide a direct answer to the question."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_qa_Generate_Question_from_Topic",
        "instruction": "Generate a question about a given topic whose answer is provided in the input."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_qa_Generate_Question_from_Topic",
        "instruction": "Given a question and its answer, generate a topic that the question is about."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_qa_Topic_Prediction_Answer_Only",
        "instruction": "Given a topic, generate a passage that fits the topic."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_qa_Topic_Prediction_Answer_Only",
        "instruction": "Given a passage, generate a question that can be answered by the passage."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_qa_Topic_Prediction_Question_Only",
        "instruction": "Given a topic, generate a question related to the topic."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_qa_Topic_Prediction_Question_Only",
        "instruction": "Given a question, determine the unit of measurement used in the question."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiki_qa_automatic_system",
        "instruction": "Generate a new question based on the given answer."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_Replace",
        "instruction": "Given a sentence with a blank, choose the correct option to fill in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_Replace",
        "instruction": "Given a sentence with two named entities and their diagnoses, identify which entity had a specific symptom."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_Replace_score_eval",
        "instruction": "Given a sentence with a blank, choose the correct option to fill in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_Replace_score_eval",
        "instruction": "Determine whether the correct option was chosen or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_Replace_score_eval",
        "instruction": "Provide a sentence with a blank and two options to fill in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_does_underscore_refer_to",
        "instruction": "Given a sentence with a blank, choose the correct word to fill in the blank based on the context."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_does_underscore_refer_to",
        "instruction": "Given a sentence with a blank and two options, determine which option correctly fills in the blank based on the context."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_does_underscore_refer_to",
        "instruction": "Given a sentence with two named entities and their respective diagnoses, determine which named entity is being referred to in a subsequent sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_does_underscore_refer_to_score_eval",
        "instruction": "Given a sentence with a blank, determine whether the blank refers to a specific word in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_fill_in_the_blank",
        "instruction": "Given a sentence with a blank, choose the correct word to fill in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_fill_in_the_blank",
        "instruction": "Given a sentence with two named entities and their diagnoses, choose the correct entity that matches a given description."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_fill_in_the_blank_score_eval",
        "instruction": "Given a sentence with a blank, choose the correct word to fill in the blank. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_fill_in_the_blank_score_eval",
        "instruction": "Given a sentence with a blank and two choices, choose the incorrect word to fill in the blank. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_fill_in_the_blank_score_eval",
        "instruction": "Given a sentence with a blank and two choices, choose the correct word to fill in the blank. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_stand_for",
        "instruction": "Given a sentence with a blank, choose the correct word to fill in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_stand_for_score_eval",
        "instruction": "Given a sentence with a blank, determine whether the blank stands for a specific word or phrase. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_stand_for_score_eval",
        "instruction": "Given a sentence with a blank, identify the word or phrase that the blank stands for."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_underscore_refer_to",
        "instruction": "Given a sentence with a blank, choose the correct word to fill in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_underscore_refer_to",
        "instruction": "Given a sentence with a blank and two answer choices, determine which answer choice correctly fills in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_underscore_refer_to",
        "instruction": "Given a sentence with two named entities and their respective diagnoses, determine which named entity had a specific symptom."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_underscore_refer_to_score_eval",
        "instruction": "Given a sentence with a blank, choose the correct word to fill in the blank based on the context."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_underscore_refer_to_score_eval",
        "instruction": "Given a sentence with a blank and two options, choose the correct option that fills in the blank based on the context. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_debiased_underscore_refer_to_score_eval",
        "instruction": "Given a sentence and a word, determine whether the word is the correct reference for the blank in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_Replace",
        "instruction": "Given a sentence with a blank, choose the correct option to fill in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_Replace_score_eval",
        "instruction": "Given a sentence with a blank, choose the correct option to fill in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_Replace_score_eval",
        "instruction": "Given a sentence with a blank, determine whether the correct option to fill in the blank is True or False. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_does_underscore_refer_to",
        "instruction": "Given a sentence with a blank, choose the correct answer choice to fill in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_does_underscore_refer_to",
        "instruction": "Given a sentence with a blank and two answer choices, determine which answer choice correctly fills in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_does_underscore_refer_to",
        "instruction": "Given a sentence with a blank and two possible referents, determine which referent correctly fills in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_does_underscore_refer_to_score_eval",
        "instruction": "Given a sentence with a blank, determine whether the blank refers to the first or second person mentioned in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_does_underscore_refer_to_score_eval",
        "instruction": "Given a sentence with a blank, determine which person the blank refers to."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_fill_in_the_blank",
        "instruction": "Fill in the blank in the given sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_fill_in_the_blank",
        "instruction": "Identify the person who enjoyed/eating intestine."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "targets_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_fill_in_the_blank_score_eval",
        "instruction": "Given a sentence with a blank, choose the correct word to fill in the blank. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_stand_for",
        "instruction": "Given a sentence with a blank, choose the correct answer choice to fill in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_stand_for_score_eval",
        "instruction": "Given a sentence with a blank, determine whether the blank stands for the first or second person mentioned in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_stand_for_score_eval",
        "instruction": "Given a sentence with a blank, determine which person the blank stands for."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_underscore_refer_to",
        "instruction": "Given a sentence with a blank, choose the correct answer choice to fill in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_underscore_refer_to",
        "instruction": "Given a sentence with a blank and two answer choices, determine which answer choice correctly fills in the blank."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "is_correct"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_underscore_refer_to_score_eval",
        "instruction": "Given a sentence with a blank, determine whether the blank refers to the first or second person mentioned in the sentence. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-winogrande_winogrande_xl_underscore_refer_to_score_eval",
        "instruction": "Given a sentence with a blank, determine which person the blank refers to."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_effect_with_label_answer",
        "instruction": "Given a process and a question about the process, choose the correct answer. Answers must be one of A, C, B."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_effect_with_label_answer",
        "instruction": "Given a process and a question about the process, determine the effect of a hypothetical scenario on the outcome. Answers must be one of A, C, B."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_what_is_the_final_step_of_the_following_process",
        "instruction": "Given a process with multiple steps, identify the step that comes last."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_what_is_the_final_step_of_the_following_process",
        "instruction": "Given the final step of a process, identify the steps that come before it."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_what_is_the_missing_first_step",
        "instruction": "What is the missing step in the given process?"
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_what_is_the_missing_first_step",
        "instruction": "Given the missing step, can you provide the complete process?"
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_what_might_be_the_first_step_of_the_process",
        "instruction": "Given a set of steps in a process, identify the first step."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_what_might_be_the_first_step_of_the_process",
        "instruction": "Given the first step of a process, identify the final outcome."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_what_might_be_the_last_step_of_the_process",
        "instruction": "Given a set of steps in a process, identify the step that repeats itself over and over again."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_what_might_be_the_last_step_of_the_process",
        "instruction": "Given a set of steps in a process, identify the last step of the process."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_what_might_be_the_last_step_of_the_process",
        "instruction": "Given the last step of a process, identify the initial steps of the process."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_which_of_the_following_is_the_supposed_perturbation",
        "instruction": "Given a process and a perturbation, identify the step of the process that is directly impacted."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_which_of_the_following_is_the_supposed_perturbation",
        "instruction": "Given a process and a perturbation, identify the step of the process that is indirectly impacted."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-wiqa_which_of_the_following_is_the_supposed_perturbation",
        "instruction": "Given a process and a perturbation, identify if any step of the process is impacted."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_DOC_tldr",
        "instruction": "Given a news article, summarize it in a few sentences."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_DOC_write_summary_of_above",
        "instruction": "Given a news article, generate a headline that summarizes the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_summarize_DOC",
        "instruction": "Given a news article, generate a headline that summarizes the article."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_summarize_DOC",
        "instruction": "Given a news article headline, provide a brief summary of the article."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-xsum_summarize_this_DOC_summary",
        "instruction": "Given a document, summarize it into a single sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_based_on_that",
        "instruction": "Given a review of a doctor, predict the rating given by the reviewer. Answers must be one of 2 stars, 4 stars, 3 stars, 1 star, 5 stars."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_based_on_that",
        "instruction": "Given a review of a doctor, predict the rating given by the reviewer based on the answer choices. Answers must be one of 2 stars, 4 stars, 3 stars, 1 star, 5 stars."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_based_on_that",
        "instruction": "Given a rating, provide a review of a doctor."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_format_rating",
        "instruction": "Given a review text, predict the rating of the review. Answers must be one of 2 stars, 4 stars, 3 stars, 1 star, 5 stars."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_format_score",
        "instruction": "Given a review text, predict the score of the review. Answers must be one of 3, 4, 5, 2, 1."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_format_star",
        "instruction": "Given a review text and answer choices, predict the number of stars given to the doctor. Answers must be one of 2 stars, 4 stars, 3 stars, 1 star, 5 stars."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_on_a_scale",
        "instruction": "Given a review, predict the rating on a scale of 1 to 5. Answers must be one of 3, 4, 5, 2, 1."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_on_a_scale",
        "instruction": "Given a review and its rating, predict the rating on a scale of 1 to 5. Answers must be one of 3, 4, 5, 2, 1."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_so_i_would",
        "instruction": "Given a review and its rating, predict the rating of the review. Answers must be one of 2 stars, 4 stars, 3 stars, 1 star, 5 stars."
    },
    {
        "input_fields": [
            "targets_pretokenized"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_so_i_would",
        "instruction": "Given a review rating, generate a review that matches the rating."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-yelp_review_full_this_place",
        "instruction": "Given a review of a doctor, predict the rating given by the reviewer. Answers must be one of 2 stars, 4 stars, 3 stars, 1 star, 5 stars."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "argilla/news-summary",
        "instruction": "Given a news article, generate a summary of the article."
    },
    {
        "input_fields": [
            "target"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "argilla/news-summary",
        "instruction": "Given a news summary, generate a headline for the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "squad",
        "instruction": "Given a context, the task is to generate a question that can be answered from the context."
    },
    {
        "input_fields": [
            "question",
            "context"
        ],
        "output_field": [
            "title"
        ],
        "task_name": "adversarial_qa-adversarialQA",
        "instruction": "Given a question and context, predict the title of the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "adversarial_qa-dbert",
        "instruction": "Given a context, generate a question based on the context."
    },
    {
        "input_fields": [
            "plot",
            "title"
        ],
        "output_field": [
            "question",
            "answers"
        ],
        "task_name": "duorc-SelfRC",
        "instruction": "Given a plot and a title, generate a question that can be answered by the plot."
    },
    {
        "input_fields": [
            "question",
            "answers"
        ],
        "output_field": [
            "plot",
            "title"
        ],
        "task_name": "duorc-SelfRC",
        "instruction": "Given a question and answers, generate a plot and a title that can answer the question."
    },
    {
        "input_fields": [
            "plot"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "duorc-ParaphraseRC",
        "instruction": "Given a plot, generate a question based on the plot."
    },
    {
        "input_fields": [
            "sentence_bad"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-determiner_noun_agreement_1",
        "instruction": "Given a sentence with incorrect determiner-noun agreement, correct the sentence."
    },
    {
        "input_fields": [
            "linguistics_term"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-wh_questions_subject_gap",
        "instruction": "Identify the linguistic term for the filler-gap dependency in a wh-question with a subject gap."
    },
    {
        "input_fields": [
            "simple_LM_method",
            "one_prefix_method",
            "two_prefix_method"
        ],
        "output_field": [
            "sentence_good"
        ],
        "task_name": "blimp-wh_questions_subject_gap",
        "instruction": "Identify the method used to generate the correct sentence."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_only_question_answer",
        "instruction": "Given a question and answer choices, choose the correct answer."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-cosmos_qa_only_question_answer",
        "instruction": "Given a correct answer and answer choices, generate a question that fits the answer."
    },
    {
        "input_fields": [
            "targets_pretokenized",
            "answer_choices"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "bigscience/P3-dbpedia_14_given_a_choice_of_categories_",
        "instruction": "Given a category, provide a text that belongs to that category."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answerKey"
        ],
        "task_name": "ai2_arc-ARC-Challenge",
        "instruction": "Given a question, predict the correct answer. Answers must be one of A, D, B, C, 2."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answerKey"
        ],
        "task_name": "ai2_arc-ARC-Easy",
        "instruction": "Identify the correct answer for the given question. Answers must be one of A, D, B, 3, C, 1."
    },
    {
        "input_fields": [
            "answerKey"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "ai2_arc-ARC-Easy",
        "instruction": "Provide the question for the given answer."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answerKey"
        ],
        "additional_input": {
            "context": "A paragraph describing the causes of fever."
        },
        "task_name": "ai2_arc-ARC-Easy",
        "instruction": "Identify the factor that causes a person to develop a fever. Answers must be one of A, D, B, 3, C, 1."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answerKey"
        ],
        "additional_input": {
            "context": "A paragraph describing the symbiotic relationship of lichens."
        },
        "task_name": "ai2_arc-ARC-Easy",
        "instruction": "Identify what the green algae supply to the fungi in the symbiotic relationship of lichens. Answers must be one of A, D, B, 3, C, 1."
    },
    {
        "input_fields": [
            "context",
            "question",
            "answer0",
            "answer1",
            "answer2",
            "answer3"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "cosmos_qa",
        "instruction": "Given a context and a question, choose the correct answer from the given options. Answers must be one of 2, 0, 3, 1."
    },
    {
        "input_fields": [
            "context",
            "question"
        ],
        "output_field": [
            "answer1"
        ],
        "task_name": "cosmos_qa",
        "instruction": "Given a context and a question, provide the correct answer."
    },
    {
        "input_fields": [
            "context",
            "question"
        ],
        "output_field": [
            "answer1",
            "answer0",
            "answer2",
            "answer3"
        ],
        "task_name": "cosmos_qa",
        "instruction": "Given a context and a question, provide the rationale for the correct answer."
    },
    {
        "input_fields": [
            "question",
            "context"
        ],
        "output_field": [
            "question_type"
        ],
        "task_name": "quail",
        "instruction": "Given a question and its context, predict the question type."
    },
    {
        "input_fields": [
            "title",
            "content"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "amazon_polarity",
        "instruction": "Given a set of keywords, the task is to predict the sentiment of the review. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "question",
            "supports"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "wiki_hop-original",
        "instruction": "Given a question, identify the event or competition mentioned in the supports section."
    },
    {
        "input_fields": [
            "supports"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "wiki_hop-original",
        "instruction": "Given a name, identify the language spoken or written by the person mentioned in the supports section."
    },
    {
        "input_fields": [
            "supports"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "wiki_hop-masked",
        "instruction": "Fill in the blank with the missing word in the given sentence(s)."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "wiki_hop-masked",
        "instruction": "Given a question, the task is to provide the answer."
    },
    {
        "input_fields": [
            "concepts"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "common_gen",
        "instruction": "Rearrange the given concepts to form a new sentence."
    },
    {
        "input_fields": [
            "fact1",
            "question"
        ],
        "output_field": [
            "answerKey"
        ],
        "task_name": "qasc",
        "instruction": "Given a fact and a question, determine if the fact supports or refutes the question. Answers must be one of H, A, C, F, B, D, E, G."
    },
    {
        "input_fields": [
            "fact2"
        ],
        "output_field": [
            "formatted_question"
        ],
        "task_name": "qasc",
        "instruction": "Given a fact and a question, generate a new question that can be answered by the fact."
    },
    {
        "input_fields": [
            "question",
            "answerKey"
        ],
        "output_field": [
            "para"
        ],
        "task_name": "quartz",
        "instruction": "Given a question and its answer key, identify the corresponding paragraph that provides the context for the question."
    },
    {
        "input_fields": [
            "question",
            "para"
        ],
        "output_field": [
            "answerKey"
        ],
        "task_name": "quartz",
        "instruction": "Given a question and its corresponding paragraph, identify the answer key. Answers must be one of A, B."
    },
    {
        "input_fields": [
            "title"
        ],
        "output_field": [
            "content"
        ],
        "task_name": "dbpedia_14",
        "instruction": "Given a company name, provide its description."
    },
    {
        "input_fields": [
            "content"
        ],
        "output_field": [
            "title"
        ],
        "task_name": "dbpedia_14",
        "instruction": "Given a company description, provide its name."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "gsm8k-main",
        "instruction": "Calculate the total number of clips sold by Natalia in April and May."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "gsm8k-main",
        "instruction": "Calculate the hourly rate of Weng for babysitting and how much she earned for a given time."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-english",
        "instruction": "Given a sentence, choose the correct version of a phrase that is grammatically correct and makes sense."
    },
    {
        "input_fields": [
            "title_body",
            "upvoted_answer"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-english",
        "instruction": "Given a sentence, provide an explanation for why a certain phrase is incorrect."
    },
    {
        "input_fields": [
            "title_body",
            "downvoted_answer"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-english",
        "instruction": "Given a sentence, provide an explanation for why a certain phrase is correct."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-academia",
        "instruction": "Given a question about academic titles, provide an answer that explains the requirements for being called \"Professor\" in the United States of America."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-academia",
        "instruction": "Given a question about pursuing a degree in computer science, provide an answer that advises whether to pursue an undergraduate or graduate degree."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-academia",
        "instruction": "Given a question about academic titles, provide an answer that explains the difference between \"Assistant Professor\", \"Associate Professor\", and \"Full Professor\"."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-academia",
        "instruction": "Given a question about pursuing a degree in computer science, provide an answer that advises whether to pursue certification training/bootcamps or an undergraduate degree."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-christianity",
        "instruction": "Given a sentence, identify the upvoted answer and downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-apple",
        "instruction": "Given a title and body of a post, predict whether the post will be upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-electronics",
        "instruction": "Given a title and body of a question, predict whether the corresponding answer will be upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gaming",
        "instruction": "Given a question about 7 Days to Die, provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gaming",
        "instruction": "Given a question about finding end ships in Minecraft, provide a solution."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-ell",
        "instruction": "Given a sentence with a date, determine whether to use \"the\" before the date and provide an explanation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-ell",
        "instruction": "Given a sentence with the word \"once\", determine the correct tense to use and provide an explanation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-judaism",
        "instruction": "Given a title and body of a question, predict whether the answer will be upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-diy",
        "instruction": "Given a question about electrical wiring, choose the best answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-diy",
        "instruction": "Given a question about removing polyurethane spray foam, choose the best answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-diy",
        "instruction": "Given a question about electrical wiring, provide a warning or caveat."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-diy",
        "instruction": "Given a question about removing polyurethane spray foam, provide an alternative solution."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-history",
        "instruction": "Given a sentence, predict whether it is upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-islam",
        "instruction": "Given a question related to Islam, provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-islam",
        "instruction": "Given a question related to Islam, provide the corresponding downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-dba",
        "instruction": "Given a database error message, can you identify the cause of the error?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-dba",
        "instruction": "Given a database error message, can you provide a solution to the error?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gamedev",
        "instruction": "Given a title and body of a question, provide a recommended resource to learn more about the topic."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gamedev",
        "instruction": "Given a title and body of a question, provide a solution to a specific problem."
    },
    {
        "input_fields": [
            "downvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gamedev",
        "instruction": "Given a solution to a specific problem, identify the corresponding problem."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gamedev",
        "instruction": "Given a recommended resource to learn more about a topic, identify the corresponding topic."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-drupal",
        "instruction": "Given a title and body of a post, extract the relevant fields."
    },
    {
        "input_fields": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-drupal",
        "instruction": "Given a set of fields, construct a coherent sentence describing the scenario."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-mathoverflow",
        "instruction": "Given a title and body of a question, predict whether the question is answerable or not."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gis",
        "instruction": "Given a question about ArcMap and Arc Hydro, provide a recommendation for how to improve the flow accumulation grid."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gis",
        "instruction": "Given a question about writing data into an Esri *.mdb personal geodatabase using QGIS, provide an explanation of the limitations of the GDAL driver for Personal Geodatabases support."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gis",
        "instruction": "Given a question about writing data into an Esri *.mdb personal geodatabase using QGIS, provide an alternative solution for using the File Geodatabase."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-graphicdesign",
        "instruction": "Given a title and body of a question, predict whether the question will receive a high or low number of upvotes."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-graphicdesign",
        "instruction": "Given a title and body of a question, predict whether the question will receive a high or low number of downvotes."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-graphicdesign",
        "instruction": "Given a title and body of a question, suggest a solution to the problem described."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-codereview",
        "instruction": "Given a piece of code, suggest improvements to make it more efficient."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-codereview",
        "instruction": "Given a piece of code, identify potential issues or bugs."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-japanese",
        "instruction": "Given a Japanese expression, provide its English meaning."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-japanese",
        "instruction": "Given a Japanese expression, provide its slang meaning."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-japanese",
        "instruction": "Given an English expression, provide its Japanese equivalent."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-japanese",
        "instruction": "Given a Japanese expression, provide its alternative spelling."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-japanese",
        "instruction": "Given a Japanese expression, provide its usage in a sentence."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-german",
        "instruction": "Given a German expression, provide an English equivalent."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-german",
        "instruction": "Given an English expression, provide a German equivalent."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-german",
        "instruction": "Provide an explanation for the origin of a German expression."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-german",
        "instruction": "Provide an explanation for the origin of an English expression."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-german",
        "instruction": "Given a sentence, identify the expression used in it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-biology",
        "instruction": "Given a sentence, determine if it is possible for a person to become ambidextrous and provide an explanation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-biology",
        "instruction": "Given a sentence, provide a definition for a given term."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-biology",
        "instruction": "Given a sentence, determine if it is describing a predator or a parasite."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-bitcoin",
        "instruction": "Given a bitcoin-related question, provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-blender",
        "instruction": "Given a Blender problem description, provide a solution."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-blender",
        "instruction": "Given a Blender problem description, provide a solution that is not recommended."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-crypto",
        "instruction": "Given a text message, provide a recommendation for the best method to encrypt and decrypt the message using elliptic curve cryptography."
    },
    {
        "input_fields": [
            "title_body",
            "upvoted_answer"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-crypto",
        "instruction": "Given a text message and its encryption key, determine the original message using the one time pad method."
    },
    {
        "input_fields": [
            "title_body",
            "downvoted_answer"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-crypto",
        "instruction": "Given a set of possible keys and a message, determine the most likely key used to encrypt the message using the one time pad method."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-boardgames",
        "instruction": "Given a question about the rules of Uno, determine the correct answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-boardgames",
        "instruction": "Given a question about the game of Monopoly, provide the number of different editions of the game."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-hinduism",
        "instruction": "Given a sentence, predict whether it is upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-chinese",
        "instruction": "Given a Chinese phrase, provide synonyms/phrases/poems related to it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-chinese",
        "instruction": "Given a Chinese phrase, provide its English translation."
    },
    {
        "input_fields": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-chinese",
        "instruction": "Given an English phrase, provide its Chinese equivalent."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-linguistics",
        "instruction": "Given a sentence, identify the reasons why a non-native English speaker may sound foreign."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-linguistics",
        "instruction": "Given a sentence, identify the gaps in syntactical fluency that may make native speakers aware that they are dealing with a non-native speaker."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-linguistics",
        "instruction": "Given a language, determine if it is Turing complete or not."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-arduino",
        "instruction": "Given a question about Arduino, provide a solution to the problem described in the question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-arduino",
        "instruction": "Given a question about Arduino, identify the potential issues and provide a solution to the problem described in the question."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-arduino",
        "instruction": "Given a solution to an Arduino problem, provide an explanation of how it works."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-arduino",
        "instruction": "Given a solution to an Arduino problem, identify potential issues with the solution and provide an alternative solution."
    },
    {
        "input_fields": [
            "downvoted_answer"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-arduino",
        "instruction": "Given a potential issue with an Arduino solution, provide a solution to the problem."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-chess",
        "instruction": "Given a chessboard and the starting position of a knight, determine the optimal move for the next player."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-chess",
        "instruction": "Given a chessboard and the starting position of a knight, determine the number of moves it would take for the game to end."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-chess",
        "instruction": "Given a chess opening, recommend a set of defenses to play against it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-cstheory",
        "instruction": "Given a regular language in binary encoding, can you determine if it is also regular in unary encoding?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-cstheory",
        "instruction": "Given a regular language in binary encoding, can you determine if it is also regular in decimal encoding?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-cstheory",
        "instruction": "Given a regular language in binary encoding, can you determine if it is also regular in ternary encoding?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-cstheory",
        "instruction": "Given a regular language in binary encoding, can you determine if it is also regular in base-n encoding?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-ja",
        "instruction": "Given a title and body of a question, predict whether the question will be upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-ja",
        "instruction": "Given a title and body of a question, predict the most likely answer to the question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-mathematica",
        "instruction": "Given a list of functions and their corresponding variables, compute the integral formula with the highest algebraic accuracy."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-mathematica",
        "instruction": "Given a list of functions and their corresponding variables, compute the integral formula with the lowest algebraic accuracy."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-mathematica",
        "instruction": "Given a list of functions and their corresponding variables, determine the nodes and coefficients of the integral formula."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-dsp",
        "instruction": "Given a question about FIR filters, provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-dsp",
        "instruction": "Given a signal function, determine if the resulting signal is periodic and what is its period."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-dsp",
        "instruction": "Given a signal function, provide the corresponding signal function that results from adding the original function to its time-reversed version."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-cogsci",
        "instruction": "Given a title and body of a post, predict whether the post will be upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-earthscience",
        "instruction": "Given a title and body of a question, predict whether the answer will be upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-literature",
        "instruction": "Given a sentence, provide an explanation of its meaning."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-lifehacks",
        "instruction": "Given a question, provide the best upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-lifehacks",
        "instruction": "Given a question, provide the worst downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-lifehacks",
        "instruction": "Given a set of playing cards, provide a method to flatten or reduce warping."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-lifehacks",
        "instruction": "Given a set of playing cards, provide a method that may ruin the cards."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-lifehacks",
        "instruction": "Given a set of playing cards, provide a method to unbend them."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-engineering",
        "instruction": "Given a sentence, identify the upvoted answer and downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-ham",
        "instruction": "Given a sentence, predict whether it is an upvoted answer or a downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-3dprinting",
        "instruction": "Given a 3D printing problem, suggest possible solutions to the problem."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-3dprinting",
        "instruction": "Given a 3D printing problem, identify the possible causes of the problem."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-3dprinting",
        "instruction": "Given a 3D printing solution, suggest the possible filament and temperature settings."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-italian",
        "instruction": "Given a sentence in Italian, translate it to English."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-italian",
        "instruction": "Given a sentence in Italian, provide an explanation of a specific grammatical rule or usage."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-italian",
        "instruction": "Given an English sentence, translate it to Italian."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-emacs",
        "instruction": "Given a question or problem in the title_body field, suggest a solution or answer based on the upvoted_answer field."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-emacs",
        "instruction": "Given a question or problem in the title_body field, suggest an alternative solution or answer based on the downvoted_answer field."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-avp",
        "instruction": "Given a sentence, provide a solution to a specific problem in post-processing."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-expatriates",
        "instruction": "Given a sentence, determine whether it is legal for a person with a certain visa to work for a foreign company while living in a certain country."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-expatriates",
        "instruction": "Given a sentence, provide a possible solution for a person with a certain visa to work for a foreign company while living in a certain country."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-elementaryos",
        "instruction": "Given a problem description, provide a solution to the problem."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-elementaryos",
        "instruction": "Given a problem description, provide a reason why the solution did not work."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-elementaryos",
        "instruction": "Given a solution, provide the problem description."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-elementaryos",
        "instruction": "Given a solution, provide a reason why the solution did not work."
    },
    {
        "input_fields": [
            "downvoted_answer"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-elementaryos",
        "instruction": "Given a reason why a solution did not work, provide a solution."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-elementaryos",
        "instruction": "Given a problem description, suggest a possible solution."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-joomla",
        "instruction": "Given a question about Joomla, provide the answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-joomla",
        "instruction": "Given a question about Joomla, provide the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-freelancing",
        "instruction": "Given a title and body of a post, recommend a tool for tracking clients, projects, files, etc."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-freelancing",
        "instruction": "Given a title and body of a post, suggest a solution for dealing with a client who refuses to pay after receiving the finished product."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-freelancing",
        "instruction": "Given a title and body of a post, recommend an open source software for project management."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-freelancing",
        "instruction": "Given a title and body of a post, suggest a way to handle a situation where a client refuses to pay after receiving the finished product."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-crafts",
        "instruction": "Given a question about crafting, provide a recipe or method to solve the problem."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-crafts",
        "instruction": "Given a question about crafting, provide a list of manufactured boards that can be used."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-crafts",
        "instruction": "Given a crafting problem, provide a list of materials that can be used to solve the problem."
    },
    {
        "input_fields": [
            "downvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-crafts",
        "instruction": "Given a crafting problem, provide a list of materials that should not be used to solve the problem."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-genealogy",
        "instruction": "Given a name that is difficult to read, provide the correct spelling of the name."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-genealogy",
        "instruction": "Given a medical history form, extract the important information from it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-latin",
        "instruction": "Given a sentence in Latin, translate it into English."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-latin",
        "instruction": "Given an English sentence, translate it into Latin."
    },
    {
        "input_fields": [
            "downvoted_answer"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-latin",
        "instruction": "Identify the correct translation of a given Latin word or phrase."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-devops",
        "instruction": "Given a Dockerfile, suggest ways to reduce the size of the created image."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-devops",
        "instruction": "Given a Dockerfile, identify the commands that can be optimized to reduce the size of the created image."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-devops",
        "instruction": "Given a Dockerfile, suggest ways to improve the performance of the created image."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-coffee",
        "instruction": "Given a coffee recipe, identify the region or culture it originated from."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-coffee",
        "instruction": "Given a coffee recipe, extract the key ingredients and preparation steps."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-coffee",
        "instruction": "Given a question about coffee, provide an explanation of the biochemistry behind it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-coffee",
        "instruction": "Given a question about coffee, provide a recommendation or advice."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-beer",
        "instruction": "Given a beer preference, recommend a beer that fits the taste buds."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-beer",
        "instruction": "Given a beer preference, recommend a beer that fits the taste buds and is light in color."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-beer",
        "instruction": "Given a beer type, explain the taste and brewing process."
    },
    {
        "input_fields": [
            "downvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-beer",
        "instruction": "Given a beer type, explain the alcohol content and addiction."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-ebooks",
        "instruction": "Given a title and body of a question, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-ebooks",
        "instruction": "Given a title and body of a question, provide the corresponding downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-bricks",
        "instruction": "Given a sentence, identify the most plausible explanation for a physical phenomenon described in the sentence."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-bricks",
        "instruction": "Given a sentence, identify the least plausible explanation for a physical phenomenon described in the sentence."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-civicrm",
        "instruction": "Given a title and body of a question, predict the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-civicrm",
        "instruction": "Given a title and body of a question, predict the corresponding downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-civicrm",
        "instruction": "Given a title and body of a question, predict the corresponding answer with the highest score."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-civicrm",
        "instruction": "Given a title and body of a question, predict the corresponding answer with the lowest score."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-bioinformatics",
        "instruction": "Given a title and body of a question, provide the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-bioinformatics",
        "instruction": "Given a title and body of a question, provide the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-esperanto",
        "instruction": "Given a sentence in Esperanto, provide its English translation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-esperanto",
        "instruction": "Given a sentence in Esperanto, provide an Esperanto word that is an auto-antonym."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-esperanto",
        "instruction": "Given an Esperanto word, provide an example sentence using the word."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-esperanto",
        "instruction": "Given a sentence in Esperanto, provide its translation in a language other than English."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-computergraphics",
        "instruction": "Given a sentence, suggest a solution to a problem related to computer graphics."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-computergraphics",
        "instruction": "Given a sentence, suggest a different solution to a problem related to computer graphics."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-computergraphics",
        "instruction": "Given a sentence, suggest a way to optimize the performance of a computer graphics application."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-computergraphics",
        "instruction": "Given a sentence, suggest a way to implement a specific feature in a computer graphics application."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-korean",
        "instruction": "Given a sentence in Korean, provide a paraphrased version of the sentence that sounds more natural."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-cardano",
        "instruction": "Given a question about minting NFTs, provide the answer with the highest number of upvotes."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-cardano",
        "instruction": "Given a question about minting NFTs, provide the answer with the lowest number of upvotes."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-softwareengineering",
        "instruction": "Given a sentence, identify the upvoted answer and downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-serverfault",
        "instruction": "Given a post, provide the best solution to the problem discussed in the post."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-rpg",
        "instruction": "Given a question about multiclassing, provide the correct answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-rpg",
        "instruction": "Given a question about multiclassing, provide the incorrect answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-rpg",
        "instruction": "Given a magic item, determine if it can be used to meet the ability score prerequisites for multiclassing."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-superuser",
        "instruction": "Given a title and body of a question, predict whether the question will receive a high or low number of upvotes."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-superuser",
        "instruction": "Given a title and body of a question, predict whether the question will receive a high or low number of downvotes."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-superuser",
        "instruction": "Given a title and body of a question, suggest a solution to the problem described in the question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-security",
        "instruction": "Given a title and body of a question, predict the most upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-security",
        "instruction": "Given a title and body of a question, predict the most downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-pt",
        "instruction": "Given a code snippet, the task is to identify the error in the code and provide a solution."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-pt",
        "instruction": "Given a percentage value, the task is to calculate the percentage of a given value."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-unix",
        "instruction": "Given a sentence, predict whether it is an upvoted answer or a downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-meta",
        "instruction": "Given a title and body of a post, predict whether the post will be upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-meta",
        "instruction": "Given a post, extract the reasons why a joke is inappropriate for a professional environment."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-meta",
        "instruction": "Given a post, extract the reasons why a question should be deleted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-politics",
        "instruction": "Given a sentence, identify the political philosophy it belongs to."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-politics",
        "instruction": "Given a sentence, identify the political philosophy it does not belong to."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-movies",
        "instruction": "Given a sentence, provide an explanation or answer to a specific question related to it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-movies",
        "instruction": "Given a sentence, provide an alternative answer or explanation to a specific question related to it."
    },
    {
        "input_fields": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-movies",
        "instruction": "Given an answer, identify the sentence it is related to."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-photo",
        "instruction": "Given a question about photo print processing, choose the correct answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-photo",
        "instruction": "Given a photo print processing question, provide an answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-photo",
        "instruction": "Given a photo print processing question, provide a counterargument."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-wordpress",
        "instruction": "Given a title and body of a post, predict whether the post will be upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-music",
        "instruction": "Given a guitar tuning, determine the reason behind it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-music",
        "instruction": "Given a guitar tuning, provide an example of a song that uses it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-music",
        "instruction": "Given a guitar tuning, explain how it affects the sound of the guitar."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-music",
        "instruction": "Given a guitar tuning, explain how it affects the ease of playing the guitar."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-music",
        "instruction": "Given a guitar tuning, explain how it affects the range of notes that can be played on the guitar."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-philosophy",
        "instruction": "Given a sentence, predict whether it is upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-money",
        "instruction": "Given a question about compound interest, provide the formula for calculating the future value of a series of monthly deposits with interest compounded weekly."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-money",
        "instruction": "Given a question about Nigerian email scammers, explain why they openly say they are from Nigeria."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-money",
        "instruction": "Given a question about compound interest, provide a formula for calculating the future value of a series of monthly deposits with interest compounded weekly, assuming the number of compounding periods per year is less than monthly."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-money",
        "instruction": "Given a question about Nigerian email scammers, explain why they might not disclose the country they work from."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-salesforce",
        "instruction": "Given a question about Salesforce password reset, provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-salesforce",
        "instruction": "Given a Salesforce formula field, determine if it can be indexed."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-salesforce",
        "instruction": "Given a Salesforce formula field, provide an alternative method to obtain the ContactID(18) from the Account."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-parenting",
        "instruction": "Given a title and body of a post, predict whether the post will be upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-travel",
        "instruction": "Given a sentence, predict whether it is an upvoted answer or a downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-tex",
        "instruction": "Given a sentence, identify the most appropriate LaTeX command to format the text."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-tex",
        "instruction": "Given a sentence with LaTeX commands, identify the original text."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-tex",
        "instruction": "Given a sentence, suggest a LaTeX command to format the text."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-ux",
        "instruction": "Given a title and body of a post, predict whether the post will be upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-sharepoint",
        "instruction": "Given a title and body of a question, provide a relevant upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-sharepoint",
        "instruction": "Given a title and body of a question, provide a relevant downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-webapps",
        "instruction": "Given a question about Google Calendar, provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-webapps",
        "instruction": "Given a Gmail search query, find the corresponding emails."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-puzzling",
        "instruction": "Given a set of phrases, the task is to find a common keyword that connects all these phrases."
    },
    {
        "input_fields": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-puzzling",
        "instruction": "Given a keyword, the task is to provide a set of phrases that are connected to the keyword."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-rus",
        "instruction": "Given a sentence, predict whether it is an upvoted answer or a downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-space",
        "instruction": "Given a title and body of a question, predict the most likely upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-space",
        "instruction": "Given a title and body of a question, predict the most likely downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-writers",
        "instruction": "Given a sentence, identify whether it is appropriate to use real historical figures as characters in a fictional story."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-writers",
        "instruction": "Given a sentence, provide examples of other works of fiction that have used real historical figures as characters."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-writers",
        "instruction": "Given a sentence, provide advice on how to become a good writer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-writers",
        "instruction": "Given a sentence, recommend books to read to improve writing skills."
    },
    {
        "input_fields": [
            "title_body",
            "upvoted_answer"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-pm",
        "instruction": "Given a question and its upvoted answer, can you identify the downvoted answer?"
    },
    {
        "input_fields": [
            "title_body",
            "downvoted_answer"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-pm",
        "instruction": "Given a question and its downvoted answer, can you identify the upvoted answer?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-spanish",
        "instruction": "Given a sentence in Spanish, provide the English translation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-spanish",
        "instruction": "Given a sentence in Spanish, provide the correct usage of the verb \"gustar\"."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-spanish",
        "instruction": "Given a sentence in Spanish, provide the appropriate verb to use for \"to obtain\" or \"to achieve\"."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-spanish",
        "instruction": "Given a sentence in Spanish, provide the appropriate verb to use for \"to manage to do something\"."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-sound",
        "instruction": "Given a sound recording, determine if it is real or computer generated."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-sound",
        "instruction": "Given a sound recording, provide a reliable method to determine if it is real or computer generated."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-sound",
        "instruction": "Given a sound recording, provide a method to determine the roundtrip latency of an audio interface."
    },
    {
        "input_fields": [
            "title_body",
            "upvoted_answer"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-sound",
        "instruction": "Given a sound recording and latency test results, explain why the results are inconsistent."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-quant",
        "instruction": "Given a title and body of a question, provide an answer that explains the difference between VXX ETF and VIX futures prices."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-quant",
        "instruction": "Given a title and body of a question, provide an answer that explains the concept of \"Contango\"."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-softwarerecs",
        "instruction": "Given a title and body of a post, predict whether the post will be upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-retrocomputing",
        "instruction": "Given a title and body of a post, predict whether the post will be upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-portuguese",
        "instruction": "Given a sentence, identify the differences between the words \"enxergar\" and \"ver\"."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-portuguese",
        "instruction": "Given a sentence, provide the correct usage of \"porque\" and \"por que\"."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-portuguese",
        "instruction": "Given a sentence, provide an example where \"ver\" and \"enxergar\" cannot be interchanged."
    },
    {
        "input_fields": [
            "downvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-portuguese",
        "instruction": "Given a sentence, provide an example where \"porque\" and \"por que\" cannot be interchanged."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-robotics",
        "instruction": "Given a sentence containing a quaternion, transform it into Euler angles."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-robotics",
        "instruction": "Given a sentence describing a robot project, suggest a sensor that could be used to localize the robot on a mirror."
    },
    {
        "input_fields": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-robotics",
        "instruction": "Given a set of Euler angles, transform them into a quaternion."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-sitecore",
        "instruction": "Given a title and body of a question, predict whether the answer will be upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-tor",
        "instruction": "Given a sentence, identify the reason why the user is unable to access the darknet."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-tor",
        "instruction": "Given a sentence, identify the solution to the user's problem."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-tor",
        "instruction": "Given a sentence, identify the type of error the user is facing."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-vi",
        "instruction": "Given a text, extract the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-vi",
        "instruction": "Given a text, extract the downvoted answer."
    },
    {
        "input_fields": [
            "title_body",
            "upvoted_answer"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-quantumcomputing",
        "instruction": "Given a question and its upvoted answer, generate a downvoted answer."
    },
    {
        "input_fields": [
            "title_body",
            "downvoted_answer"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-quantumcomputing",
        "instruction": "Given a question and its downvoted answer, generate an upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-musicfans",
        "instruction": "Given a piece of classical music, identify the name of the song and composer using an app."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-musicfans",
        "instruction": "Provide an explanation for why it is difficult to categorize music objectively based on genres."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-musicfans",
        "instruction": "Given a set of objective characteristics and patterns of a song, categorize the song into a genre."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-opendata",
        "instruction": "Given a question about cryptocurrency data, provide a source for free tick-by-tick data with a second-based timeframe."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-opendata",
        "instruction": "Given a question about commercial flight routes, provide a website that provides flight-related information through an API."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-opendata",
        "instruction": "Given a question about cryptocurrency data, provide a way to get historical dataset using exchange API."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-stellar",
        "instruction": "Given a Stellar network and a client, generate a transaction sequence number for a new account using the Go SDK."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-stellar",
        "instruction": "What is the solution to the problem of using a memo instead of an address in a QR code for a Stellar exchange?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-stellar",
        "instruction": "What is the difference between Horizon 0.15.2 and 0.16 in terms of transaction auto sequence?"
    },
    {
        "input_fields": [
            "downvoted_answer"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-stellar",
        "instruction": "What is the SEP-0007 protocol and how can it be used to solve the problem of using a memo instead of an address in a QR code?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-monero",
        "instruction": "Given a title and body of a post, predict whether the post is likely to be upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-stackapps",
        "instruction": "Given a question, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-stackapps",
        "instruction": "Given a question, provide the corresponding downvoted answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-stackapps",
        "instruction": "Given an answer, provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-apple",
        "instruction": "Given a sentence, suggest an alternative script to achieve the same goal."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-english",
        "instruction": "Given a sentence, can you suggest a better way to write it?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-english",
        "instruction": "Given a sentence, can you identify the grammatical error?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-english",
        "instruction": "Given a sentence, can you suggest a better way to phrase a question?"
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-codereview",
        "instruction": "Given a code snippet, identify if there is a bug in the code."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-codereview",
        "instruction": "Given a code snippet, suggest ways to optimize the code."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-codereview",
        "instruction": "Given a code snippet, identify the purpose of the code."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-dba",
        "instruction": "Given a database backup schedule and the number of transactions per day, calculate the frequency of backups required to minimize data loss in case of a crash."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-dba",
        "instruction": "Given a set of credentials for a Microsoft SQL Server cluster, determine the virtual IP address that should be used to connect to the active node."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-dba",
        "instruction": "Given a database backup schedule and the number of transactions per day, recommend a backup strategy that minimizes performance impact."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-mathoverflow",
        "instruction": "Given a math problem, can you provide the solution?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-electronics",
        "instruction": "Given a voltage waveform and an inductor, plot the current waveform."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-electronics",
        "instruction": "Given a circuit diagram and a current waveform, calculate the voltage waveform."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-electronics",
        "instruction": "Given a list of electronic components, determine whether they should be classified as analog or mixed-signal."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-electronics",
        "instruction": "Given a description of a search for electronic components, provide tips for more effective searching."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-mathematica",
        "instruction": "Given a set of commands, can you draw a hollow circle using the polygon command?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-mathematica",
        "instruction": "What is the difference between the output of Erfc[-30. + 10^-1 I] in Mathematica 7.0.1 and 8.0?"
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-mathematica",
        "instruction": "Can you provide an example of a command that returns a number that is extremely close to 2 using Erfc?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-drupal",
        "instruction": "Given a Drupal 7 site, write a function to add javascript to a specific set of pages based on the url."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-drupal",
        "instruction": "Given a Drupal 7 site, write a function to automatically send email notifications to subscribers when new content is published."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-drupal",
        "instruction": "Given a Drupal 7 site, suggest a module to use for sending email notifications to subscribers when new content is published."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-magento",
        "instruction": "Given a Magento 2 store, where should the user paste the tracking code from Google?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-magento",
        "instruction": "Given a Magento 2 store, how to add a custom error message when payment fails?"
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-magento",
        "instruction": "Given a Magento 2 store, what is the recommended way to handle exceptions?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-gaming",
        "instruction": "Given a question about Steam Greenlight, provide the answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-gaming",
        "instruction": "What is the maximum revive time in the game?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-gaming",
        "instruction": "What is the formula for calculating revive time?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-gaming",
        "instruction": "What is the starting revive time in the game?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ell",
        "instruction": "Given a sentence, determine whether to use \"a\", \"some\", or \"the\" before a proper noun."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ell",
        "instruction": "Given a sentence, identify the correct preposition to use in a phrase."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ell",
        "instruction": "Given a sentence, identify the idiomatic expression used."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-gamedev",
        "instruction": "Given a Direct3D vertex layout, what are the attributes extracted from the buffer?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-gamedev",
        "instruction": "How to mute game sounds and game music separately in Unity?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-gamedev",
        "instruction": "What are the differences between the new audio system that shipped with Unity 5 and the old one?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-gis",
        "instruction": "Given a question about opening large DEM files, predict the amount of RAM required to process the file."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-gis",
        "instruction": "Given a question about profiling rendering performance in QGIS, predict the best practices for simplification and storage of data to ensure that it can be viewed and interacted with in the most responsive manner."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-gis",
        "instruction": "Given a question about rendering performance in QGIS, predict the indicators about rendering performances that Snail can provide."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-askubuntu",
        "instruction": "Identify the issue with the internet connection based on the description provided."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-askubuntu",
        "instruction": "Provide a solution to fix the internet connection issue."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-askubuntu",
        "instruction": "Identify the default sound input for Skype and make it persistent."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-askubuntu",
        "instruction": "Investigate possible resolutions for a bug in Ubuntu's integration of PulseAudio and alsa-utils."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-diy",
        "instruction": "Given a sentence, identify the reason for cold air coming out of electrical outlets when bathroom exhaust fan is running."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-diy",
        "instruction": "Given a sentence, suggest a solution to stop a fan from squeaking."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-diy",
        "instruction": "Given a sentence, identify the type of fluid that can be used to lubricate a fan."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-judaism",
        "instruction": "Given a question about Jewish law, provide the corresponding halachic ruling."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-judaism",
        "instruction": "Given a halachic ruling, provide the corresponding question about Jewish law."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-crypto",
        "instruction": "Given a use case and a hashing function, suggest a secure way to create different \"passwords\" from a user-specific secret."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-crypto",
        "instruction": "Given a key exchange protocol, suggest its applications and use cases."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-android",
        "instruction": "Given a sentence, identify the error message and provide a solution."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-christianity",
        "instruction": "Given a question about Christianity, provide a concise answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-graphicdesign",
        "instruction": "Given a logo design and a vehicle wrap, determine whether the direction of travel should be reversed to match the movement of the vehicle."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-graphicdesign",
        "instruction": "Given a logo design for an app, provide critiques and advice for improvement."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-graphicdesign",
        "instruction": "Given a logo design, suggest a local context-specific element to incorporate into the design."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-aviation",
        "instruction": "Identify the type of plane based on a given image or model."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-aviation",
        "instruction": "Provide the minimum vertical and horizontal distance allowed between aircraft operating in European airspace."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-aviation",
        "instruction": "What are the special rules that apply to mitigate the risk of wake turbulence?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ethereum",
        "instruction": "Given a code snippet and an error message, identify the cause of the error."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ethereum",
        "instruction": "Given a code snippet, provide a corrected version of the code."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ethereum",
        "instruction": "Given a code snippet, identify the function being called and its arguments."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ethereum",
        "instruction": "Given a code snippet, identify the programming language being used."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ethereum",
        "instruction": "Given a code snippet, identify the purpose of the code."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-biology",
        "instruction": "Identify the species of the beetle based on the description and image provided."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-biology",
        "instruction": "Provide a list of research institutes or NGOs that collect and make available world-wide ecology data."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-biology",
        "instruction": "Given a set of ecology parameters, provide the corresponding data sources."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-datascience",
        "instruction": "Given a code snippet, identify the error message and provide a solution."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-datascience",
        "instruction": "Given a code snippet, identify the input and output arrays and convert them into a dictionary of arrays."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-datascience",
        "instruction": "Given a code snippet, identify the system configuration and suggest possible solutions to improve performance."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-law",
        "instruction": "Given a legal scenario, identify the potential legal obligations of a person involved."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-law",
        "instruction": "Given a legal scenario, identify the potential legal consequences of a person involved."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-law",
        "instruction": "Given a legal scenario, identify the potential legal defenses of a person involved."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-dsp",
        "instruction": "Given a signal x(t), compute its Fourier Transform X(f)."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-dsp",
        "instruction": "Given a signal x(t), compute its inverse Fourier Transform x(-t)."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-dsp",
        "instruction": "Given a signal x(t), compute its complex conjugate x*(t)."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-dsp",
        "instruction": "Given a signal x(t), determine if it satisfies the condition x(-t) = x*(t)."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-dsp",
        "instruction": "Given a signal x(t), determine if its Fourier Transform X(f) is real-valued."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-japanese",
        "instruction": "Given a sentence containing the word \"\u9054\u78e8\", provide a metaphorical meaning for the word in the given context."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-japanese",
        "instruction": "Given a verb in English, provide the corresponding verb and noun+suru verb in Japanese, and explain the difference in usage between the two."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-japanese",
        "instruction": "Given a sentence in Japanese, provide the corresponding English translation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-hermeneutics",
        "instruction": "Given a sentence, generate a coherent summary of the main idea."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-arduino",
        "instruction": "Given a code snippet, identify the error in the code and suggest a correction."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-arduino",
        "instruction": "Given a code snippet, suggest a modification to achieve a specific goal."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-arduino",
        "instruction": "Given a code snippet, identify the purpose of the code and suggest a use case."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-cooking",
        "instruction": "Given a recipe for a food item, suggest a binding agent that can be used to make it gluten-free."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-cooking",
        "instruction": "Given a description of a coffee, explain how it should be taken and what the accompanying items are."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-hinduism",
        "instruction": "Given a sentence, identify the technical definition of a word."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-hinduism",
        "instruction": "Given a sentence, extract the names of the Seven Rishis."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-hinduism",
        "instruction": "Given a sentence, identify the type of weapon used in Mahabharata and Ramayana."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-codegolf",
        "instruction": "Given the width of a quarry and the speed of an excavator, can you generate a progress report of the excavation process?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-boardgames",
        "instruction": "Given a description of a card game, identify the name of the game and its scoring system."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-boardgames",
        "instruction": "Given a card game name, provide a description of the game and its scoring system."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-boardgames",
        "instruction": "Given a card name, identify its type and subtype."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-boardgames",
        "instruction": "Given a card type and subtype, provide a list of cards that match the criteria."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-emacs",
        "instruction": "Given a code snippet, help to debug it by suggesting a better way to debug."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-emacs",
        "instruction": "Given a code snippet, suggest a modification to achieve a specific functionality."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-emacs",
        "instruction": "Given a code snippet, suggest a modification to improve its performance."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-gardening",
        "instruction": "Given a gardening question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-astronomy",
        "instruction": "Given a gravitational force, calculate the resulting acceleration vector."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-astronomy",
        "instruction": "Given a picture of the sky, explain why the Milky Way appears to be arcing across the sky."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-astronomy",
        "instruction": "Given a celestial body and its orbit, calculate the gravitational force between them."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-islam",
        "instruction": "Given a question about Islam, provide the correct answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-islam",
        "instruction": "Given a hadith, determine its authenticity."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-german",
        "instruction": "Given a sentence in German, provide an English translation."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-german",
        "instruction": "Given an English sentence, provide a German translation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-german",
        "instruction": "Identify the difference between two similar phrases in German."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-german",
        "instruction": "Identify the difference between two similar phrases in English."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-fitness",
        "instruction": "Given a fitness question, provide the best answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-french",
        "instruction": "Given a sentence in French, provide an idiomatic English translation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-anime",
        "instruction": "Given a sentence, identify the word that is mispronounced and provide the correct pronunciation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-anime",
        "instruction": "Given a sentence, identify the purpose of ecchi scenes in anime series."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-anime",
        "instruction": "Given a sentence, identify the demographic that is likely to enjoy anime series with ecchi scenes."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-craftcms",
        "instruction": "Given a Craft entry with a category field, output the nested category as a breadcrumb trail."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-craftcms",
        "instruction": "Given a Craft entry with multiple relationship fields, output the title of the state to which the town belongs."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-craftcms",
        "instruction": "Given a Craft entry with multiple relationship fields, output the title of the county to which the town belongs."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ai",
        "instruction": "Given a set of stock data, identify the macro-econometric variables that may influence the stock prices."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ai",
        "instruction": "Given a set of stock data, identify the relevant predictor variables that have some relationship with the response variable."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ai",
        "instruction": "Find an open-source implementation for graph convolution networks for weighted graphs."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-civicrm",
        "instruction": "Given a title and body of a post, predict the number of upvotes it will receive."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-earthscience",
        "instruction": "Given a question about subduction zones, predict the likelihood of a megathrust earthquake occurring in a specific area of the subduction zone."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-earthscience",
        "instruction": "Given a description of a cloud phenomenon, identify the name of the phenomenon."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-earthscience",
        "instruction": "Given a description of a cloud phenomenon, predict the type of orographic cloud that will result."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-joomla",
        "instruction": "Given a Joomla error message, can you identify the cause of the error?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-joomla",
        "instruction": "Given a SQL query, can you identify the error message and provide a solution?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-joomla",
        "instruction": "Given a SQL query, can you extract the latest timestamp and display it in a specific format?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-homebrew",
        "instruction": "Given a Kombucha recipe, predict the thickness of the Kombucha based on the size of the SCOBY and the duration of fermentation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-homebrew",
        "instruction": "Given a description of a brewing method, predict the likelihood of contamination based on the sanitation practices and the type of fermentation vessel used."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-homebrew",
        "instruction": "Given a description of a brewing method, predict the effectiveness of a HEPA filter in reducing contamination."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-expatriates",
        "instruction": "Given a question about unemployment benefits in a European state, provide the reporting requirements for collecting unemployment benefits in that state."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-expatriates",
        "instruction": "Given a European state and its reporting requirements for collecting unemployment benefits, provide the name of the state."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-expatriates",
        "instruction": "Given a question about safety in South Africa, provide advice on the necessary safety measures to take when living in a rented place in Cape Town."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-expatriates",
        "instruction": "Given advice on the necessary safety measures to take when living in a rented place in Cape Town, provide the question about safety in South Africa."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-latin",
        "instruction": "Given a Latin phrase, can you provide the English translation?"
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-latin",
        "instruction": "Given an English phrase, can you provide the Latin translation?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-latin",
        "instruction": "Given a Latin phrase, can you provide the context or explanation?"
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-latin",
        "instruction": "Given an English phrase, can you provide the context or explanation?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-matheducators",
        "instruction": "Given a question about math courses, recommend which courses to take for someone who wants to become a secondary math educator."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-matheducators",
        "instruction": "Given a question about the Monty Hall problem, explain the solution in a clear and concise way."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-matheducators",
        "instruction": "Given a list of math courses, recommend which courses to take for someone who wants to become a high school math teacher."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-matheducators",
        "instruction": "Given a question about explaining the Monty Hall problem, provide an alternative explanation to the \"extend it to 100 doors and eliminate 98\" method."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ham",
        "instruction": "Given a question about QSSTV/Hamlib and RTL-SDR/HackRF One, provide a solution to the problem."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ham",
        "instruction": "Given a question about Ham radio safety, provide an answer to the question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-bioinformatics",
        "instruction": "Given a command line error message, suggest a solution to fix the error."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-devops",
        "instruction": "Given a command line option, explain its meaning."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-computergraphics",
        "instruction": "Given a question about OpenGL ES, provide the corresponding answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-martialarts",
        "instruction": "Given a martial arts technique, provide a rationale for its classification."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-freelancing",
        "instruction": "Given a sentence, predict whether a licensing agreement is necessary or not."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-lifehacks",
        "instruction": "Given a sentence, suggest a way to remove a specific smell from clothing or helmet."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-lifehacks",
        "instruction": "Given a sentence, suggest a way to remove metal residue from dishes."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-lifehacks",
        "instruction": "Given a cleaning solution, suggest other uses for it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-cseducators",
        "instruction": "Given a sentence, identify the floating point inaccuracies and explain why they occur."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-cseducators",
        "instruction": "Given a technical document, suggest a project-based approach to learning the tool."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-cseducators",
        "instruction": "Given a sentence, identify the key concepts and summarize the main idea."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-materials",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-hardwarerecs",
        "instruction": "Given a question about hardware, provide a short answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-iot",
        "instruction": "Given a sentence, extract the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-eosio",
        "instruction": "Given a command line error message, provide a solution to fix the error."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-eosio",
        "instruction": "Given a version of nodeos, find a compatible snapshot repository."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-languagelearning",
        "instruction": "Given a sentence in Thai, provide the English translation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-languagelearning",
        "instruction": "Given a German word or phrase, provide the English translation."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-languagelearning",
        "instruction": "Given a sentence in English, provide the German translation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-coffee",
        "instruction": "Given a coffee brewing scenario, what are the factors that influence crema production?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-coffee",
        "instruction": "What is the effect of distilled water or water filtered by reverse osmosis on the taste of coffee?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-coffee",
        "instruction": "What are the consequences of carbon dioxide content in coffee on foam volume, persistence, and consistence?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ebooks",
        "instruction": "Identify the tool(s) for validating an epub file."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ebooks",
        "instruction": "Suggest alternative readers for an epub file with audio that is not working on iBooks."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ebooks",
        "instruction": "Provide a list of resources for media overlay epub3 books."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-drones",
        "instruction": "Given a question about drone regulations in Australia, provide the corresponding answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-conlang",
        "instruction": "Given a conlang, identify the type of possession used for each of the four categories: spatial relationships, physical parts, kinship bonds, and objects essential for survival."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-conlang",
        "instruction": "Given a conlang, identify the term for the scenario where two conlangs have the same syntactical structure and can be translated by simply exchanging words while maintaining the same (or very similar) structure."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-stats",
        "instruction": "Given a probability distribution, write the log-likelihood function for a random sample and find the maximum likelihood estimator for a parameter."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-stats",
        "instruction": "What are the different interpretations of probability?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-stats",
        "instruction": "What are the different interpretations of statistics?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-wordpress",
        "instruction": "Given a title and body of a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-scifi",
        "instruction": "Given a sentence, determine if it passes the Bechdel test."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-scifi",
        "instruction": "Given a sentence, extract the reason why a character is concerned about hiding their powers."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ru",
        "instruction": "Given a Python code snippet, identify the error and provide a corrected version of the code."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ru",
        "instruction": "Given a JavaScript interface, provide a library or plugin that can be used to implement the interface."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ru",
        "instruction": "Given a JavaScript interface, provide a code snippet that can be used to implement the interface."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-sharepoint",
        "instruction": "Given a SharePoint list title, provide the corresponding REST API endpoint."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-sharepoint",
        "instruction": "Given a SharePoint REST API endpoint, provide the corresponding list title."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-sharepoint",
        "instruction": "Given a SharePoint list title, provide alternative ways to move the list between two sites."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-rpg",
        "instruction": "Given a Shadowrun game scenario, provide suggestions for how to keep the game simple and avoid overwhelming the players."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-rpg",
        "instruction": "Given a Shadowrun game scenario, provide suggestions for how to make the game more challenging and interesting."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-rpg",
        "instruction": "Given a specific spell and scenario in Dungeons and Dragons, determine whether the spell can be used and what the outcome would be."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-meta",
        "instruction": "Given a question, determine whether it is acceptable to provide links to online resources that contain a solution. If so, what is the netiquette regarding links to e.g. online resources behind paywalls, articles in non-english language, books out of print, etc.?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-workplace",
        "instruction": "Given a sentence, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ux",
        "instruction": "Given a sentence, identify the assumptions made by the author and suggest ways to validate them."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ux",
        "instruction": "Given a sentence, suggest alternative interfaces to the high-power LED/LCD/OLED screen."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ux",
        "instruction": "Given a sentence, suggest ways to reduce battery consumption for devices that run on a battery."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-webmasters",
        "instruction": "Given a title and body of a post, generate a summary of the post."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-raspberrypi",
        "instruction": "Given a question about QEMU and Raspberry Pi, provide the answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-raspberrypi",
        "instruction": "Given a Raspberry Pi question, provide a command to backup the SD card."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-raspberrypi",
        "instruction": "Given a Raspberry Pi question, provide a list of processes to shut down before making a backup using dd."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-music",
        "instruction": "Given a musical key and two options for modulation, choose the correct direction to modulate to achieve a desired effect."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-music",
        "instruction": "Given a musical score and a recommended fingering, determine whether the fingering is optimal and suggest an alternative if necessary."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-music",
        "instruction": "Provide a brief explanation of Harmonic Brightness & Darkness and its relationship to the Circle of Fifths."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-puzzling",
        "instruction": "Given a graph with colored nodes representing countries and their currencies, identify the currency of a specific country."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-puzzling",
        "instruction": "Given a set of historical clues, identify the historical event or period being referenced."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-puzzling",
        "instruction": "Given a sentence, extract all the countries mentioned in it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-movies",
        "instruction": "Given a movie title and plot summary, predict the reason for a character's absence from the movie/show."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-quant",
        "instruction": "Given a question about Monte Carlo methods, provide a formula for estimating convergence."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-quant",
        "instruction": "Given a set of SDE coefficients and a payoff function, calculate the rate of convergence for Monte Carlo methods."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-quant",
        "instruction": "Given a set of currencies and their corresponding OIS rates, calculate the CTD discount curve."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-quant",
        "instruction": "Given a set of XYZUSD forward basis curves, calculate the CTD curve discount factors."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-politics",
        "instruction": "Given a sentence, predict whether it is a legal statement or not."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-politics",
        "instruction": "Given a sentence, predict whether it is a political ad or not."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-politics",
        "instruction": "Given a sentence, predict whether it is a false and misleading information or not."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-space",
        "instruction": "Given a sentence, can you provide a brief summary of the main idea?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-mechanics",
        "instruction": "Given a sentence, identify the problem and provide a solution."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-writers",
        "instruction": "Given a sentence, identify the legal implications of self-publishing an anthology of unpublished short stories."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-writers",
        "instruction": "Given a sentence, describe the physical sensation of pain in a realistic manner."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-writers",
        "instruction": "Given a sentence, identify the source of the pain and the character experiencing it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-webapps",
        "instruction": "Given a question about Google Analytics, provide a solution using the Google Analytics Reporting API and Google Apps Scripts."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-webapps",
        "instruction": "Given a URL, determine if Pastebin.com can turn it into a clickable link."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-webapps",
        "instruction": "Provide a list of similar services to Pastebin.com that support clickable URLs."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-softwarerecs",
        "instruction": "Given a shopping list and a list of stores, the task is to optimize the shopping list by finding the stores where each item can be found."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-softwarerecs",
        "instruction": "Given a list of stores and the items available at each store, the task is to generate a list of items available at each store."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-softwarerecs",
        "instruction": "Given a set of requirements for a multi-vendor marketplace, the task is to recommend a suitable framework or script."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-softwarerecs",
        "instruction": "Given a set of requirements for a multi-vendor marketplace, the task is to recommend a suitable payment gateway."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-pm",
        "instruction": "Given a sentence, generate a summary of the main points."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-outdoors",
        "instruction": "Given a sentence about bobcats breeding with house cats, predict whether there is scientific evidence of it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-outdoors",
        "instruction": "Given a sentence about avoiding leeches, predict the best method to prevent them."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-tridion",
        "instruction": "Given a Tridion component, extract the details of the user who last modified it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-tridion",
        "instruction": "Given a Tridion component, extract the date it was last modified."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-tridion",
        "instruction": "Given a Tridion component, extract the type of item it is (e.g. Component, Page, Publication, etc.)."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-tridion",
        "instruction": "Given a Tridion component, extract the author of the item."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-tridion",
        "instruction": "Given a Tridion component, extract the error message and solution."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-sports",
        "instruction": "Given a cricket scenario, determine who would be run out if the non-striker doesn't run but the batsman on strike does."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-sports",
        "instruction": "Given a football scenario, determine if grabbing and holding the shoulder pads under someone\u2019s chin is considered Defensive Holding or not."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-russian",
        "instruction": "Given a word in French and its corresponding word in Russian, can you provide the Latin suffix?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-russian",
        "instruction": "Given a word in Russian, can you provide a concise and literary translation in English?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-russian",
        "instruction": "Given a word in Russian, can you provide a slang translation in English?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-woodworking",
        "instruction": "Given a woodworking machine, determine if it can be made to run on a single-phase 240V, 20A circuit and what is involved in doing so."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-woodworking",
        "instruction": "Can you recommend a HEPA vacuum or dust extractor that can be used with non-Festool tools?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ukrainian",
        "instruction": "Given a sentence in Ukrainian, translate it to English."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-ukrainian",
        "instruction": "Given a sentence in Ukrainian, provide a definition or explanation in Ukrainian."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-opendata",
        "instruction": "Given a business idea related to immigration, the task is to estimate the total addressable market by finding the number of people who physically relocate to another country with the intention to settle there permanently or acquire legal immigrant status in a foreign country in a year."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-poker",
        "instruction": "Given a poker scenario, determine if it is against the rules or not."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-poker",
        "instruction": "Provide advice on how to learn to play poker."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-poker",
        "instruction": "Given a poker scenario, provide the optimal strategy to maximize winnings."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-windowsphone",
        "instruction": "Given a phone model and a problem description, provide a solution to the problem."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-moderators",
        "instruction": "Given a question/answers website, suggest ways to keep users engaged in the initial stage."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-stackapps",
        "instruction": "Given a question title and body, predict the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-stellar",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-vegetarianism",
        "instruction": "Given a question about vitamin B12 absorption, predict whether the answer will mention an upper limit on absorption."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-vegetarianism",
        "instruction": "Given a question about Marmite, predict whether the answer will state that it is vegan."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-gaming",
        "instruction": "Given a question about Abathur in Heroes of the Storm, determine whether Abathur's Adrenal Overload buff affects his Monstrosity's attack speed."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-gaming",
        "instruction": "Given a question about friendship in a game, determine the amount of friendship points lost when shooting someone."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-codereview",
        "instruction": "Identify the problems with the given code."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-codereview",
        "instruction": "Suggest a better implementation for the given code."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-codereview",
        "instruction": "Identify the potential resource leaks in the given code."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-codereview",
        "instruction": "Suggest a better error handling mechanism for the given code."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-codereview",
        "instruction": "Suggest a better way to handle multithreading in the given code."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-mathoverflow",
        "instruction": "Given a title and body of a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-electronics",
        "instruction": "Given a sentence containing technical terms, explain the meaning of the terms in simpler language."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-electronics",
        "instruction": "Given a sentence containing technical terms, provide a step-by-step guide on how to perform the task described in the sentence."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-electronics",
        "instruction": "Given a sentence containing technical terms, identify the specific technology or process being described."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-apple",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-apple",
        "instruction": "Given a title and body of a question, predict the best advice."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-apple",
        "instruction": "Given a title and body of a question, predict the solution to a specific problem."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-diy",
        "instruction": "Given a sentence with a question about installation, provide a step-by-step guide to solve the problem."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-diy",
        "instruction": "Given a sentence with a question about cabinet hinges, provide a solution to the problem."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-diy",
        "instruction": "Given a solution to a problem, identify the problem it solves."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-magento",
        "instruction": "Suggest a solution for the given problem."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-gamedev",
        "instruction": "Given a game scenario, suggest a clever economic manipulation to retain players."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-gamedev",
        "instruction": "Given a game scenario, suggest a way to increase player retention."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-gamedev",
        "instruction": "Given a game scenario, suggest a way to convert players to paying customers."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-gamedev",
        "instruction": "Given a game scenario, suggest a way to increase player engagement."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-gamedev",
        "instruction": "Given a game scenario, suggest a way to improve the game experience."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-mathematica",
        "instruction": "Given a question or problem, provide the corresponding upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ell",
        "instruction": "Given a sentence, identify the meaning of \"What did you say?\""
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ell",
        "instruction": "Given a sentence, identify the meaning of \"What was I saying?\""
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ell",
        "instruction": "Given a sentence, identify the correct article to use for a noun."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-drupal",
        "instruction": "Given a question or problem in the title_body field, provide a solution or answer in the upvoted_answer field."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-android",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-crypto",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-cs",
        "instruction": "Given a graph and a starting vertex, find the shortest walk that covers k nodes."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-cs",
        "instruction": "Given a logic formula, determine the number of unique objects that satisfy it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-cs",
        "instruction": "Given a graph, add a new vertex and connect it to all other vertices to create a new graph. Determine if the new graph contains a walk of length (at most) n starting at s and covering all vertices."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-chemistry",
        "instruction": "Given a chemical compound, explain why it fumes when concentrated."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-chemistry",
        "instruction": "Given the depth of the sea, calculate the amount of hydrogen gas produced by water electrolysis."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-chemistry",
        "instruction": "What is the effect of pressure on water electrolysis of sea water with increase in depth?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-japanese",
        "instruction": "Given a phrase in English, provide the corresponding Japanese word or phrase."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-japanese",
        "instruction": "Given a Japanese word or phrase, provide the corresponding English phrase."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-japanese",
        "instruction": "Given a Japanese word or phrase, provide an example sentence using the word or phrase."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-japanese",
        "instruction": "Given an English phrase, provide an example sentence using the corresponding Japanese word or phrase."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-cooking",
        "instruction": "Given a question or statement, provide a concise and informative answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-hinduism",
        "instruction": "Given a question, provide the answer to the question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-dsp",
        "instruction": "Given a title and body of a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-bicycles",
        "instruction": "Identify the type of bike frame based on the provided image."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-bicycles",
        "instruction": "Provide a solution to the sudden deflation of a mountain bike tube."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-bicycles",
        "instruction": "Identify the cause of the sudden deflation of a mountain bike tube."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ethereum",
        "instruction": "Given a question about Ethereum state, provide the answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ethereum",
        "instruction": "Given an Ethereum state update, what is the old value?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-bitcoin",
        "instruction": "Given a Bitcoin mining difficulty level and hardware specifications, predict the expected earnings per unit time."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-bitcoin",
        "instruction": "Explain the process of selling Bitcoin using an ATM."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-bitcoin",
        "instruction": "Identify the key factors to consider when using a Bitcoin ATM to sell Bitcoin."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-islam",
        "instruction": "Given a sentence, provide an explanation of a specific term or concept mentioned in it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-german",
        "instruction": "Given a German verb with a prefix, provide the possible meanings of the prefix based on the verb it is attached to."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-german",
        "instruction": "Given a German verb with a prefix and its meaning, provide an example of a compound verb using this prefix and another verb."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-german",
        "instruction": "Given a German sentence with an impersonal imperative replacement form, provide the original imperative form of the verb."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-german",
        "instruction": "Given a German sentence with an impersonal imperative replacement form, provide a translation of the sentence."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-boardgames",
        "instruction": "Given a board game question, predict the most upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-economics",
        "instruction": "Given a GDP series in local currency units and PPP rates, transform it into nominal US dollars."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-economics",
        "instruction": "Given a nominal GDP in USD and a price index, transform it into real GDP in USD."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-economics",
        "instruction": "Find the GDP in constant US dollars for a given year and country."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-economics",
        "instruction": "Identify the differences between a macroeconomic model under a currency substitution scheme and a standard DSGE model."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-economics",
        "instruction": "Provide alternative models to DSGE for incorporating currency considerations in macroeconomic models."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-emacs",
        "instruction": "Given a Unicode character name, insert the corresponding character using Emacs."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-emacs",
        "instruction": "Suggest an Emacs package that provides a tree-like view of files and sub-directories of a specific directory."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-buddhism",
        "instruction": "Given a question related to Buddhism, provide a concise answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-buddhism",
        "instruction": "Given an answer related to Buddhism, provide the question that it answers."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-gardening",
        "instruction": "Identify the type of plant and suggest a solution to the problem."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-gardening",
        "instruction": "Provide tips on using grass clippings as a mulch in a garden."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-gardening",
        "instruction": "Suggest ways to improve airflow in a garden to prevent pests."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-astronomy",
        "instruction": "Given a question about Universal Time and Mean Solar Time, provide an answer with a reference to a credible source."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-fitness",
        "instruction": "Given a fitness problem, provide a list of possible causes of muscle spasms."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-fitness",
        "instruction": "Given a fitness problem, provide ways to prevent muscle spasms from happening in the future."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-fitness",
        "instruction": "Given a fitness problem, determine whether doing abs exercises with weight will make you stronger."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-fitness",
        "instruction": "Given a fitness problem, provide a list of abs exercises that involve weight."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-cstheory",
        "instruction": "Given a title or body of a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-linguistics",
        "instruction": "Given a sentence, identify the instances of epenthesis and deletion and explain the logic behind them within Optimality Theory."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-linguistics",
        "instruction": "Provide examples of languages that show affricate-to-plosive fortition, both synchronically and diachronically."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-linguistics",
        "instruction": "Given a sentence, identify instances of Correspondence Theory and provide examples of insertions."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-cogsci",
        "instruction": "Given a psychological concept, provide a brief explanation of it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-cogsci",
        "instruction": "Given a psychological concept, provide a resource or reference for further reading."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-cogsci",
        "instruction": "Given a psychological concept, provide an example of how it can be applied in real life."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-cogsci",
        "instruction": "Given a psychological concept, provide a counterexample or exception to it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-french",
        "instruction": "Translate the French sentence to English."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-french",
        "instruction": "Translate the English sentence to French."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-health",
        "instruction": "Given a sentence, predict the likelihood of a certain event happening based on the statistics provided."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-health",
        "instruction": "Given a sentence, extract the risk factors associated with a certain event."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-health",
        "instruction": "Given a sentence, extract the mechanism of action of a certain drug."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-chess",
        "instruction": "Given a chess game, can you predict the winner?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-chess",
        "instruction": "Given a chess game, can you predict the opening used?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-chess",
        "instruction": "Given a chess game, can you predict the next move?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-interpersonal",
        "instruction": "Given a title and body of a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-expressionengine",
        "instruction": "Given a title and body of a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-earthscience",
        "instruction": "Given a sentence, identify the causes of seasonal fluctuations in sea level."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-earthscience",
        "instruction": "Given a sentence, explain why Venezuela is dry."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-earthscience",
        "instruction": "Given a sentence, identify the location with the largest sea level seasonal cycles."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-civicrm",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-homebrew",
        "instruction": "Given a title or body of a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-hsm",
        "instruction": "Given a title or body of a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-avp",
        "instruction": "Given a video, split it into multiple components and animate them separately."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-avp",
        "instruction": "Given a video, automatically capture stills from it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-avp",
        "instruction": "Suggest a programming language or tool to capture stills from a video."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-expatriates",
        "instruction": "Given a job contract in Germany, calculate the payment for a given number of hours worked."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-expatriates",
        "instruction": "Given a job contract in Germany, what is the purpose of time registration?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-expatriates",
        "instruction": "Given a job contract in Germany, what is the purpose of tracking time spent on different projects?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-expatriates",
        "instruction": "Given a situation in the UK, will a graduate of an accredited Masters programme be eligible for a post-study work visa?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-expatriates",
        "instruction": "Given a situation in the UK, what is the likelihood that the Tier 1 (Post-study work) route will reopen?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-matheducators",
        "instruction": "Given a title or body of a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-3dprinting",
        "instruction": "Identify the possible causes and fixes for a specific problem in 3D printing."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-devops",
        "instruction": "Given a sentence, identify the security risks associated with copying an app to a specific location inside a docker image."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-devops",
        "instruction": "Given a sentence, recommend AWS Management Tools for monitoring an application deployed on AWS."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-devops",
        "instruction": "Given a sentence, identify the benefits of implementing Amazon Web Services monitoring with Nagios."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-bioinformatics",
        "instruction": "Given a package name, return its license."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-bioinformatics",
        "instruction": "Given a license, return all packages that use it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-bioinformatics",
        "instruction": "Given a bam file and a read group, extract the reads from the bam file that belong to the read group."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-bioinformatics",
        "instruction": "Given a bam file, extract the reads from the bam file that do not belong to any read group."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-computergraphics",
        "instruction": "Given a 3D structure, suggest a suitable 3D file format for representing it in ASCII format."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-computergraphics",
        "instruction": "Suggest a visualization technique for a unit quaternion on the surface of a unit sphere."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-computergraphics",
        "instruction": "Given a 3D structure, suggest a GUI design tool that can be used to create it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-elementaryos",
        "instruction": "Identify the issue with AppCenter installation and provide instructions to fix it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-elementaryos",
        "instruction": "Provide a solution to fix distorted sound quality via Bose QC35 headphones on Juno."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-elementaryos",
        "instruction": "Suggest a solution to fix Bluetooth headphone issues on elementaryOS."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-martialarts",
        "instruction": "Given a martial arts related question, provide a diagnosis and exercises to help with a specific injury."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-martialarts",
        "instruction": "Given a martial arts related question, recommend a martial art that is suitable for aging bodies."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-martialarts",
        "instruction": "Given a martial arts related question, recommend adaptations to techniques to accommodate injuries."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-hardwarerecs",
        "instruction": "Given a laptop model and its specifications, recommend a suitable price range for it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-hardwarerecs",
        "instruction": "Given a laptop model and its specifications, recommend a suitable SSD upgrade for it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-hardwarerecs",
        "instruction": "Given a laptop model and its specifications, recommend a suitable GPU upgrade for it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-hardwarerecs",
        "instruction": "Given a laptop model and its specifications, recommend a suitable battery upgrade for it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-hardwarerecs",
        "instruction": "Given a laptop model and its specifications, recommend a suitable CPU upgrade for it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-lifehacks",
        "instruction": "Given a problem, provide a solution that uses parchment paper to prevent brownies from sticking to the pan."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-lifehacks",
        "instruction": "Given a problem, provide a solution that uses a rubber band to keep earphones in place while wearing a helmet."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-lifehacks",
        "instruction": "Given a solution, provide a problem that it solves."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-crafts",
        "instruction": "Given a title or body of a question, generate a list of possible answers based on the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-materials",
        "instruction": "Extract the upvoted answer from the given data."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-cseducators",
        "instruction": "Given a question or problem, provide a concise and helpful answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-languagelearning",
        "instruction": "Given a language learning goal and a timeframe, provide an estimate of the number of hours required to achieve the goal."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-languagelearning",
        "instruction": "Provide advice on language learning based on a given situation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-languagelearning",
        "instruction": "Provide information on official language certification exams."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-beer",
        "instruction": "Suggest a filtering system for a given type of liquid and its sediment."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-beer",
        "instruction": "Provide the pronunciation of a given word in a specific language."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-beer",
        "instruction": "Explain the meaning of a term in a specific context."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ebooks",
        "instruction": "Given a text containing special characters, provide the corresponding html entity code for each special character."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ebooks",
        "instruction": "Given a Kindle model and a website, provide instructions on how to view the website on the Kindle."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-esperanto",
        "instruction": "Given a sentence in Esperanto, provide the corresponding English translation."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-esperanto",
        "instruction": "Given an Esperanto word, provide its English translation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-esperanto",
        "instruction": "Identify the correct usage of \"en\" and \"dum\" in a sentence."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-esperanto",
        "instruction": "Provide an example sentence using \"en\" or \"dum\" correctly."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-cardano",
        "instruction": "Given a title and body of a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-conlang",
        "instruction": "Given a phoneme, suggest a symbol to represent it in a writing system."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-conlang",
        "instruction": "Given a set of symbols, suggest the corresponding phonemes they represent."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-conlang",
        "instruction": "Given a verb, suggest a possible adverb or dependent clause that could evolve from it."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-conlang",
        "instruction": "Given an adverb or dependent clause, suggest a possible verb that could have evolved from it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-drones",
        "instruction": "Given a drone-related question, provide a solution to the problem."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-drones",
        "instruction": "Given a drone-related solution, identify the problem it solves."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-iota",
        "instruction": "Given a title and body of a post, predict whether the post will receive a high number of upvotes or not."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-salesforce",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-wordpress",
        "instruction": "Given a text field, the task is to sanitize the input data in the field."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-wordpress",
        "instruction": "Given a text field, the task is to extract the function used to sanitize the input data."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-wordpress",
        "instruction": "Given a text field, the task is to identify the premium plugin used to add a text field if a specific option is selected."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-scifi",
        "instruction": "Given a sentence, identify the Lovecraft story it references."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-scifi",
        "instruction": "Given a sentence, identify the characters and their motivations."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-scifi",
        "instruction": "Given a sentence, identify the common themes in the series."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-physics",
        "instruction": "Given a physics problem, can you extract the relevant parameters and provide the solution?"
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-physics",
        "instruction": "Given a physics problem and its solution, can you identify the relevant parameters?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-sharepoint",
        "instruction": "Given a SharePoint search query tool, can you find the Managed Property that contains the rewritten parent URL or the item URL?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-sharepoint",
        "instruction": "Given a list of items with categories, can you group them by category and produce a list with headings for each group?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-sharepoint",
        "instruction": "Can you provide a display template that does path trimming?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-security",
        "instruction": "Given a hash and salt, generate a dynamic config for john the ripper jumbo to crack the hash."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-security",
        "instruction": "What are the recommended security measures for website log-in and registration?"
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-security",
        "instruction": "Given a website security scenario, identify the appropriate security measures to implement."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-worldbuilding",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-softwareengineering",
        "instruction": "Given a Rust function with a trait Add, write the correct return type for the function."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-softwareengineering",
        "instruction": "Given a scenario in an MVC framework, suggest a solution for handling shared code."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-softwareengineering",
        "instruction": "Given a scenario in an MVC framework, suggest a solution for loading data for a sidebar."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-softwareengineering",
        "instruction": "Given a scenario in an MVC framework, suggest a solution for calling class methods on a model directly from a view."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-money",
        "instruction": "Given a title and body of a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-travel",
        "instruction": "Given a travel destination, provide a list of reasons why it is a good starting location for an India trip."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-travel",
        "instruction": "Provide advice on traveling with sensitive electronics on a plane."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-travel",
        "instruction": "Identify the nearest international airport to a given location."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-photo",
        "instruction": "Given a question about film photography, provide a step-by-step guide to reloading a 126 film cartridge with 35mm film."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-photo",
        "instruction": "Given a camera model from Canon, find a comparable model from Nikon."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-webmasters",
        "instruction": "Given a title and body of a post, predict the likelihood of the post being upvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-workplace",
        "instruction": "Given a job interview question, determine whether it is illegal to ask in Switzerland."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-workplace",
        "instruction": "Given a job interview question, provide advice on how to answer it."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-workplace",
        "instruction": "Given a job applicant's education status, provide advice on how to list it on a resume."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ux",
        "instruction": "Given a title and body of a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-philosophy",
        "instruction": "Given a philosophical concept, provide a definition and distinguish it from other related concepts."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-philosophy",
        "instruction": "Given a definition, identify the philosophical concept it refers to."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-philosophy",
        "instruction": "Given a philosophical concept, provide an example of how it applies in real life."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-music",
        "instruction": "Given a musical progression from one note to another, identify the key and the scale used in the song."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-music",
        "instruction": "Given a set of notes on a harmonica, identify the major scale and chords that can be played."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-politics",
        "instruction": "Given a question about drug laws, provide the corresponding answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-politics",
        "instruction": "Given an answer about drug laws, provide the corresponding question."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-movies",
        "instruction": "Given a title and a body of a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-skeptics",
        "instruction": "Given a question related to health or wellness, determine if the claim is supported by scientific evidence."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-skeptics",
        "instruction": "Given a scientific study, determine the effect of a specific diet on immune response."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-skeptics",
        "instruction": "Given a product or treatment, determine if there is scientific evidence to support its claimed benefits."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-raspberrypi",
        "instruction": "Given a title and body of a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-puzzling",
        "instruction": "Fill in the blank with the correct letter based on the pattern in the triangle."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-puzzling",
        "instruction": "Find the meaning of the given word."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-puzzling",
        "instruction": "Provide a new word with a similar pattern as the given word."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-mechanics",
        "instruction": "Given a question or problem, provide a solution or method to solve it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-mechanics",
        "instruction": "Given a part or component of a vehicle, provide the size or specification of the required replacement part."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-networkengineering",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-quant",
        "instruction": "Given a title and body of a question, predict the most upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-spanish",
        "instruction": "Given a sentence, provide an explanation or definition of a specific term or concept mentioned in it."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-scicomp",
        "instruction": "Given a title and body of a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-pets",
        "instruction": "Given a sentence, identify the body language that should be used when dealing with aggressive dogs."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-pets",
        "instruction": "Given a sentence, identify the types of bacteria that live on every surface of a fish tank."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-pets",
        "instruction": "Given a sentence, identify the steps to set up a new fish tank."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-sqa",
        "instruction": "Given a sentence, extract the solution to the problem."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-sqa",
        "instruction": "Given a sentence, extract the method to login with http proxy using selenium."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-sound",
        "instruction": "Given a title or body of a post, recommend the best software for voice manipulation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-sound",
        "instruction": "Given a title or body of a post, recommend the best approach for enhancing natural voice."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-sound",
        "instruction": "Given a title or body of a post, explain the concept of drum replacement."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-pm",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-reverseengineering",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-reverseengineering",
        "instruction": "Given a title and body of a question, predict the CRC calculation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-reverseengineering",
        "instruction": "Given a title and body of a question, predict the function address."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-retrocomputing",
        "instruction": "Given a retro computer build, suggest a practical use for two or more VDPs or sound chips."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-retrocomputing",
        "instruction": "Given a practical use for a retro computer build, suggest the necessary hardware extensions."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-retrocomputing",
        "instruction": "Given a retro computer build, suggest a way to improve its performance."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-sports",
        "instruction": "Given a question about NHL history, provide the answer."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-sports",
        "instruction": "Given a rule from the Laws of the Game, explain the rule in plain language."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-robotics",
        "instruction": "Given a question about sensor noise and bias, predict whether an additional sensor is required to eliminate the cumulative position error."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-robotics",
        "instruction": "Given a question about a point on a robotic arm, compute the Jacobian with only the joints leading up to that link."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-robotics",
        "instruction": "Given a question about robotics, identify the type of sensor required to report any kind of position measurement."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-opensource",
        "instruction": "Given a GPL license, can you determine if the preamble and instructions can be omitted?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-opensource",
        "instruction": "Given a scenario, can you determine if the GPL requires the distribution of modified software?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-opensource",
        "instruction": "Given a scenario, can you determine if an NDA can be used to prevent the disclosure of a modified work?"
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-woodworking",
        "instruction": "Provide the advantages and disadvantages of using pine wood."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-woodworking",
        "instruction": "Explain the expansion and contraction of engineered bamboo."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-woodworking",
        "instruction": "Provide a comparison between the dimensional changes of bamboo and wood."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ukrainian",
        "instruction": "Given a Ukrainian word, provide its transliteration in English."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ukrainian",
        "instruction": "Given a Ukrainian word, provide its English synonym."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ukrainian",
        "instruction": "Given an English word, provide its Ukrainian translation."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ukrainian",
        "instruction": "Given a Ukrainian word, provide its definition in Ukrainian."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ukrainian",
        "instruction": "Given an English word, provide its definition in Ukrainian."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-patents",
        "instruction": "Given a title and body of a question, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-mythology",
        "instruction": "Given a question about Greek mythology, provide a brief answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-mythology",
        "instruction": "Given a name of a Greek goddess, provide a brief description of her."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-mythology",
        "instruction": "Given a story from Greek mythology, summarize it in a few sentences."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-or",
        "instruction": "Given a problem description, provide a mathematical model for the problem."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-or",
        "instruction": "Given a mathematical model, provide a problem description."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-windowsphone",
        "instruction": "Given a question, provide a list of equivalent apps that can be used on Windows Phone 7."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-windowsphone",
        "instruction": "Given a question, provide a list of apps that can recognise the song being played on the radio."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-windowsphone",
        "instruction": "Given a song, provide a list of apps that can recognise it."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-windowsphone",
        "instruction": "Given a song, provide a recommendation for a music streaming service."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-stackapps",
        "instruction": "Given a question title and body, retrieve the accepted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-moderators",
        "instruction": "Given a title and body of a post, predict the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl-ru",
        "instruction": "Given a sentence, suggest a possible upvoted answer for it."
    },
    {
        "input_fields": [
            "sentence_A",
            "sentence_B"
        ],
        "output_field": [
            "relatedness_score"
        ],
        "task_name": "sick",
        "instruction": "Determine the relatedness score between two sentences."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "snli",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis contradicts, entails, or is neutral to the premise. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "snli",
        "instruction": "Given a premise and a label, generate a hypothesis that contradicts, entails, or is neutral to the premise."
    },
    {
        "input_fields": [
            "targets",
            "multiple_choice_targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-abstract_narrative_understanding",
        "instruction": "Given a proverb, the task is to choose the narrative that best illustrates the proverb."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-analogical_similarity",
        "instruction": "Given a category of analogical similarity, provide two sentence episodes that fit this category."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-auto_categorization",
        "instruction": "Given a category, generate a question that belongs to this category."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-color",
        "instruction": "Given a color, provide its HCL representation."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-conlang_translation",
        "instruction": "Given an English sentence, translate it to Adna."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-contextual_parametric_knowledge_conflicts",
        "instruction": "Given a context and a correct answer, generate a question that can be answered by the context."
    },
    {
        "input_fields": [
            "multiple_choice_targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-contextual_parametric_knowledge_conflicts",
        "instruction": "Given a context and multiple choices, generate a question that can be answered by the context."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-elementary_math_qa",
        "instruction": "Can you design an arithmetic problem based on the formula?"
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-elementary_math_qa",
        "instruction": "Provide the arithmetic operations based on the answer."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-emoji_movie",
        "instruction": "Given a movie, provide the corresponding emoji."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-emojis_emotion_prediction",
        "instruction": "Given an emotion, choose the emoji that corresponds to the emotion."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-english_russian_proverbs",
        "instruction": "Given a Russian proverb/idiom, provide an English proverb/idiom with a similar meaning."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-entailed_polarity_hindi",
        "instruction": "Given an answer, provide the corresponding fact and question."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-gem",
        "instruction": "Given a simplified sentence, generate multiple variations of the original sentence."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-hindi_question_answering",
        "instruction": "Given an answer, find the corresponding question."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-identify_odd_metaphor",
        "instruction": "Extract the metaphorical language used in the sentence."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-implicit_relations",
        "instruction": "Given an answer, generate a context and a question."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-indic_cause_and_effect",
        "instruction": "Identify the cause and effect relationship in the sentence."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-intent_recognition",
        "instruction": "Given an intent, provide an example utterance."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-international_phonetic_alphabet_transliterate",
        "instruction": "Given a sentence in IPA, transliterate it into English."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-language_games",
        "instruction": "Translate a sentence from English to \"Egg language\"."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-linguistics_puzzles",
        "instruction": "Given a sentence in the made-up language, generate a sentence in English based on the translation."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-medical_questions_russian",
        "instruction": "Given a medical question and answer, determine the associated disease or symptom."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs",
            "multiple_choice_targets"
        ],
        "task_name": "tasksource/bigbench-novel_concepts",
        "instruction": "Given a common concept or theme, generate a list of items that relate to it."
    },
    {
        "input_fields": [
            "targets",
            "multiple_choice_targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-odd_one_out",
        "instruction": "Given a word that does not belong to a group, provide a list of words that belong to the group."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-periodic_elements",
        "instruction": "Given the name of an element, provide its atomic number."
    },
    {
        "input_fields": [
            "multiple_choice_targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-persian_idioms",
        "instruction": "Given a literal meaning, select the option which contains the corresponding Persian idiom."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-physics",
        "instruction": "Given a physics formula, generate a word problem that can be solved using this formula."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-real_or_fake_text",
        "instruction": "Given a sentence, identify the article it belongs to."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-repeat_copy_logic",
        "instruction": "Given a repeated output, generate the sentence with a repeat pattern."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-scientific_press_release",
        "instruction": "Given a press release title, generate a scientific headline that corresponds to it."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-semantic_parsing_in_context_sparc",
        "instruction": "Given a SQL query and a database schema, generate a natural language question that can be answered by the query."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-semantic_parsing_spider",
        "instruction": "Given a SQL query, extract the question being asked."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-similarities_abstraction",
        "instruction": "Given a similarity, provide a set of two objects that share this similarity."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-simp_turing_concept",
        "instruction": "Given an output string, output the corresponding input string."
    },
    {
        "input_fields": [
            "multiple_choice_targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-social_support",
        "instruction": "Given a label of unsupportive, neutral, or supportive, provide a reply that fits the label."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-suicide_risk",
        "instruction": "Given a level of suicide risk (no risk, low risk, moderate risk, or severe risk), provide a text that matches that level of risk."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs",
            "multiple_choice_targets"
        ],
        "task_name": "tasksource/bigbench-swahili_english_proverbs",
        "instruction": "Given an English proverb, choose the closest Swahili proverb/idiom in meaning."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-swedish_to_german_proverbs",
        "instruction": "Given a German proverb, provide the corresponding Swedish proverb."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-symbol_interpretation",
        "instruction": "Given a sentence that is not consistent with a structure of emojis, identify the structure."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-android",
        "instruction": "Given a question about Android devices, provide the best answer based on the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-android",
        "instruction": "Given a question about Android devices, provide the worst answer based on the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-android",
        "instruction": "Given a question about Android devices, provide a list of apps that can help locate a misplaced device."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-android",
        "instruction": "Given a question about Android devices, provide the file path for the wallpaper image."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-android",
        "instruction": "Given a question about Android devices, provide a list of folders to check for a missing image."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gardening",
        "instruction": "Given a gardening problem, provide a solution."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gardening",
        "instruction": "Identify the plant based on the description."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gardening",
        "instruction": "Given a plant, provide care advice."
    },
    {
        "input_fields": [
            "upvoted_answer"
        ],
        "output_field": [
            "title_body"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gardening",
        "instruction": "Given a plant, identify the plant type."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gardening",
        "instruction": "Given a gardening problem, identify the problem."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-gardening",
        "instruction": "Given a gardening problem, provide a humorous solution."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-raspberrypi",
        "instruction": "Given a post about Netflix on Raspberry Pi, recommend a solution to watch Netflix on Raspberry Pi."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-raspberrypi",
        "instruction": "Given a post about stuck in a rebooting loop, recommend a solution to fix the problem."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-raspberrypi",
        "instruction": "Given a post about Netflix on Raspberry Pi, recommend a solution to watch Netflix on Raspberry Pi that does not require purchasing PlayOn."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-raspberrypi",
        "instruction": "Given a post about stuck in a rebooting loop, recommend a solution to fix the problem that does not involve checking the power supply."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-mechanics",
        "instruction": "Given a title and body of a question, predict whether the answer will be upvoted or downvoted."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-mechanics",
        "instruction": "Given a title and body of a question, predict the most likely reason for the engine management light to come on after installing a new exhaust."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer",
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-mechanics",
        "instruction": "Given a title and body of a question, predict whether upgrading the starter wire will help improve the performance of the vehicle."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-windowsphone",
        "instruction": "Given a question about Windows Phone, provide the best answer based on the upvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl-windowsphone",
        "instruction": "Given a question about Windows Phone, provide the worst answer based on the downvoted answer."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-blender",
        "instruction": "Given a sentence, identify the modifier that can be used to fix the issue."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-blender",
        "instruction": "Given a sentence, identify the issue that needs to be fixed."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl-blender",
        "instruction": "Given a sentence, suggest a possible solution to the issue."
    },
    {
        "input_fields": [
            "targets"
        ],
        "output_field": [
            "inputs"
        ],
        "task_name": "tasksource/bigbench-cryptonite",
        "instruction": "Given the answer, generate a cryptic crossword clue."
    },
    {
        "input_fields": [
            "test_list"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "mbpp-full",
        "instruction": "Write a function to find the length of the longest increasing subsequence from the given list."
    },
    {
        "input_fields": [
            "test_list"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "mbpp-full",
        "instruction": "Write a function to find the first non-repeated character in a given string."
    },
    {
        "input_fields": [
            "test_list"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "mbpp-full",
        "instruction": "Write a function to find the sum of all prime numbers in a given range."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "prompt"
        ],
        "task_name": "mbpp-sanitized",
        "instruction": "Given a python function, the task is to identify the prompt for the function."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "language"
        ],
        "task_name": "codeparrot/github-code-clean-all-all",
        "instruction": "Identify the programming language used in the code snippet."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-all-all",
        "instruction": "Identify the license used in the code snippet."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-all-all",
        "instruction": "Identify the repository name where the code snippet is located."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-all-all",
        "instruction": "Identify the size of the code snippet."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-all-all",
        "instruction": "Identify the path of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "language"
        ],
        "task_name": "codeparrot/github-code-clean-all-mit",
        "instruction": "Extract the programming language used in the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-all-mit",
        "instruction": "Extract the size of the code file."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-all-mit",
        "instruction": "Extract the license used for the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-all-mit",
        "instruction": "Extract the repository name where the code is located."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-all-mit",
        "instruction": "Extract the path of the code file."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-all-apache-2.0",
        "instruction": "Given a file path, return the code in the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "language"
        ],
        "task_name": "codeparrot/github-code-clean-all-gpl-3.0",
        "instruction": "Given a code snippet, extract the programming language used."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-all-gpl-3.0",
        "instruction": "Given a code snippet, extract the size of the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-all-gpl-3.0",
        "instruction": "Given a repository name, extract the path of the code file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-all-gpl-3.0",
        "instruction": "Given a code snippet, extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "language"
        ],
        "task_name": "codeparrot/github-code-clean-all-gpl-2.0",
        "instruction": "Identify the programming language of the code snippet."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-all-gpl-2.0",
        "instruction": "Identify the license of the code snippet."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-all-gpl-2.0",
        "instruction": "Identify the repository name of the code snippet."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-all-gpl-2.0",
        "instruction": "Identify the size of the code snippet."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-all-gpl-2.0",
        "instruction": "Identify the path of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "language"
        ],
        "task_name": "codeparrot/github-code-clean-all-bsd-3-clause",
        "instruction": "Given a code snippet, extract the programming language used."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-all-bsd-3-clause",
        "instruction": "Given a code snippet, extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-all-bsd-3-clause",
        "instruction": "Given a code snippet, extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-all-bsd-3-clause",
        "instruction": "Given a code snippet, extract the path of the file."
    },
    {
        "input_fields": [
            "repo_name",
            "path",
            "language"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-all-lgpl-3.0",
        "instruction": "Given a repository name, path and language, return the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "language"
        ],
        "task_name": "codeparrot/github-code-clean-all-lgpl-2.1",
        "instruction": "Given a code snippet, identify the programming language used."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-all-lgpl-2.1",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-all-lgpl-2.1",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-all-lgpl-2.1",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "language"
        ],
        "task_name": "codeparrot/github-code-clean-all-bsd-2-clause",
        "instruction": "Identify the programming language used in the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-all-bsd-2-clause",
        "instruction": "Identify the repository name where the code snippet is located."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-all-bsd-2-clause",
        "instruction": "Identify the license used for the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-all-bsd-2-clause",
        "instruction": "Identify the size of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-all-bsd-2-clause",
        "instruction": "Identify the path of the code snippet within the repository."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "language"
        ],
        "task_name": "codeparrot/github-code-clean-all-cc0-1.0",
        "instruction": "Given a code snippet, extract the programming language used."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-all-cc0-1.0",
        "instruction": "Given a code snippet, extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-all-cc0-1.0",
        "instruction": "Given a code snippet, extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-all-cc0-1.0",
        "instruction": "Given a code snippet, extract the path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-all-epl-1.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-all-epl-1.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-all-epl-1.0",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "language"
        ],
        "task_name": "codeparrot/github-code-clean-all-mpl-2.0",
        "instruction": "Given a code snippet, extract the programming language used."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-all-mpl-2.0",
        "instruction": "Given a code snippet, extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-all-mpl-2.0",
        "instruction": "Given a code snippet, extract the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-all-mpl-2.0",
        "instruction": "Given a code snippet, extract the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "language"
        ],
        "task_name": "codeparrot/github-code-clean-all-unlicense",
        "instruction": "Given a code snippet, identify the programming language used."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-all-unlicense",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-all-unlicense",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-all-unlicense",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "language"
        ],
        "task_name": "codeparrot/github-code-clean-all-isc",
        "instruction": "Extract the programming language used in the code."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-all-isc",
        "instruction": "Extract the license used in the code."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-all-isc",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-all-isc",
        "instruction": "Extract the repository name of the code."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-all-isc",
        "instruction": "Extract the path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "language"
        ],
        "task_name": "codeparrot/github-code-clean-all-artistic-2.0",
        "instruction": "Given a code snippet, extract the programming language used."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-all-artistic-2.0",
        "instruction": "Given a repository name, extract the path of the file."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-all-artistic-2.0",
        "instruction": "Given a repository name, extract the size of the file."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Assembly-all",
        "instruction": "Extract the code from the given repository and file path."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Assembly-all",
        "instruction": "Identify the license used in the code."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Assembly-gpl-3.0",
        "instruction": "Identify the license under which the code is released."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Assembly-gpl-3.0",
        "instruction": "Identify the repository name where the code is located."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Assembly-gpl-2.0",
        "instruction": "Identify the license under which the code is released."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Assembly-gpl-2.0",
        "instruction": "Identify the size of the code snippet."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Assembly-gpl-2.0",
        "instruction": "Identify the repository name of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Assembly-agpl-3.0",
        "instruction": "Given a code snippet, identify the repository name and path of the file."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Assembly-lgpl-3.0",
        "instruction": "Identify the license under which the code is distributed."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Assembly-lgpl-3.0",
        "instruction": "Identify the repository name where the code is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Assembly-lgpl-3.0",
        "instruction": "Identify the size of the code snippet. Answers must be one of 4383, 4146, 7649, 367."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-mit",
        "instruction": "Extract the file path from the given code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-mit",
        "instruction": "Extract the repository name from the given code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-mit",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-apache-2.0",
        "instruction": "Given a code snippet, identify the repository name where it belongs."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-apache-2.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-apache-2.0",
        "instruction": "Given a repository name, identify the path of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-gpl-3.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-gpl-3.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-gpl-3.0",
        "instruction": "Given a repository name and path, return the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-gpl-2.0",
        "instruction": "Given a code snippet, identify the repository name where it is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-gpl-2.0",
        "instruction": "Given a code snippet, identify the file path where it is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-gpl-2.0",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-bsd-3-clause",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-unlicense",
        "instruction": "Identify the license of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-unlicense",
        "instruction": "Identify the size of the code. Answers must be one of 1037, 21."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C-all",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C-all",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C-all",
        "instruction": "Given a code snippet, identify the path of the code file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C-mit",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C-mit",
        "instruction": "Extract the repository name and path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C-apache-2.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C-apache-2.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C-apache-2.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C-gpl-3.0",
        "instruction": "Given a code snippet, extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C-gpl-3.0",
        "instruction": "Given a code snippet, extract the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C-gpl-3.0",
        "instruction": "Given a code snippet, extract the size of the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-C-gpl-2.0",
        "instruction": "Identify the repository name where the code is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C-gpl-2.0",
        "instruction": "Identify the size of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C-gpl-2.0",
        "instruction": "Identify the path of the code snippet."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-C-agpl-3.0",
        "instruction": "Given a repository name and path, return the corresponding code snippet."
    },
    {
        "input_fields": [
            "language"
        ],
        "output_field": [
            "code",
            "repo_name",
            "path",
            "license",
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C-agpl-3.0",
        "instruction": "Given a language, return all code snippets written in that language."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C-lgpl-3.0",
        "instruction": "Given a code snippet, identify the repository name where it is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C-lgpl-3.0",
        "instruction": "Given a code snippet, identify the file path where it is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C-lgpl-3.0",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C-lgpl-2.1",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C-lgpl-2.1",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C-lgpl-2.1",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C-bsd-2-clause",
        "instruction": "Given a code snippet, identify the repository name and path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C-bsd-2-clause",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C-epl-1.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C-epl-1.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C-epl-1.0",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C-mpl-2.0",
        "instruction": "Extract the repository name and path of the code file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C-artistic-2.0",
        "instruction": "Given a code snippet, extract the size of the code. Answers must be one of 7347, 585, 5030, 575, 197, 1158."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C#-mit",
        "instruction": "Given a code snippet, extract the path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C#-mit",
        "instruction": "Given a code snippet, extract the name of the repository."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C#-mit",
        "instruction": "Given a code snippet, extract the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C#-apache-2.0",
        "instruction": "Given a code snippet, can you identify the repository name?"
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C#-apache-2.0",
        "instruction": "Given a code snippet, can you identify the size of the code?"
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-C#-apache-2.0",
        "instruction": "Given a repository name and path, can you extract the code snippet?"
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C#-gpl-3.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C#-gpl-3.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C#-gpl-3.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C#-gpl-2.0",
        "instruction": "Extract the repository name from the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C#-gpl-2.0",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C#-gpl-2.0",
        "instruction": "Extract the path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C#-bsd-3-clause",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C#-bsd-3-clause",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C#-bsd-3-clause",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C#-agpl-3.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C#-agpl-3.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-C#-agpl-3.0",
        "instruction": "Given a repository name and path, extract the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C#-lgpl-3.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C#-lgpl-3.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-C#-lgpl-3.0",
        "instruction": "Given a repository name and path, retrieve the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C#-lgpl-2.1",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C#-lgpl-2.1",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C#-lgpl-2.1",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C#-bsd-2-clause",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C#-bsd-2-clause",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C#-bsd-2-clause",
        "instruction": "Given a code snippet, identify the path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C#-cc0-1.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C#-cc0-1.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C#-cc0-1.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C#-epl-1.0",
        "instruction": "Extract the size of the code snippet. Answers must be one of 6283, 1663, 918, 2207, 29196, 1171, 4613, 817, 179."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-C#-epl-1.0",
        "instruction": "Extract the repository name and path of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C#-unlicense",
        "instruction": "Extract the size of the code in bytes."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C#-isc",
        "instruction": "Extract the size of the code file. Answers must be one of 5235, 3009, 2227, 5785, 751, 918."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C++-all",
        "instruction": "Given a code snippet, extract the size of the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-C++-all",
        "instruction": "Given a code snippet, extract the repository name."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-C++-all",
        "instruction": "Given a code snippet, extract the path of the file."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C++-apache-2.0",
        "instruction": "Identify the license under which the code is released."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C++-apache-2.0",
        "instruction": "Identify the repository name where the code is located."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-C++-apache-2.0",
        "instruction": "Identify the size of the code snippet."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C++-apache-2.0",
        "instruction": "Identify the path of the code snippet."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-C++-apache-2.0",
        "instruction": "Identify the code snippet based on the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C++-gpl-2.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C++-gpl-2.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C++-gpl-2.0",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C++-bsd-3-clause",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C++-bsd-3-clause",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C++-bsd-3-clause",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C++-agpl-3.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C++-agpl-3.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-C++-agpl-3.0",
        "instruction": "Given a repository name and path, extract the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C++-bsd-2-clause",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C++-bsd-2-clause",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C++-bsd-2-clause",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C++-epl-1.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C++-epl-1.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C++-epl-1.0",
        "instruction": "Given a code snippet, identify the size of the file. Answers must be one of 3968, 6354, 33872, 1521, 1734, 2013, 6344, 454, 4968."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "license"
        ],
        "task_name": "codeparrot/github-code-clean-CMake-all",
        "instruction": "Extract the license used in the code. Answers must be one of gpl-3.0, bsd-3-clause, apache-2.0, gpl-2.0, mit, bsd-2-clause, lgpl-3.0."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-CMake-all",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-CMake-mit",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-CMake-gpl-3.0",
        "instruction": "Identify the license used in the code snippet."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-CMake-gpl-3.0",
        "instruction": "Identify the repository name where the code is located."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-CMake-gpl-3.0",
        "instruction": "Identify the size of the code snippet."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-CMake-gpl-3.0",
        "instruction": "Identify the path of the code snippet."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-CSS-mit",
        "instruction": "Given a repository name, return the size of the repository."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-CSS-apache-2.0",
        "instruction": "Given a CSS code snippet, identify the repository name and path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-CSS-apache-2.0",
        "instruction": "Given a CSS code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-CSS-gpl-2.0",
        "instruction": "Identify the repository name and path for the given code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-CSS-bsd-3-clause",
        "instruction": "Identify the repository name and path of the CSS file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-CSS-bsd-3-clause",
        "instruction": "Identify the size of the CSS file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-CSS-lgpl-3.0",
        "instruction": "Find the repository name and path of the CSS file."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-CSS-lgpl-3.0",
        "instruction": "Determine the license of the code."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-CSS-lgpl-3.0",
        "instruction": "Calculate the size of the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-CSS-epl-1.0",
        "instruction": "Given a repository name, find the path of the CSS file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-CSS-mpl-2.0",
        "instruction": "Given a CSS code snippet, identify the repository name and path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-CSS-mpl-2.0",
        "instruction": "Given a CSS code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-CSS-unlicense",
        "instruction": "Identify the repository name and path of the CSS file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-CSS-unlicense",
        "instruction": "Identify the size of the CSS file."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-CSS-isc",
        "instruction": "Identify the license used in the repository."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Dockerfile-all",
        "instruction": "Extract the name of the repository from the given code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "license"
        ],
        "task_name": "codeparrot/github-code-clean-Dockerfile-all",
        "instruction": "Extract the license type used in the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Dockerfile-all",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Dockerfile-all",
        "instruction": "Extract the path of the code."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Dockerfile-mit",
        "instruction": "Extract the license used in the code."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Dockerfile-mit",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Dockerfile-apache-2.0",
        "instruction": "Identify the repository name and path of the Dockerfile."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Dockerfile-apache-2.0",
        "instruction": "Identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Dockerfile-bsd-3-clause",
        "instruction": "Extract the size of the Dockerfile. Answers must be one of 131, 166, 644, 2724, 180, 2109."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Dockerfile-bsd-3-clause",
        "instruction": "Extract the repository name of the Dockerfile."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Dockerfile-bsd-3-clause",
        "instruction": "Extract the path of the Dockerfile."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-GO-all",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-GO-all",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-GO-all",
        "instruction": "Given a repository name and path, extract the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-GO-apache-2.0",
        "instruction": "Given a code snippet, extract the repository name and path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-GO-apache-2.0",
        "instruction": "Given a code snippet, extract the size of the code."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-GO-gpl-3.0",
        "instruction": "Given a repository name and a file path, return the corresponding code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-GO-bsd-3-clause",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-GO-bsd-3-clause",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-GO-bsd-3-clause",
        "instruction": "Given a code snippet, identify the path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-GO-agpl-3.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-GO-agpl-3.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-GO-agpl-3.0",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-GO-mpl-2.0",
        "instruction": "Given a code snippet, identify the repository name where the code is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-GO-mpl-2.0",
        "instruction": "Given a code snippet, identify the file path where the code is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-GO-mpl-2.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-all",
        "instruction": "Extract the file path from the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-all",
        "instruction": "Extract the name of the repository from the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "license"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-all",
        "instruction": "Extract the license type of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-all",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-mit",
        "instruction": "Extract the repository name and path from the given code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-mit",
        "instruction": "Calculate the size of the given code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-apache-2.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-apache-2.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-apache-2.0",
        "instruction": "Given a repository name, identify the path of the code file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-gpl-3.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-gpl-3.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-gpl-3.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-gpl-2.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-gpl-2.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-gpl-2.0",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-bsd-2-clause",
        "instruction": "Given a code snippet, extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-bsd-2-clause",
        "instruction": "Given a code snippet, extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-bsd-2-clause",
        "instruction": "Given a code snippet, extract the path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-mpl-2.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-mpl-2.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-mpl-2.0",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-gpl-3.0",
        "instruction": "Identify the repository name and path of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-gpl-3.0",
        "instruction": "Determine the size of the code snippet."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-gpl-2.0",
        "instruction": "Given a license, find all the files that use this license."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-gpl-2.0",
        "instruction": "Given a repository name, find all the files in this repository."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-bsd-3-clause",
        "instruction": "Extract the license of the code from the given input."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-bsd-3-clause",
        "instruction": "Extract the repository name from the given input."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-bsd-3-clause",
        "instruction": "Extract the size of the code from the given input."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-bsd-3-clause",
        "instruction": "Extract the path of the code from the given input."
    },
    {
        "input_fields": [
            "language"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-bsd-3-clause",
        "instruction": "Extract the language of the code from the given input."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-bsd-2-clause",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-bsd-2-clause",
        "instruction": "Extract the repository name."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-bsd-2-clause",
        "instruction": "Extract the path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-epl-1.0",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-mpl-2.0",
        "instruction": "Given a repository name and a file path, return the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-isc",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-isc",
        "instruction": "Extract the repository name."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-isc",
        "instruction": "Extract the path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-artistic-2.0",
        "instruction": "Identify the repository name and path of the code file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-HTML-artistic-2.0",
        "instruction": "Identify the size of the code file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Java-all",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Java-all",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Java-all",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Java-gpl-3.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Java-gpl-3.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Java-gpl-3.0",
        "instruction": "Given a repository name and path, retrieve the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Java-bsd-3-clause",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Java-bsd-3-clause",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Java-bsd-3-clause",
        "instruction": "Given a repository name and path, extract the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Java-agpl-3.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Java-agpl-3.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Java-agpl-3.0",
        "instruction": "Given a repository name and path, extract the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Java-lgpl-3.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Java-lgpl-3.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Java-lgpl-3.0",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Java-lgpl-2.1",
        "instruction": "Given a repository name, return the size of the code in bytes."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Java-lgpl-2.1",
        "instruction": "Given a code snippet, return the path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Java-bsd-2-clause",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Java-bsd-2-clause",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Java-bsd-2-clause",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Java-cc0-1.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Java-cc0-1.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Java-cc0-1.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Java-epl-1.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Java-epl-1.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Java-epl-1.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Java-mpl-2.0",
        "instruction": "Identify the repository name and path of the Java code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Java-unlicense",
        "instruction": "Given a code snippet, extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Java-unlicense",
        "instruction": "Given a code snippet, extract the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Java-unlicense",
        "instruction": "Given a code snippet, extract the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Java-isc",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Java-isc",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Java-isc",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Java-artistic-2.0",
        "instruction": "Extract the size of the code. Answers must be one of 6371, 1401, 1808, 6686, 562, 602, 2424, 1600, 1071."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-mit",
        "instruction": "Given a repository name and path, provide the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-apache-2.0",
        "instruction": "Given a code snippet, identify the repository name where it is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-apache-2.0",
        "instruction": "Given a code snippet, identify the file path where it is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-apache-2.0",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-apache-2.0",
        "instruction": "Given a repository name and file path, retrieve the code snippet."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-gpl-3.0",
        "instruction": "Extract the license of the code from the input."
    },
    {
        "input_fields": [
            "language"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-gpl-3.0",
        "instruction": "Extract the language of the code from the input."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-gpl-3.0",
        "instruction": "Extract the repository name of the code from the input."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-gpl-3.0",
        "instruction": "Extract the path of the code from the input."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-gpl-3.0",
        "instruction": "Extract the size of the code from the input."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-gpl-2.0",
        "instruction": "Identify the license used for the code snippet."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-gpl-2.0",
        "instruction": "Identify the repository name where the code snippet is located."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-gpl-2.0",
        "instruction": "Identify the size of the code snippet."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-gpl-2.0",
        "instruction": "Identify the path of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-lgpl-3.0",
        "instruction": "Identify the size of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-lgpl-3.0",
        "instruction": "Identify the path of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-lgpl-2.1",
        "instruction": "Given a code snippet, extract the size of the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-lgpl-2.1",
        "instruction": "Given a code snippet, extract the repository name."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-lgpl-2.1",
        "instruction": "Given a code snippet, extract the path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-bsd-2-clause",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-bsd-2-clause",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-bsd-2-clause",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-epl-1.0",
        "instruction": "Given a code snippet, can you identify the repository name?"
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-epl-1.0",
        "instruction": "Given a code snippet, can you identify the file path?"
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-epl-1.0",
        "instruction": "Given a code snippet, can you identify the size of the code?"
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-mpl-2.0",
        "instruction": "Given a code file, extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-mpl-2.0",
        "instruction": "Given a code file, extract the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-mpl-2.0",
        "instruction": "Given a code file, extract the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-unlicense",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-unlicense",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-unlicense",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-isc",
        "instruction": "Identify the license used in the code snippet."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-isc",
        "instruction": "Given a repository name and file path, return the code snippet."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-isc",
        "instruction": "Given a repository name, return the size of the repository."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-isc",
        "instruction": "Given a code snippet, identify the repository name and file path."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-JavaScript-isc",
        "instruction": "Given a license, return the repository name and file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Julia-all",
        "instruction": "Given a code snippet, identify the repository name where it is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Julia-all",
        "instruction": "Given a code snippet, identify the size of the code in bytes."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Julia-all",
        "instruction": "Given a repository name and path, extract the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Julia-mit",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Julia-mit",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Julia-mit",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Julia-gpl-3.0",
        "instruction": "Given a code file, extract the name of the repository it belongs to."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Julia-gpl-3.0",
        "instruction": "Given a code file, extract the size of the file. Answers must be one of 1114, 451, 606, 2420, 1102."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Julia-gpl-3.0",
        "instruction": "Given a code file, extract the path of the file."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Lua-all",
        "instruction": "Given a repository name and a file path, extract the corresponding code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Lua-gpl-3.0",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Lua-gpl-2.0",
        "instruction": "Given a Lua code, can you identify the repository name?"
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Lua-gpl-2.0",
        "instruction": "Given a Lua code, can you identify the path of the code?"
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Lua-gpl-2.0",
        "instruction": "Given a Lua code, can you identify the size of the code?"
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Lua-bsd-3-clause",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Lua-bsd-3-clause",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Lua-bsd-3-clause",
        "instruction": "Given a repository name and path, extract the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Lua-bsd-2-clause",
        "instruction": "Given a code snippet, identify the repository name and path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Lua-bsd-2-clause",
        "instruction": "Given a code snippet, identify the size of the file. Answers must be one of 1144, 35215, 9216, 6488, 418, 5662."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "license"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-all",
        "instruction": "Given a code file, extract the license type used in the code. Answers must be one of isc, gpl-3.0, bsd-3-clause, apache-2.0, lgpl-2.1, gpl-2.0, artistic-2.0, mit, bsd-2-clause, lgpl-3.0."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-all",
        "instruction": "Given a code file, extract the size of the file."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-all",
        "instruction": "Given a repository name, extract the path of the code file."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "license"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-all",
        "instruction": "Given a repository name, extract the license type used in the code. Answers must be one of isc, gpl-3.0, bsd-3-clause, apache-2.0, lgpl-2.1, gpl-2.0, artistic-2.0, mit, bsd-2-clause, lgpl-3.0."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-mit",
        "instruction": "Given a code file, extract the repository name where the code is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-mit",
        "instruction": "Given a code file, extract the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-mit",
        "instruction": "Given a code file, extract the path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-apache-2.0",
        "instruction": "Extract the size of the code snippet."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-gpl-3.0",
        "instruction": "Given a repository name and a path, extract the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-gpl-2.0",
        "instruction": "Extract the repository name from the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-gpl-2.0",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-gpl-2.0",
        "instruction": "Extract the path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-bsd-3-clause",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-bsd-3-clause",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-bsd-3-clause",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-bsd-3-clause",
        "instruction": "Given a repository name and file path, return the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-lgpl-3.0",
        "instruction": "Given a code file, extract the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-lgpl-3.0",
        "instruction": "Given a code file, extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-lgpl-3.0",
        "instruction": "Given a code file, extract the path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-lgpl-2.1",
        "instruction": "Given a code file, extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-lgpl-2.1",
        "instruction": "Given a code file, extract the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-lgpl-2.1",
        "instruction": "Given a code file, extract the path of the file."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-bsd-2-clause",
        "instruction": "Given a repository name and a file path, return the corresponding code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-isc",
        "instruction": "Identify the repository name where the code snippet is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-isc",
        "instruction": "Identify the size of the code snippet. Answers must be one of 194, 922, 147, 208, 330, 695, 659, 326."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-isc",
        "instruction": "Identify the path of the code snippet within the repository."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path",
            "language",
            "license",
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-all",
        "instruction": "Extract the metadata of the code file."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-all",
        "instruction": "Identify the code file with the largest size."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-mit",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-apache-2.0",
        "instruction": "Identify the license used for the code snippet."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-apache-2.0",
        "instruction": "Given a repository name and path, retrieve the code snippet."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-gpl-3.0",
        "instruction": "Extract the code from the given repository and file path."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-gpl-3.0",
        "instruction": "Identify the license used in the repository."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-gpl-3.0",
        "instruction": "Calculate the size of the repository."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-gpl-2.0",
        "instruction": "Extract the code from the given repository and file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-gpl-2.0",
        "instruction": "Identify the size of the code."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-bsd-3-clause",
        "instruction": "Extract the license used in the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-bsd-3-clause",
        "instruction": "Extract the repository name from the code snippet."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-bsd-3-clause",
        "instruction": "Extract the size of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-bsd-3-clause",
        "instruction": "Extract the path of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-bsd-2-clause",
        "instruction": "Identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-bsd-2-clause",
        "instruction": "Identify the repository name of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-bsd-2-clause",
        "instruction": "Identify the path of the code."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-mpl-2.0",
        "instruction": "Extract the license used for the code snippet."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-mpl-2.0",
        "instruction": "Given a repository name and path, extract the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Markdown-artistic-2.0",
        "instruction": "Extract the repository name from the given code URL."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-all",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-all",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-all",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-all",
        "instruction": "Given a repository name and file path, extract the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-apache-2.0",
        "instruction": "Identify the repository name where the code is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-apache-2.0",
        "instruction": "Identify the file path of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-apache-2.0",
        "instruction": "Identify the size of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-gpl-3.0",
        "instruction": "Extract the file path from the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-gpl-3.0",
        "instruction": "Extract the repository name from the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-gpl-3.0",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-agpl-3.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-agpl-3.0",
        "instruction": "Given a code snippet, identify the path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-agpl-3.0",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-lgpl-2.1",
        "instruction": "Given a code snippet, identify the repository name where it is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-lgpl-2.1",
        "instruction": "Given a code snippet, identify the file path where it is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-lgpl-2.1",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-lgpl-2.1",
        "instruction": "Given a repository name and file path, extract the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-bsd-2-clause",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-bsd-2-clause",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-bsd-2-clause",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-epl-1.0",
        "instruction": "Extract the license used in the code."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-epl-1.0",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-epl-1.0",
        "instruction": "Extract the repository name."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-epl-1.0",
        "instruction": "Extract the path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-mpl-2.0",
        "instruction": "Identify the repository name and path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-PHP-mpl-2.0",
        "instruction": "Identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-mit",
        "instruction": "Extract the size of the code in bytes."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-mit",
        "instruction": "Extract the path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-apache-2.0",
        "instruction": "Extract the repository name from the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-apache-2.0",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-apache-2.0",
        "instruction": "Extract the path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-apache-2.0",
        "instruction": "Extract the repository name and path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-gpl-3.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-gpl-3.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-gpl-3.0",
        "instruction": "Given a repository name and path, return the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-gpl-2.0",
        "instruction": "Given a code file, extract the name of the repository it belongs to."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-gpl-2.0",
        "instruction": "Given a code file, extract the path of the file within the repository."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-gpl-2.0",
        "instruction": "Given a code file, extract the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-bsd-3-clause",
        "instruction": "Given a Perl script, extract the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-bsd-3-clause",
        "instruction": "Given a Perl script, extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-bsd-3-clause",
        "instruction": "Given a Perl script, extract the path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-lgpl-2.1",
        "instruction": "Extract the repository name and path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-unlicense",
        "instruction": "Given a Perl code snippet, identify the number of lines of code. Answers must be one of 302, 17940, 6777, 1254, 2019."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-artistic-2.0",
        "instruction": "Extract the path of the Perl file from the repository."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-artistic-2.0",
        "instruction": "Identify the license of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-artistic-2.0",
        "instruction": "Calculate the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-PowerShell-all",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-PowerShell-all",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-PowerShell-all",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-PowerShell-mit",
        "instruction": "Identify the repository name where the code is located."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-PowerShell-mit",
        "instruction": "Identify the license used for the code."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-PowerShell-mit",
        "instruction": "Identify the size of the code file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-PowerShell-mit",
        "instruction": "Identify the path of the code file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-PowerShell-apache-2.0",
        "instruction": "Given a code snippet, identify the repository name where it is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-PowerShell-apache-2.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-PowerShell-apache-2.0",
        "instruction": "Given a repository name and path, provide the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-PowerShell-gpl-3.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-PowerShell-gpl-3.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-PowerShell-gpl-3.0",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Python-apache-2.0",
        "instruction": "Extract the code from the given repository and file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Python-apache-2.0",
        "instruction": "Identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Python-bsd-3-clause",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Python-bsd-3-clause",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Python-bsd-3-clause",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code",
            "repo_name"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Python-agpl-3.0",
        "instruction": "Extract the path of the code file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Python-lgpl-2.1",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Python-lgpl-2.1",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Python-lgpl-2.1",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Python-bsd-2-clause",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Python-bsd-2-clause",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Python-bsd-2-clause",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Python-epl-1.0",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Python-epl-1.0",
        "instruction": "Extract the repository name."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Python-epl-1.0",
        "instruction": "Extract the path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Python-artistic-2.0",
        "instruction": "Given a code file, extract the name of the repository it belongs to."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Python-artistic-2.0",
        "instruction": "Given a code file, extract the path of the file within the repository."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Python-artistic-2.0",
        "instruction": "Given a code file, extract the size of the file. Answers must be one of 12272, 3342, 1849, 2661, 48571, 11845, 449."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-all",
        "instruction": "Given a code snippet, extract the name of the repository and the path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-all",
        "instruction": "Given a code snippet, extract the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-mit",
        "instruction": "Given a code snippet, extract the name of the repository and the path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-mit",
        "instruction": "Given a code snippet, extract the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-apache-2.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-apache-2.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-apache-2.0",
        "instruction": "Given a repository name and path, extract the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-gpl-3.0",
        "instruction": "Given a code snippet, extract the path of the file where the code is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-bsd-3-clause",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-agpl-3.0",
        "instruction": "Given a code snippet, predict the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-agpl-3.0",
        "instruction": "Given a code snippet, predict the size of the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-agpl-3.0",
        "instruction": "Given a repository name, predict the path of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-lgpl-2.1",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-lgpl-2.1",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-lgpl-2.1",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-bsd-2-clause",
        "instruction": "Extract the license of the code from the given input."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-bsd-2-clause",
        "instruction": "Extract the size of the code from the given input."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-bsd-2-clause",
        "instruction": "Extract the repository name from the given input."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-bsd-2-clause",
        "instruction": "Extract the path of the code from the given input."
    },
    {
        "input_fields": [
            "language"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-bsd-2-clause",
        "instruction": "Extract the programming language of the code from the given input."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-cc0-1.0",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-cc0-1.0",
        "instruction": "Extract the repository name."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-cc0-1.0",
        "instruction": "Extract the path of the code."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-cc0-1.0",
        "instruction": "Extract the license of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-epl-1.0",
        "instruction": "Given a code snippet, identify the size of the file. Answers must be one of 10645, 2394."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-unlicense",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-unlicense",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-unlicense",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-isc",
        "instruction": "Identify the license of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-isc",
        "instruction": "Identify the repository name of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-isc",
        "instruction": "Identify the size of the code snippet. Answers must be one of 753, 561, 31, 376, 6243, 170."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-isc",
        "instruction": "Identify the path of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Rust-all",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Rust-all",
        "instruction": "Given a code snippet, identify the path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "license"
        ],
        "task_name": "codeparrot/github-code-clean-Rust-all",
        "instruction": "Given a code snippet, identify the license used. Answers must be one of unlicense, gpl-3.0, bsd-3-clause, agpl-3.0, apache-2.0, lgpl-2.1, mit, mpl-2.0."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Rust-all",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Rust-mit",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Rust-mit",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Rust-bsd-3-clause",
        "instruction": "Can you design a code snippet based on the license?"
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Rust-mpl-2.0",
        "instruction": "Given a code snippet, identify the size of the code in bytes."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Rust-mpl-2.0",
        "instruction": "Given a code snippet, identify the repository name where the code is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Rust-mpl-2.0",
        "instruction": "Given a code snippet, identify the path of the file where the code is located."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-SQL-all",
        "instruction": "Identify the repository name and path of the SQL file."
    },
    {
        "input_fields": [
            "language",
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-SQL-all",
        "instruction": "Identify the programming language and license of the SQL file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-SQL-mit",
        "instruction": "Identify the repository name and path of the SQL file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-SQL-gpl-2.0",
        "instruction": "Given a code snippet, extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-SQL-gpl-2.0",
        "instruction": "Given a code snippet, extract the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-SQL-gpl-2.0",
        "instruction": "Given a code snippet, extract the size of the file."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-SQL-agpl-3.0",
        "instruction": "Given a path, return the corresponding SQL file."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-SQL-agpl-3.0",
        "instruction": "Given a license, return the corresponding repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-SQL-mpl-2.0",
        "instruction": "Given a SQL code, extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-all",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-all",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-all",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-mit",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-mit",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-mit",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-apache-2.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-apache-2.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-apache-2.0",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-gpl-3.0",
        "instruction": "Given a code file, extract the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-gpl-3.0",
        "instruction": "Given a code file, extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-gpl-3.0",
        "instruction": "Given a code file, extract the path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-gpl-2.0",
        "instruction": "Given a code snippet, extract the size of the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-gpl-2.0",
        "instruction": "Given a code snippet, extract the repository name."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-gpl-2.0",
        "instruction": "Given a code snippet, extract the path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-bsd-3-clause",
        "instruction": "Given a code snippet, extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-bsd-3-clause",
        "instruction": "Given a code snippet, extract the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-bsd-3-clause",
        "instruction": "Given a code snippet, extract the size of the code."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-lgpl-3.0",
        "instruction": "Extract the license of the code from the input."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-lgpl-3.0",
        "instruction": "Extract the size of the code from the input."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-lgpl-3.0",
        "instruction": "Extract the repository name from the input."
    },
    {
        "input_fields": [
            "language"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-lgpl-3.0",
        "instruction": "Extract the language of the code from the input."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-lgpl-3.0",
        "instruction": "Extract the path of the code from the input."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-bsd-2-clause",
        "instruction": "Given a code snippet, extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-bsd-2-clause",
        "instruction": "Given a code snippet, extract the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-bsd-2-clause",
        "instruction": "Given a code snippet, extract the size of the file."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Shell-apache-2.0",
        "instruction": "Given a repository name and a path, return the corresponding code snippet."
    },
    {
        "input_fields": [
            "language"
        ],
        "output_field": [
            "code",
            "repo_name",
            "path",
            "license",
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Shell-apache-2.0",
        "instruction": "Given a language, return all code snippets written in that language."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Shell-bsd-3-clause",
        "instruction": "Find the size of the code in bytes."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Shell-agpl-3.0",
        "instruction": "Given a code file, extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Shell-agpl-3.0",
        "instruction": "Given a code file, extract the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Shell-agpl-3.0",
        "instruction": "Given a code file, extract the size of the file."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Shell-lgpl-3.0",
        "instruction": "Extract the license of the code from the given input."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Shell-lgpl-3.0",
        "instruction": "Extract the repository name from the given input."
    },
    {
        "input_fields": [
            "language"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Shell-lgpl-3.0",
        "instruction": "Extract the language of the code from the given input."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Shell-lgpl-2.1",
        "instruction": "Identify the repository name where the code snippet is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Shell-lgpl-2.1",
        "instruction": "Identify the size of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Shell-lgpl-2.1",
        "instruction": "Identify the path of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Shell-lgpl-2.1",
        "instruction": "Identify the repository name and path of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-all",
        "instruction": "Identify the repository name and path of the code file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-all",
        "instruction": "Identify the size of the code file."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-apache-2.0",
        "instruction": "Extract the code from the given repository and file path."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-apache-2.0",
        "instruction": "Identify the license used for the code."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-apache-2.0",
        "instruction": "Identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-gpl-3.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-gpl-3.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-gpl-3.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-gpl-2.0",
        "instruction": "Extract the size of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-gpl-2.0",
        "instruction": "Extract the repository name from the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-gpl-2.0",
        "instruction": "Extract the path of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-lgpl-2.1",
        "instruction": "Extract the size of the code file."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-lgpl-2.1",
        "instruction": "Extract the repository name of the code file."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-lgpl-2.1",
        "instruction": "Extract the path of the code file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-bsd-2-clause",
        "instruction": "Identify the repository name where the code is located."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-bsd-2-clause",
        "instruction": "Identify the license used for the code."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-bsd-2-clause",
        "instruction": "Identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-bsd-2-clause",
        "instruction": "Identify the path of the code file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-TeX-mit",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-TeX-mit",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-TeX-mit",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TeX-mit",
        "instruction": "Given a repository name and file path, retrieve the code snippet."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TeX-apache-2.0",
        "instruction": "Extract the license of the code from the input."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TeX-apache-2.0",
        "instruction": "Extract the repository name from the input."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TeX-apache-2.0",
        "instruction": "Extract the path of the code from the input."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-TeX-bsd-2-clause",
        "instruction": "Given a code snippet, extract the path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Visual Basic-mit",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Visual Basic-apache-2.0",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Visual Basic-apache-2.0",
        "instruction": "Extract the repository name of the code."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Visual Basic-apache-2.0",
        "instruction": "Extract the path of the code."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Visual Basic-gpl-2.0",
        "instruction": "Identify the license type of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Visual Basic-gpl-2.0",
        "instruction": "Identify the repository name of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Visual Basic-gpl-2.0",
        "instruction": "Identify the file path of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Visual Basic-gpl-2.0",
        "instruction": "Identify the size of the code snippet. Answers must be one of 5959, 1062, 10546, 1631, 43558, 801, 30511."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-simple",
        "instruction": "Given a set of actions, the task is to generate a natural language command that describes the actions."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-simple",
        "instruction": "Given a natural language command, the task is to generate a sequence of actions that can be executed to complete the command."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-addprim_jump",
        "instruction": "Given a set of commands, the task is to extract the action(s) associated with the command(s)."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-addprim_jump",
        "instruction": "Given a set of actions, the task is to generate a set of commands that would result in those actions."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-addprim_turn_left",
        "instruction": "Given a set of commands, the task is to generate the corresponding sequence of actions."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-addprim_turn_left",
        "instruction": "Given a sequence of actions, the task is to generate the corresponding set of commands."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-filler_num2",
        "instruction": "Given a sequence of commands, generate the corresponding sequence of actions."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-filler_num2",
        "instruction": "Given a sequence of actions, generate the corresponding sequence of commands."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-filler_num3",
        "instruction": "Given a sequence of commands, generate the corresponding sequence of actions."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-filler_num3",
        "instruction": "Given a sequence of actions, generate the corresponding sequence of commands."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-length",
        "instruction": "Given a set of commands, generate a sequence of actions to perform the commands."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-length",
        "instruction": "Given a set of actions, generate a sequence of commands that could have led to those actions."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-template_around_right",
        "instruction": "Given a sequence of commands, generate the corresponding sequence of actions."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-template_around_right",
        "instruction": "Given a sequence of actions, generate the corresponding sequence of commands."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-template_jump_around_right",
        "instruction": "Given a sequence of commands, generate the corresponding sequence of actions."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-template_jump_around_right",
        "instruction": "Given a sequence of actions, generate the corresponding sequence of commands."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-template_opposite_right",
        "instruction": "Given a set of commands, the task is to generate the corresponding set of actions."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-template_opposite_right",
        "instruction": "Given a set of actions, the task is to generate the corresponding set of commands."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-template_right",
        "instruction": "Given a sequence of commands, generate the corresponding sequence of actions."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-template_right",
        "instruction": "Given a sequence of actions, generate the corresponding sequence of commands."
    },
    {
        "input_fields": [
            "commands"
        ],
        "output_field": [
            "actions"
        ],
        "task_name": "scan-template_right",
        "instruction": "Given a sequence of commands, generate the corresponding sequence of actions with the left and right directions swapped."
    },
    {
        "input_fields": [
            "actions"
        ],
        "output_field": [
            "commands"
        ],
        "task_name": "scan-template_right",
        "instruction": "Given a sequence of actions, generate the corresponding sequence of commands with the left and right directions swapped."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "multi_nli",
        "instruction": "Given a premise and hypothesis, predict whether the hypothesis is entailed by the premise or not. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise"
        ],
        "output_field": [
            "genre"
        ],
        "task_name": "multi_nli",
        "instruction": "Extract the genre of the text. Answers must be one of telephone, slate, travel, fiction, government."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-agpl-3.0",
        "instruction": "Extract the size of the code. Answers must be one of 199, 329."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-agpl-3.0",
        "instruction": "Extract the code from the given path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-lgpl-2.1",
        "instruction": "Extract the repository name from the given code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-lgpl-2.1",
        "instruction": "Extract the path from the given code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-lgpl-2.1",
        "instruction": "Extract the size of the code. Answers must be one of 491, 47."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-isc",
        "instruction": "Extract the size of the code. Answers must be one of 521, 370."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-isc",
        "instruction": "Extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Batchfile-isc",
        "instruction": "Extract the path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C-cc0-1.0",
        "instruction": "Given a code file, extract the name of the repository."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C-cc0-1.0",
        "instruction": "Given a code file, extract the path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C-isc",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C-isc",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C-isc",
        "instruction": "Given a code snippet, identify the path of the code file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C++-mpl-2.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C++-mpl-2.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C++-mpl-2.0",
        "instruction": "Given a code snippet, identify the size of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-C++-unlicense",
        "instruction": "Given a code snippet, extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-C++-unlicense",
        "instruction": "Given a code snippet, extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-C++-unlicense",
        "instruction": "Given a code snippet, extract the path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-CMake-lgpl-3.0",
        "instruction": "Extract the repository name from the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-CMake-lgpl-3.0",
        "instruction": "Extract the path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-CMake-lgpl-3.0",
        "instruction": "Extract the size of the code. Answers must be one of 1739, 1413, 3185, 861, 1045, 5236, 3987."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code",
            "repo_name",
            "path",
            "language",
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-CSS-agpl-3.0",
        "instruction": "Given a CSS code snippet, identify the license used in the code."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Dockerfile-gpl-3.0",
        "instruction": "Extract the license used in the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Dockerfile-gpl-3.0",
        "instruction": "Extract the repository name from the given data."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Dockerfile-gpl-3.0",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-agpl-3.0",
        "instruction": "Extract the size of the code. Answers must be one of 62, 288, 282, 609, 3593, 6475, 5569."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-unlicense",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-unlicense",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Haskell-unlicense",
        "instruction": "Given a code snippet, identify the size of the code. Answers must be one of 1179, 1767, 541."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Julia-lgpl-2.1",
        "instruction": "Identify the license used for the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Julia-lgpl-2.1",
        "instruction": "Identify the repository name where the code snippet is located."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Julia-lgpl-2.1",
        "instruction": "Identify the size of the code snippet. Answers must be one of 97, 453."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Julia-lgpl-2.1",
        "instruction": "Identify the path of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Lua-agpl-3.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Lua-agpl-3.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Lua-agpl-3.0",
        "instruction": "Given a repository name and path, return the code snippet."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Lua-lgpl-3.0",
        "instruction": "Identify the license of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Lua-lgpl-3.0",
        "instruction": "Identify the repository name of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Lua-lgpl-3.0",
        "instruction": "Identify the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Lua-lgpl-3.0",
        "instruction": "Identify the path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-agpl-3.0",
        "instruction": "Given a code file, extract the number of lines of code. Answers must be one of 177, 1613, 1763, 1513, 376, 110, 391, 1351."
    },
    {
        "input_fields": [
            "repo_name",
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-unlicense",
        "instruction": "Given a repository name and a file path, return the code snippet."
    },
    {
        "input_fields": [
            "language"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-unlicense",
        "instruction": "Given a language, return the repositories that contain code in that language."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Makefile-unlicense",
        "instruction": "Given a size range, return the repositories that have code within that size range."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-agpl-3.0",
        "instruction": "Given a code file, extract the name of the repository it belongs to."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-agpl-3.0",
        "instruction": "Given a code file, extract the path of the file."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Perl-agpl-3.0",
        "instruction": "Given a code file, extract the size of the file."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-PowerShell-bsd-2-clause",
        "instruction": "Identify the license used for the code."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-PowerShell-bsd-2-clause",
        "instruction": "Identify the size of the code."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-lgpl-3.0",
        "instruction": "Extract the license of the code from the input."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-lgpl-3.0",
        "instruction": "Extract the size of the code from the input."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-lgpl-3.0",
        "instruction": "Extract the repository name from the input."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-lgpl-3.0",
        "instruction": "Extract the path of the code from the input."
    },
    {
        "input_fields": [
            "language"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Ruby-lgpl-3.0",
        "instruction": "Extract the programming language of the code from the input."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Rust-unlicense",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Rust-unlicense",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Rust-unlicense",
        "instruction": "Given a code snippet, identify the size of the code. Answers must be one of 5241, 33788, 5630, 251, 35."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-agpl-3.0",
        "instruction": "Given a code snippet, extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-agpl-3.0",
        "instruction": "Given a code snippet, extract the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-agpl-3.0",
        "instruction": "Given a code snippet, extract the path of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-mpl-2.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-mpl-2.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Scala-mpl-2.0",
        "instruction": "Given a code snippet, identify the size of the code. Answers must be one of 4250, 8837, 2534, 601, 6430, 2829."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-Shell-mpl-2.0",
        "instruction": "Identify the license used in the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name",
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-Shell-mpl-2.0",
        "instruction": "Identify the repository name and path of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-Shell-mpl-2.0",
        "instruction": "Identify the size of the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-bsd-3-clause",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-lgpl-3.0",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-epl-1.0",
        "instruction": "Extract the repository name from the given code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "path"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-epl-1.0",
        "instruction": "Extract the path of the file from the given code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-epl-1.0",
        "instruction": "Extract the size of the file from the given code snippet. Answers must be one of 1660, 1438, 222, 478, 366, 11054, 2261, 10895."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-isc",
        "instruction": "Extract the license of the code from the input."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-isc",
        "instruction": "Extract the size of the code from the input."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-isc",
        "instruction": "Extract the repository name from the input."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-isc",
        "instruction": "Extract the path of the code from the input."
    },
    {
        "input_fields": [
            "language"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TypeScript-isc",
        "instruction": "Extract the language of the code from the input."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TeX-cc0-1.0",
        "instruction": "Extract the license used for the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TeX-cc0-1.0",
        "instruction": "Extract the repository name from the given data."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TeX-cc0-1.0",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "path"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/github-code-clean-TeX-cc0-1.0",
        "instruction": "Extract the path of the code."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-abstract_algebra",
        "instruction": "Choose the correct answer for the abstract algebra problem. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-abstract_algebra",
        "instruction": "Provide the order of the cyclic subgroup of Z_n generated by a given element. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-abstract_algebra",
        "instruction": "Find the factor group of Z_n/<k>. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-anatomy",
        "instruction": "Choose the correct answer for the anatomy question. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-anatomy",
        "instruction": "Identify the structure that travels through the parotid gland. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-business_ethics",
        "instruction": "Choose the correct answer for the given question. Answers must be one of A, C, B."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-clinical_knowledge",
        "instruction": "Given a set of choices, identify the correct answer to a question about muscle fibres. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-clinical_knowledge",
        "instruction": "Given a set of post-operative observations, identify the observation that would NOT be expected after surgery. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-college_chemistry",
        "instruction": "Choose the correct answer for the chemistry question. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-college_chemistry",
        "instruction": "Provide the correct answer for the chemistry question. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "answer"
        ],
        "output_field": [
            "question",
            "choices"
        ],
        "task_name": "tasksource/mmlu-college_chemistry",
        "instruction": "Can you design a chemistry question based on the answer?"
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-college_mathematics",
        "instruction": "Choose the correct answer for the math problem. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "choices"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "tasksource/mmlu-college_mathematics",
        "instruction": "Provide the correct polynomial given the roots."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-college_mathematics",
        "instruction": "Calculate the volume of a solid in xyz-space given the bounding surfaces. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-college_medicine",
        "instruction": "Identify the correct explanation for a given phenomenon. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-college_medicine",
        "instruction": "Identify the scenario where a desire has become a temptation. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-college_physics",
        "instruction": "Given a physics question and answer choices, identify the correct answer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-econometrics",
        "instruction": "Given a question and a set of choices, identify the correct answer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-electrical_engineering",
        "instruction": "Given a question and answer choices, select the correct answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-elementary_mathematics",
        "instruction": "Calculate the total cost of a purchase given the cost of individual items. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-elementary_mathematics",
        "instruction": "Solve for the unknown variable in a linear equation. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-global_facts",
        "instruction": "Given a question and answer choices, select the correct answer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-global_facts",
        "instruction": "Given a question and answer choices, generate a plausible answer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-high_school_biology",
        "instruction": "Given a biology question and its answer choices, identify the correct answer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-high_school_chemistry",
        "instruction": "Identify the correct answer for the given question. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-high_school_chemistry",
        "instruction": "Provide the correct answer for the given question. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-high_school_computer_science",
        "instruction": "Given a Python function, choose the correct function that removes all leading and trailing whitespace in a string. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-high_school_computer_science",
        "instruction": "Given a description, choose the correct term that best describes it. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-high_school_european_history",
        "instruction": "Given a question and choices, choose the correct answer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-high_school_european_history",
        "instruction": "Given a question, provide the correct answer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-high_school_geography",
        "instruction": "Given a geography question and a list of choices, choose the correct answer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-high_school_government_and_politics",
        "instruction": "Given a question and answer choices, select the correct answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-high_school_government_and_politics",
        "instruction": "Given a question and answer choices, select the incorrect answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-high_school_macroeconomics",
        "instruction": "Identify the correct answer for the given question. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "answer"
        ],
        "output_field": [
            "question",
            "choices"
        ],
        "task_name": "tasksource/mmlu-high_school_macroeconomics",
        "instruction": "Given the answer, provide the question and choices."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-high_school_mathematics",
        "instruction": "Given a group of people who either always tell the truth or always lie, find the sum of all possible values of the number of people who always tell the truth. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-high_school_mathematics",
        "instruction": "Given a group of people who either always tell the truth or always lie, determine the number of people who always lie. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-high_school_statistics",
        "instruction": "Identify the correct answer for a given statistics question. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-high_school_world_history",
        "instruction": "Given a question and its answer choices, choose the correct answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-high_school_world_history",
        "instruction": "Given a historical text and a term, provide the definition of the term. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-human_aging",
        "instruction": "Given a question and a set of choices, choose the correct answer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "choices",
            "answer"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "tasksource/mmlu-human_aging",
        "instruction": "Given a set of choices and a correct answer, generate a question."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-human_sexuality",
        "instruction": "Given a question and a set of choices, identify the correct answer. Answers must be one of A, D, B."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-human_sexuality",
        "instruction": "Given a question and the incorrect choices, provide the correct answer. Answers must be one of A, D, B."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-international_law",
        "instruction": "Given a passage, determine whether it qualifies as innocent passage or not. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "choices"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "tasksource/mmlu-international_law",
        "instruction": "What is the definition of innocent passage?"
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-international_law",
        "instruction": "What is the kind of State practice required for international law? Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-international_law",
        "instruction": "What are the requirements for State practice in international law? Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-jurisprudence",
        "instruction": "Choose the correct answer for the given question. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-jurisprudence",
        "instruction": "Provide the correct answer for the given question. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "choices"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "tasksource/mmlu-jurisprudence",
        "instruction": "Can you design a question based on the given choices?"
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-logical_fallacies",
        "instruction": "Identify the logical fallacy described in the question. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-logical_fallacies",
        "instruction": "Choose the correct answer for the given question. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-management",
        "instruction": "Given a question and a set of choices, choose the correct answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-medical_genetics",
        "instruction": "Given a genetic disorder and a set of options, choose the most likely cause of the high frequency of the allele associated with the disorder in certain human populations. Answers must be one of A, D, B."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-medical_genetics",
        "instruction": "Given a technique and a set of options, choose the most accurate description of the technique. Answers must be one of A, D, B."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-miscellaneous",
        "instruction": "Given a question and a list of choices, select the correct answer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "choices",
            "answer"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "tasksource/mmlu-miscellaneous",
        "instruction": "Given a list of choices and a correct answer, generate a question."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-moral_disputes",
        "instruction": "Given a moral argument and its response, identify the best description of the response. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-moral_disputes",
        "instruction": "Identify the source of natural rights according to a given philosopher. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-moral_disputes",
        "instruction": "Given a set of choices, identify the correct answer to a moral or philosophical question. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-nutrition",
        "instruction": "Choose the correct answer for the nutrition question. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "choices"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "tasksource/mmlu-nutrition",
        "instruction": "Provide the correct statement about plasma lipoproteins."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-nutrition",
        "instruction": "Identify the statement that correctly describes \"prediabetes\". Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-philosophy",
        "instruction": "Given a philosophical question and a set of choices, choose the correct answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-prehistory",
        "instruction": "Given a question and a list of choices, choose the correct answer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-professional_accounting",
        "instruction": "Calculate the recognized gain and basis for a property transfer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-professional_accounting",
        "instruction": "Calculate the cost of goods manufactured based on income statement data. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-professional_law",
        "instruction": "Given a legal scenario, choose the correct answer based on the contract law. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-professional_law",
        "instruction": "Given a legal scenario, determine whether the Fourth Amendment was violated. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-professional_medicine",
        "instruction": "Given a medical scenario, choose the best course of action for the physician. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-professional_psychology",
        "instruction": "Identify the correct answer for the given question. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-public_relations",
        "instruction": "Given a question and a set of choices, identify the correct answer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-security_studies",
        "instruction": "Given a statement, choose the correct definition of social constructivism. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-security_studies",
        "instruction": "What is the effect of chemical weapons on international society? Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "answer"
        ],
        "output_field": [
            "question",
            "choices"
        ],
        "task_name": "tasksource/mmlu-security_studies",
        "instruction": "Can you design a question based on a given answer?"
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-sociology",
        "instruction": "Given a question and a set of choices, choose the correct answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-sociology",
        "instruction": "Given a question and a set of choices, provide the correct answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-us_foreign_policy",
        "instruction": "Given a question and multiple choices, identify the correct answer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-virology",
        "instruction": "Given a question and a set of choices, identify the correct answer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-world_religions",
        "instruction": "Choose the correct answer for the given question. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-world_religions",
        "instruction": "Provide the correct answer for the given question. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "answer"
        ],
        "output_field": [
            "question",
            "choices"
        ],
        "task_name": "tasksource/mmlu-world_religions",
        "instruction": "Can you design a question based on the answer?"
    },
    {
        "input_fields": [
            "question",
            "question_concept"
        ],
        "output_field": [
            "answerKey"
        ],
        "task_name": "commonsense_qa",
        "instruction": "Given a question and a concept, choose the correct answer. Answers must be one of A, D, B, C, E."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "question_concept"
        ],
        "task_name": "commonsense_qa",
        "instruction": "Given a question, provide the corresponding concept."
    },
    {
        "input_fields": [
            "question_concept"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "commonsense_qa",
        "instruction": "Given a concept, provide a question that relates to it."
    },
    {
        "input_fields": [
            "input_output"
        ],
        "output_field": [
            "solutions"
        ],
        "task_name": "codeparrot/apps-competition",
        "instruction": "Given the input_output field, write a function that takes in the inputs and returns the expected output."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "violence"
        ],
        "task_name": "ethos-multilabel",
        "instruction": "Determine if the text is violent or not. Answers must be one of not_violent, violent."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "directed_vs_generalized"
        ],
        "task_name": "ethos-multilabel",
        "instruction": "Determine if the text is directed or generalized. Answers must be one of generalied, directed."
    },
    {
        "input_fields": [
            "annotated_formula"
        ],
        "output_field": [
            "correct"
        ],
        "task_name": "math_qa",
        "instruction": "Given the annotated formula, calculate the answer to the math problem. Answers must be one of b, d, a, c, e."
    },
    {
        "input_fields": [
            "Problem"
        ],
        "output_field": [
            "linear_formula"
        ],
        "task_name": "math_qa",
        "instruction": "Given the problem, provide the linear formula used to solve it."
    },
    {
        "input_fields": [
            "Problem"
        ],
        "output_field": [
            "category"
        ],
        "task_name": "math_qa",
        "instruction": "Given the problem, provide the category of the math problem. Answers must be one of general, probability, geometry, other, physics, gain."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-college_computer_science",
        "instruction": "Given a multiple-choice question and its answer choices, select the correct answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-computer_security",
        "instruction": "Given a question and a set of choices, choose the correct answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-computer_security",
        "instruction": "Given a scenario and a set of choices, choose the security features that will prevent a security breach. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-conceptual_physics",
        "instruction": "Choose the correct answer for the physics question. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-conceptual_physics",
        "instruction": "Provide the correct answer for the physics question. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "answer"
        ],
        "output_field": [
            "question",
            "choices"
        ],
        "task_name": "tasksource/mmlu-conceptual_physics",
        "instruction": "Can you design a physics question based on the answer?"
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-high_school_physics",
        "instruction": "Given a physics problem and answer choices, select the correct answer. Answers must be one of A, C, B, D."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "tasksource/mmlu-high_school_psychology",
        "instruction": "Given a question and a set of choices, identify the correct answer. Answers must be one of A, D, B, C."
    },
    {
        "input_fields": [
            "paragraph",
            "question"
        ],
        "output_field": [
            "sentence_answer"
        ],
        "task_name": "lmqg/qg_squad",
        "instruction": "Given a paragraph and a question, the task is to extract the sentence that answers the question."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "subreddit"
        ],
        "task_name": "go_emotions-raw",
        "instruction": "Identify the subreddit where the text was posted."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "author"
        ],
        "task_name": "go_emotions-raw",
        "instruction": "Identify the author of the text."
    },
    {
        "input_fields": [
            "sentences",
            "question",
            "options"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "cbt-CN",
        "instruction": "Given a sentence and a blank, fill in the blank with the correct word."
    },
    {
        "input_fields": [
            "sentences",
            "question",
            "options"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "cbt-CN",
        "instruction": "Given a sentence and a blank, identify the correct word that fills in the blank."
    },
    {
        "input_fields": [
            "sentences",
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "cbt-CN",
        "instruction": "Given a sentence, identify the speaker of a quote."
    },
    {
        "input_fields": [
            "long_answer",
            "final_decision"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "pubmed_qa-pqa_labeled",
        "instruction": "Given a long answer, predict whether the final decision is yes or no."
    },
    {
        "input_fields": [
            "question",
            "final_decision"
        ],
        "output_field": [
            "long_answer"
        ],
        "task_name": "pubmed_qa-pqa_artificial",
        "instruction": "Given a question and its final decision, provide the long answer."
    },
    {
        "input_fields": [
            "long_answer",
            "final_decision"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "pubmed_qa-pqa_artificial",
        "instruction": "Given a long answer and its final decision, provide the question."
    },
    {
        "input_fields": [
            "content"
        ],
        "output_field": [
            "title"
        ],
        "task_name": "cbt-raw",
        "instruction": "Given a text, predict the title of the book."
    },
    {
        "input_fields": [
            "title"
        ],
        "output_field": [
            "content"
        ],
        "task_name": "cbt-raw",
        "instruction": "Given a book title, predict the content of the book."
    },
    {
        "input_fields": [
            "masked_sentence"
        ],
        "output_field": [
            "sub_surface"
        ],
        "task_name": "lama-trex",
        "instruction": "Fill in the blank with the correct road for the given sentence."
    },
    {
        "input_fields": [
            "masked_sentence",
            "sub_label"
        ],
        "output_field": [
            "obj_label"
        ],
        "task_name": "lama-squad",
        "instruction": "Given a sentence and a blank, predict the word that should be in the blank."
    },
    {
        "input_fields": [
            "negated"
        ],
        "output_field": [
            "masked_sentence"
        ],
        "task_name": "lama-squad",
        "instruction": "Determine whether the sentence is negated or not."
    },
    {
        "input_fields": [
            "masked_sentence",
            "template"
        ],
        "output_field": [
            "obj"
        ],
        "task_name": "lama-google_re",
        "instruction": "Given a masked sentence and a template, fill in the blank with the correct answer."
    },
    {
        "input_fields": [
            "sub",
            "pred"
        ],
        "output_field": [
            "obj"
        ],
        "task_name": "lama-google_re",
        "instruction": "Given a subject and a predicate, provide the object."
    },
    {
        "input_fields": [
            "sub",
            "pred"
        ],
        "output_field": [
            "masked_sentence"
        ],
        "task_name": "lama-google_re",
        "instruction": "Given a subject and a predicate, provide the masked sentence."
    },
    {
        "input_fields": [
            "sub",
            "pred"
        ],
        "output_field": [
            "obj"
        ],
        "task_name": "lama-conceptnet",
        "instruction": "Given a subject and a predicate, predict the object."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-single-supporting-fact",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-single-supporting-fact",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise or not entailed by the premise."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-two-supporting-facts",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-two-supporting-facts",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise if the label is \"entailed\", or not entailed by the premise if the label is \"not-entailed\"."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-three-supporting-facts",
        "instruction": "Given a premise and a hypothesis, determine whether the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-three-supporting-facts",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise if the label is \"entailed\", or not entailed by the premise if the label is \"not-entailed\"."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-two-arg-relations",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-two-arg-relations",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise if the label is \"entailed\", or not entailed by the premise if the label is \"not-entailed\"."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-three-arg-relations",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-three-arg-relations",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise or not."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "metaeval/babi_nli-three-arg-relations",
        "instruction": "Given a hypothesis and a label, generate a premise that entails the hypothesis or not."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-yes-no-questions",
        "instruction": "Determine whether the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-yes-no-questions",
        "instruction": "Generate a new hypothesis based on the premise."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-counting",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-counting",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise if the label is \"entailed\", or not entailed by the premise if the label is \"not-entailed\"."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-lists-sets",
        "instruction": "Given a premise and a hypothesis, determine whether the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-lists-sets",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise if the label is \"entailed\", or not entailed by the premise if the label is \"not-entailed\"."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-simple-negation",
        "instruction": "Determine whether the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-indefinite-knowledge",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-indefinite-knowledge",
        "instruction": "Given a premise and a hypothesis, identify the relationship between the two. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-indefinite-knowledge",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise or not."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-basic-coreference",
        "instruction": "Given a premise and a hypothesis, determine whether the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-basic-coreference",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise if the label is \"entailed\", and not entailed otherwise."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "metaeval/babi_nli-basic-coreference",
        "instruction": "Given a hypothesis and a label, generate a premise that entails the hypothesis if the label is \"entailed\", and does not entail the hypothesis otherwise."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-conjunction",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-conjunction",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise if the label is \"entailed\", and not entailed otherwise."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "metaeval/babi_nli-conjunction",
        "instruction": "Given a hypothesis and a label, generate a premise that entails the hypothesis if the label is \"entailed\", and does not entail the hypothesis otherwise."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-compound-coreference",
        "instruction": "Given a premise and a hypothesis, determine whether the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-compound-coreference",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise if the label is \"entailed\", and not entailed otherwise."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "metaeval/babi_nli-compound-coreference",
        "instruction": "Given a hypothesis and a label, generate a premise that entails the hypothesis if the label is \"entailed\", and does not entail the hypothesis otherwise."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-time-reasoning",
        "instruction": "Given a premise and a hypothesis, determine whether the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-time-reasoning",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise or not entailed by the premise."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-basic-deduction",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-basic-deduction",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise if the label is \"entailed\", or not entailed by the premise if the label is \"not-entailed\"."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-basic-induction",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-basic-induction",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise if the label is \"entailed\", or not entailed by the premise if the label is \"not-entailed\"."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-positional-reasoning",
        "instruction": "Given a premise and a hypothesis, predict whether the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-positional-reasoning",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise if the label is \"entailed\", or not entailed by the premise if the label is \"not-entailed\"."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-size-reasoning",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-size-reasoning",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise if the label is \"entailed\", or not entailed by the premise if the label is \"not-entailed\"."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-path-finding",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-path-finding",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise if the label is \"entailed\", or not entailed by the premise if the label is \"not-entailed\"."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/babi_nli-agents-motivations",
        "instruction": "Given a premise and a hypothesis, determine whether the hypothesis is entailed by the premise. Answers must be one of not-entailed, entailed."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "metaeval/babi_nli-agents-motivations",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise or not entailed by the premise."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "metaeval/babi_nli-agents-motivations",
        "instruction": "Given a hypothesis and a label, generate a premise that entails the hypothesis or does not entail the hypothesis."
    },
    {
        "input_fields": [
            "Dialogue_ID"
        ],
        "output_field": [
            "Utterance"
        ],
        "task_name": "silicone-dyda_da",
        "instruction": "Given a dialogue ID, extract all the utterances in the dialogue."
    },
    {
        "input_fields": [
            "Label"
        ],
        "output_field": [
            "Utterance"
        ],
        "task_name": "silicone-dyda_da",
        "instruction": "Given a label, extract all the utterances with the same label."
    },
    {
        "input_fields": [
            "Dialogue_ID"
        ],
        "output_field": [
            "Utterance"
        ],
        "task_name": "silicone-dyda_e",
        "instruction": "Extract the dialogue ID from the data."
    },
    {
        "input_fields": [
            "Utterance"
        ],
        "output_field": [
            "Idx"
        ],
        "task_name": "silicone-dyda_e",
        "instruction": "Provide the index of the utterance in the dialogue."
    },
    {
        "input_fields": [
            "Utterance"
        ],
        "output_field": [
            "Emotion"
        ],
        "task_name": "silicone-iemocap",
        "instruction": "Given an utterance, predict the emotion of the speaker. Answers must be one of exc, fru, ang, xxx, neu, sur, hap."
    },
    {
        "input_fields": [
            "Emotion"
        ],
        "output_field": [
            "Utterance"
        ],
        "task_name": "silicone-iemocap",
        "instruction": "Given an emotion, provide an utterance that expresses that emotion."
    },
    {
        "input_fields": [
            "Dialogue_ID"
        ],
        "output_field": [
            "Utterance"
        ],
        "task_name": "silicone-iemocap",
        "instruction": "Given a dialogue ID, provide all utterances in that dialogue."
    },
    {
        "input_fields": [
            "Utterance"
        ],
        "output_field": [
            "Speaker"
        ],
        "task_name": "silicone-maptask",
        "instruction": "Identify the speaker of the utterance. Answers must be one of f, g."
    },
    {
        "input_fields": [
            "Utterance"
        ],
        "output_field": [
            "Dialogue_Act"
        ],
        "task_name": "silicone-maptask",
        "instruction": "Identify the dialogue act of the utterance."
    },
    {
        "input_fields": [
            "Utterance"
        ],
        "output_field": [
            "Label"
        ],
        "task_name": "silicone-maptask",
        "instruction": "Identify the label of the utterance."
    },
    {
        "input_fields": [
            "Utterance"
        ],
        "output_field": [
            "Idx"
        ],
        "task_name": "silicone-maptask",
        "instruction": "Identify the index of the utterance."
    },
    {
        "input_fields": [
            "Utterance"
        ],
        "output_field": [
            "Speaker"
        ],
        "task_name": "silicone-meld_e",
        "instruction": "Identify the speaker of the utterance. Answers must be one of Monica, Chandler, Customer, Phoebe, Joey, Sergei, Ross, The Interviewer, Rachel, Jade."
    },
    {
        "input_fields": [
            "Utterance"
        ],
        "output_field": [
            "Dialogue_ID"
        ],
        "task_name": "silicone-meld_e",
        "instruction": "Match the utterance with its corresponding dialogue ID. Answers must be one of 7, 8, 9, 0, 3, 6, 4, 5, 2, 1."
    },
    {
        "input_fields": [
            "Utterance"
        ],
        "output_field": [
            "Speaker"
        ],
        "task_name": "silicone-meld_s",
        "instruction": "Identify the speaker of the utterance. Answers must be one of Monica, Chandler, Customer, Phoebe, Joey, Sergei, Ross, The Interviewer, Rachel, Jade."
    },
    {
        "input_fields": [
            "Utterance"
        ],
        "output_field": [
            "Speaker"
        ],
        "task_name": "silicone-oasis",
        "instruction": "Identify the speaker of the utterance. Answers must be one of b, a."
    },
    {
        "input_fields": [
            "Utterance"
        ],
        "output_field": [
            "Dialogue_Act"
        ],
        "task_name": "silicone-oasis",
        "instruction": "Identify the dialogue act of the utterance."
    },
    {
        "input_fields": [
            "Utterance"
        ],
        "output_field": [
            "Label"
        ],
        "task_name": "silicone-oasis",
        "instruction": "Identify the label of the utterance."
    },
    {
        "input_fields": [
            "Utterance"
        ],
        "output_field": [
            "Idx"
        ],
        "task_name": "silicone-oasis",
        "instruction": "Identify the index of the utterance."
    },
    {
        "input_fields": [
            "Utterance"
        ],
        "output_field": [
            "Dialogue_Act"
        ],
        "task_name": "silicone-swda",
        "instruction": "Identify the dialogue act of the utterance. Answers must be one of b, +, %, qo, na, sv_fx, x, ba_fe, qy^d, sd."
    },
    {
        "input_fields": [
            "From_Caller"
        ],
        "output_field": [
            "Utterance"
        ],
        "task_name": "silicone-swda",
        "instruction": "Identify the speaker who initiated the conversation."
    },
    {
        "input_fields": [
            "To_Caller"
        ],
        "output_field": [
            "Utterance"
        ],
        "task_name": "silicone-swda",
        "instruction": "Identify the speaker who responded to the conversation."
    },
    {
        "input_fields": [
            "Conv_ID"
        ],
        "output_field": [
            "Utterance"
        ],
        "task_name": "silicone-swda",
        "instruction": "Identify the conversation ID."
    },
    {
        "input_fields": [
            "answers"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "wikitablequestions-random-split-1",
        "instruction": "Given an answer, provide the question."
    },
    {
        "input_fields": [
            "answers"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "wikitablequestions-random-split-4",
        "instruction": "Provide a question based on the given answer."
    },
    {
        "input_fields": [
            "answers"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "wikitablequestions-random-split-5",
        "instruction": "Given an answer, provide the question."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "snips_built_in_intents",
        "instruction": "Given a sentence, determine the intent of the user."
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "snips_built_in_intents",
        "instruction": "Given an intent, generate a sentence that matches the intent."
    },
    {
        "input_fields": [
            "questions",
            "answers"
        ],
        "output_field": [
            "questions_answers"
        ],
        "task_name": "lmqg/qag_squad",
        "instruction": "Given a question and answer, the task is to combine them into a single string."
    },
    {
        "input_fields": [
            "claim"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "fever-v2.0",
        "instruction": "Given a claim, predict whether it supports or refutes a given statement. Answers must be one of Not Enough Info, SUPPORTS, NOT ENOUGH INFO, REFUTES."
    },
    {
        "input_fields": [
            "context",
            "prompt"
        ],
        "output_field": [
            "utterance"
        ],
        "task_name": "empathetic_dialogues",
        "instruction": "Given a context and a prompt, generate an empathetic response."
    },
    {
        "input_fields": [
            "context",
            "utterance"
        ],
        "output_field": [
            "selfeval"
        ],
        "task_name": "empathetic_dialogues",
        "instruction": "Given a context and an utterance, predict the self-evaluation score of the speaker."
    },
    {
        "input_fields": [
            "prompt"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "empathetic_dialogues",
        "instruction": "Given a prompt, predict the sentiment of the context."
    },
    {
        "input_fields": [
            "context",
            "question-X",
            "answer-Y",
            "goldstandard1"
        ],
        "output_field": [
            "judgements"
        ],
        "task_name": "circa",
        "instruction": "Given a context and a question asked by X, determine if the answer provided by Y matches the gold standard answer."
    },
    {
        "input_fields": [
            "context",
            "question-X"
        ],
        "output_field": [
            "canquestion-X"
        ],
        "task_name": "circa",
        "instruction": "Given a context and a question asked by X, can you provide a possible answer for Y?"
    },
    {
        "input_fields": [
            "description"
        ],
        "output_field": [
            "abstract"
        ],
        "task_name": "big_patent-f",
        "instruction": "Given a patent description, identify the key features of the invention."
    },
    {
        "input_fields": [
            "abstract"
        ],
        "output_field": [
            "description"
        ],
        "task_name": "big_patent-f",
        "instruction": "Given an abstract, generate a patent description."
    },
    {
        "input_fields": [
            "description"
        ],
        "output_field": [
            "abstract"
        ],
        "task_name": "big_patent-y",
        "instruction": "Given a description of an invention, identify the key features of the invention."
    },
    {
        "input_fields": [
            "abstract"
        ],
        "output_field": [
            "description"
        ],
        "task_name": "big_patent-y",
        "instruction": "Given an abstract of an invention, provide a description of the invention."
    },
    {
        "input_fields": [
            "description"
        ],
        "output_field": [
            "abstract"
        ],
        "task_name": "big_patent-y",
        "instruction": "Identify the components and their functions in a given technical diagram."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "hyperpartisan"
        ],
        "task_name": "hyperpartisan_news_detection-byarticle",
        "instruction": "Given a news article, predict whether it is hyperpartisan or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "title"
        ],
        "task_name": "hyperpartisan_news_detection-byarticle",
        "instruction": "Given a news article, extract the title."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "published_at"
        ],
        "task_name": "hyperpartisan_news_detection-byarticle",
        "instruction": "Given a news article, extract the publication date."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "hyperpartisan_news_detection-byarticle",
        "instruction": "Given a news article, extract the URL."
    },
    {
        "input_fields": [
            "passage",
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "boolq",
        "instruction": "Given a passage and a question, predict the answer (True/False). Answers must be one of True, False."
    },
    {
        "input_fields": [
            "passage"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "boolq",
        "instruction": "Given a passage and a question, generate a new question that can be answered by the passage."
    },
    {
        "input_fields": [
            "question",
            "options"
        ],
        "output_field": [
            "correct"
        ],
        "task_name": "aqua_rat-raw",
        "instruction": "Choose the correct answer for the given question. Answers must be one of A, C, B, D, E."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "rationale"
        ],
        "task_name": "aqua_rat-raw",
        "instruction": "Provide the rationale for the given question."
    },
    {
        "input_fields": [
            "rationale"
        ],
        "output_field": [
            "question",
            "options",
            "correct"
        ],
        "task_name": "aqua_rat-raw",
        "instruction": "Can you design a math problem based on the given rationale?"
    },
    {
        "input_fields": [
            "question",
            "article"
        ],
        "output_field": [
            "topic"
        ],
        "task_name": "selqa-answer_selection_analysis",
        "instruction": "Given a question and an article, predict the topic of the article."
    },
    {
        "input_fields": [
            "section",
            "topic"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "selqa-answer_selection_analysis",
        "instruction": "Given a section and a topic, provide a question related to the topic in the section."
    },
    {
        "input_fields": [
            "question",
            "article"
        ],
        "output_field": [
            "is_paraphrase"
        ],
        "task_name": "selqa-answer_selection_analysis",
        "instruction": "Given a question and an article, predict whether the question is a paraphrase of the article. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "question",
            "article"
        ],
        "output_field": [
            "topic"
        ],
        "task_name": "selqa-answer_triggering_analysis",
        "instruction": "Identify the topic of the article based on the given question."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "is_paraphrase"
        ],
        "task_name": "selqa-answer_triggering_analysis",
        "instruction": "Determine whether the given question is a paraphrase of another question in the dataset. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "question",
            "article"
        ],
        "output_field": [
            "section"
        ],
        "task_name": "selqa-answer_triggering_analysis",
        "instruction": "Identify the article section based on the given question."
    },
    {
        "input_fields": [
            "query"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "spider",
        "instruction": "Given a SQL query, generate the corresponding natural language question."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "query"
        ],
        "task_name": "spider",
        "instruction": "Given a natural language question, generate the corresponding SQL query."
    },
    {
        "input_fields": [
            "original"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "md_gender_bias-new_data",
        "instruction": "Correct the gender of the person in the given sentence."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "class_type"
        ],
        "task_name": "md_gender_bias-new_data",
        "instruction": "Determine the type of bias in the given sentence. Answers must be one of about, partner, self."
    },
    {
        "input_fields": [
            "text",
            "gender"
        ],
        "output_field": [
            "chosen_topic"
        ],
        "task_name": "md_gender_bias-wizard",
        "instruction": "Given a text and the gender of the speaker, predict the chosen topic."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "binary_label"
        ],
        "task_name": "md_gender_bias-convai2_inferred",
        "instruction": "Given a text, predict the binary label of the speaker (male/female). Answers must be one of ABOUT:male, ABOUT:female."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "ternary_label"
        ],
        "task_name": "md_gender_bias-convai2_inferred",
        "instruction": "Given a text, predict the ternary label of the speaker (male/female/gender-neutral)."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "binary_score"
        ],
        "task_name": "md_gender_bias-convai2_inferred",
        "instruction": "Given a text, predict the binary score of the speaker being male."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "ternary_score"
        ],
        "task_name": "md_gender_bias-convai2_inferred",
        "instruction": "Given a text, predict the ternary score of the speaker being gender-neutral."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "binary_label"
        ],
        "task_name": "md_gender_bias-light_inferred",
        "instruction": "Given a text, predict the binary label of the text. Answers must be one of ABOUT:male, ABOUT:female."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "binary_score"
        ],
        "task_name": "md_gender_bias-light_inferred",
        "instruction": "Given a text, predict the binary score of the text."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "ternary_score"
        ],
        "task_name": "md_gender_bias-light_inferred",
        "instruction": "Given a text, predict the ternary score of the text."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "binary_label"
        ],
        "task_name": "md_gender_bias-opensubtitles_inferred",
        "instruction": "Given a sentence, predict the binary gender label (male/female) with the highest score. Answers must be one of ABOUT:male, ABOUT:female."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "ternary_label"
        ],
        "task_name": "md_gender_bias-opensubtitles_inferred",
        "instruction": "Given a sentence, predict the ternary gender label (male/female/neutral) with the highest score."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "binary_label",
            "binary_score"
        ],
        "task_name": "md_gender_bias-opensubtitles_inferred",
        "instruction": "Given a sentence, predict the gender label (male/female) with the highest score and its corresponding score. Answers must be one of ABOUT:male, ABOUT:female."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "ternary_label",
            "ternary_score"
        ],
        "task_name": "md_gender_bias-opensubtitles_inferred",
        "instruction": "Given a sentence, predict the gender label (male/female/neutral) with the highest score and its corresponding score."
    },
    {
        "input_fields": [
            "word_masculine"
        ],
        "output_field": [
            "word_feminine"
        ],
        "task_name": "md_gender_bias-gendered_words",
        "instruction": "Given a masculine word, provide a feminine equivalent."
    },
    {
        "input_fields": [
            "word_feminine"
        ],
        "output_field": [
            "word_masculine"
        ],
        "task_name": "md_gender_bias-gendered_words",
        "instruction": "Given a feminine word, provide a masculine equivalent."
    },
    {
        "input_fields": [
            "answer"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "neural_code_search-evaluation_dataset",
        "instruction": "Given a code snippet, can you provide a brief description of what it does?"
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer_url"
        ],
        "task_name": "neural_code_search-evaluation_dataset",
        "instruction": "Given a question, can you provide the URL to the answer?"
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "filepath",
            "method_name",
            "start_line",
            "end_line"
        ],
        "task_name": "neural_code_search-search_corpus",
        "instruction": "Given a URL, extract the file path, method name, start line and end line of the code snippet."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/web_nlg-en",
        "instruction": "Given a city and an airport, generate a sentence describing the relationship between them."
    },
    {
        "input_fields": [
            "question",
            "nq_answer"
        ],
        "output_field": [
            "nq_doc_title"
        ],
        "task_name": "ambig_qa-full",
        "instruction": "Given a question and the correct answer, predict the document title where the answer can be found."
    },
    {
        "input_fields": [
            "nq_doc_title",
            "nq_answer"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "ambig_qa-full",
        "instruction": "Given a document title and the correct answer, predict the question that was asked."
    },
    {
        "input_fields": [
            "content"
        ],
        "output_field": [
            "rating"
        ],
        "task_name": "google_wellformed_query",
        "instruction": "Given a question, predict the rating of the question on a scale of 0 to 1. Answers must be one of 0.8, 0.0, 0.4, 0.2, 1.0, 0.6."
    },
    {
        "input_fields": [
            "rating"
        ],
        "output_field": [
            "content"
        ],
        "task_name": "google_wellformed_query",
        "instruction": "Given a rating, predict the question that is most likely to have that rating."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "numer_sense",
        "instruction": "Fill in the blank with the correct number."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "numer_sense",
        "instruction": "Identify the number of diseases represented by histiocytosis."
    },
    {
        "input_fields": [
            "question",
            "correct_answers",
            "incorrect_answers"
        ],
        "output_field": [
            "best_answer"
        ],
        "task_name": "truthful_qa-generation",
        "instruction": "Given a question and a set of answers, identify the correct answer."
    },
    {
        "input_fields": [
            "category"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "truthful_qa-generation",
        "instruction": "Given a set of misconceptions, generate a question."
    },
    {
        "input_fields": [
            "answer"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "nq_open",
        "instruction": "Given an answer, provide the corresponding question."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "biosses",
        "instruction": "Given two sentences, determine if they are synthetically lethal or not."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "relatedness_score"
        ],
        "task_name": "sem_eval_2014_task_1",
        "instruction": "Given a premise and a hypothesis, predict the relatedness score."
    },
    {
        "input_fields": [
            "entailment_judgment",
            "relatedness_score"
        ],
        "output_field": [
            "premise",
            "hypothesis"
        ],
        "task_name": "sem_eval_2014_task_1",
        "instruction": "Given an entailment judgment and a relatedness score, predict the premise and hypothesis."
    },
    {
        "input_fields": [
            "source_tokens"
        ],
        "output_field": [
            "error_location"
        ],
        "task_name": "great_code",
        "instruction": "Given a set of source tokens, the task is to identify the error location."
    },
    {
        "input_fields": [
            "source_tokens"
        ],
        "output_field": [
            "has_bug"
        ],
        "task_name": "great_code",
        "instruction": "Given a set of source tokens, the task is to identify whether there is a bug or not. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "source_tokens"
        ],
        "output_field": [
            "bug_kind",
            "bug_kind_name"
        ],
        "task_name": "great_code",
        "instruction": "Given a set of source tokens, the task is to identify the kind of bug. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "context",
            "question"
        ],
        "output_field": [
            "reference"
        ],
        "task_name": "mocha",
        "instruction": "Given a context and a question, predict the reference answer."
    },
    {
        "input_fields": [
            "context",
            "reference"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "mocha",
        "instruction": "Given a context and a reference answer, generate a plausible question."
    },
    {
        "input_fields": [
            "complex_sentence"
        ],
        "output_field": [
            "simple_sentence_1",
            "simple_sentence_2"
        ],
        "task_name": "wiki_split",
        "instruction": "Given a complex sentence, the task is to split it into two simple sentences."
    },
    {
        "input_fields": [
            "simple_sentence_1",
            "simple_sentence_2"
        ],
        "output_field": [
            "complex_sentence"
        ],
        "task_name": "wiki_split",
        "instruction": "Given two simple sentences, the task is to combine them into a complex sentence."
    },
    {
        "input_fields": [
            "normal_sentence",
            "simple_sentence"
        ],
        "output_field": [
            "gleu_score"
        ],
        "task_name": "wiki_auto-manual",
        "instruction": "Given a normal sentence and a simple sentence, calculate the GLEU score."
    },
    {
        "input_fields": [
            "normal_sentence"
        ],
        "output_field": [
            "simple_sentence"
        ],
        "task_name": "wiki_auto-manual",
        "instruction": "Given a normal sentence, provide a simplified version of the sentence."
    },
    {
        "input_fields": [
            "title",
            "headline"
        ],
        "output_field": [
            "source"
        ],
        "task_name": "newspop",
        "instruction": "Given a news article, predict the source of the article."
    },
    {
        "input_fields": [
            "title",
            "headline"
        ],
        "output_field": [
            "publish_date"
        ],
        "task_name": "newspop",
        "instruction": "Given a news article, predict the publish date of the article."
    },
    {
        "input_fields": [
            "question",
            "scenario"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "sharc",
        "instruction": "Given a question and a scenario, determine if the answer is \"Yes\" or \"No\"."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "adv_glue-adv_sst2",
        "instruction": "Given a sentence, predict its sentiment label. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "question1",
            "question2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "adv_glue-adv_qqp",
        "instruction": "Determine if two questions are asking the same thing. Answers must be one of duplicate, not_duplicate."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "adv_glue-adv_mnli",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis contradicts the premise. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "adv_glue-adv_mnli",
        "instruction": "Given a premise and a label, generate a hypothesis that is consistent with the premise and the label."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "adv_glue-adv_mnli_mismatched",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis is entailed by the premise. Answers must be one of entailment, neutral, contradiction."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "adv_glue-adv_mnli_mismatched",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis contradicts the premise. Answers must be one of entailment, neutral, contradiction."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "adv_glue-adv_mnli_mismatched",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis is neutral with respect to the premise. Answers must be one of entailment, neutral, contradiction."
    },
    {
        "input_fields": [
            "question",
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "adv_glue-adv_qnli",
        "instruction": "Determine if the sentence entails the answer to the question. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "adv_glue-adv_rte",
        "instruction": "Determine whether the two sentences entail each other or not. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence1"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "adv_glue-adv_rte",
        "instruction": "Given a sentence, predict whether it entails the other sentence or not. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "adv_glue-adv_rte",
        "instruction": "Given a sentence, generate a sentence that entails it."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "pile-of-law/pile-of-law-scotus_filings",
        "instruction": "Extract the URL of the document."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "created_timestamp"
        ],
        "task_name": "pile-of-law/pile-of-law-oig",
        "instruction": "Extract the created timestamp from the text. Answers must be one of 2008, 2014, 2009, 2011."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "created_timestamp"
        ],
        "task_name": "pile-of-law/pile-of-law-olc_memos",
        "instruction": "Determine the date when the memo was created."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "pile-of-law/pile-of-law-olc_memos",
        "instruction": "Identify the URL of the memo."
    },
    {
        "input_fields": [
            "created_timestamp"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "pile-of-law/pile-of-law-echr",
        "instruction": "Identify the year in which the case was created."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "pile-of-law/pile-of-law-fre",
        "instruction": "Given a legal rule, identify the URL where it can be found."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "pile-of-law/pile-of-law-eoir",
        "instruction": "Extract the URL of the decision."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "pile-of-law/pile-of-law-irs_legal_advice_memos",
        "instruction": "Extract the URL of the document."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "zest",
        "instruction": "Given a context, provide a question that can be answered by the context."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "language"
        ],
        "task_name": "CodedotAI/code_clippy_github-all-gpl-2.0",
        "instruction": "Extract the programming language used in the code."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-all-gpl-2.0",
        "instruction": "Extract the size of the code file."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "language"
        ],
        "task_name": "CodedotAI/code_clippy_github-all-gpl-3.0",
        "instruction": "Identify the programming language used in the code."
    },
    {
        "input_fields": [
            "file_path"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-all-gpl-3.0",
        "instruction": "Find the size of the code file."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "language"
        ],
        "task_name": "CodedotAI/code_clippy_github-all-bsd-3-clause",
        "instruction": "Given a code file, extract the language it is written in."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-all-bsd-3-clause",
        "instruction": "Given a code file, extract the size of the file."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-all-bsd-3-clause",
        "instruction": "Given a code file, extract the repository name."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-all-bsd-3-clause",
        "instruction": "Given a code file, extract the file path."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "language"
        ],
        "task_name": "CodedotAI/code_clippy_github-all-lgpl-3.0",
        "instruction": "Given a code file, extract the language used."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-all-lgpl-2.1",
        "instruction": "Extract the file path from the code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "language"
        ],
        "task_name": "CodedotAI/code_clippy_github-all-lgpl-2.1",
        "instruction": "Identify the programming language used in the code snippet."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code_text"
        ],
        "task_name": "CodedotAI/code_clippy_github-all-lgpl-2.1",
        "instruction": "Find the repository name of the code snippet."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code_text"
        ],
        "task_name": "CodedotAI/code_clippy_github-all-lgpl-2.1",
        "instruction": "What is the size of the code snippet?"
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "language"
        ],
        "task_name": "CodedotAI/code_clippy_github-all-mpl-2.0",
        "instruction": "Given a code file, extract the language used in the code."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-all-mpl-2.0",
        "instruction": "Given a code file, extract the size of the file."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-all-mpl-2.0",
        "instruction": "Given a code file, extract the repository name."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-all-mpl-2.0",
        "instruction": "Given a code file, extract the file path."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-C-all",
        "instruction": "Extract the file path from the code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-C-all",
        "instruction": "Extract the size of the code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-C-all",
        "instruction": "Extract the repository name from the code snippet."
    },
    {
        "input_fields": [
            "file_path"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-C-apache-2.0",
        "instruction": "Find the size of the code file."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-C-gpl-2.0",
        "instruction": "Extract the repository name from the code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-C-gpl-2.0",
        "instruction": "Extract the file path from the code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-C-gpl-2.0",
        "instruction": "Extract the size of the code snippet."
    },
    {
        "input_fields": [
            "file_path"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-C#-all",
        "instruction": "Given a code file, extract the name of the repository it belongs to."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-C++-all",
        "instruction": "Given a code file, extract the size of the file."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-C++-all",
        "instruction": "Given a code file, extract the repository name."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "license"
        ],
        "task_name": "CodedotAI/code_clippy_github-C++-all",
        "instruction": "Given a code file, extract the license used in the file. Answers must be one of gpl-2.0, mit."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-C++-all",
        "instruction": "Given a code file, extract the file path."
    },
    {
        "input_fields": [
            "code_text",
            "repo_name"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-C++-gpl-2.0",
        "instruction": "Extract the file path from the code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-CSS-all",
        "instruction": "Extract the file path from the code file."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-CSS-all",
        "instruction": "Extract the repository name from the code file."
    },
    {
        "input_fields": [
            "file_path"
        ],
        "output_field": [
            "code_text"
        ],
        "task_name": "CodedotAI/code_clippy_github-CSS-all",
        "instruction": "Extract the code text from the code file."
    },
    {
        "input_fields": [
            "code_text",
            "repo_name"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-GO-all",
        "instruction": "Extract the file path from the code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-HTML-mit",
        "instruction": "Extract the file path from the given code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-HTML-mit",
        "instruction": "Extract the repository name from the given code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-HTML-mit",
        "instruction": "Extract the size of the code file from the given code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-HTML-apache-2.0",
        "instruction": "Extract the file path from the given code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-HTML-apache-2.0",
        "instruction": "Extract the repository name from the given code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-HTML-apache-2.0",
        "instruction": "Extract the size of the code file from the given code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-HTML-gpl-2.0",
        "instruction": "Extract the file path from the code text."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-HTML-gpl-2.0",
        "instruction": "Extract the repository name from the code text."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-HTML-gpl-2.0",
        "instruction": "Extract the size of the code text."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-HTML-gpl-3.0",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-HTML-gpl-3.0",
        "instruction": "Extract the repository name."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code_text"
        ],
        "task_name": "CodedotAI/code_clippy_github-HTML-gpl-3.0",
        "instruction": "Extract the license used in the code."
    },
    {
        "input_fields": [
            "file_path"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-HTML-gpl-3.0",
        "instruction": "Extract the file path of the code."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-HTML-mpl-2.0",
        "instruction": "Extract the repository name from the given code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-HTML-mpl-2.0",
        "instruction": "Extract the file path from the given code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-HTML-mpl-2.0",
        "instruction": "Extract the size of the code file from the given code snippet."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code_text"
        ],
        "task_name": "CodedotAI/code_clippy_github-Java-mit",
        "instruction": "Identify the license used for the code."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code_text"
        ],
        "task_name": "CodedotAI/code_clippy_github-Java-mit",
        "instruction": "Identify the size of the code."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-Java-apache-2.0",
        "instruction": "Given a code file, extract the repository name where the file is located."
    },
    {
        "input_fields": [
            "file_path"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-Java-lgpl-2.1",
        "instruction": "Given a code file, extract the name of the repository it belongs to."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "file_path",
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-gpl-3.0",
        "instruction": "Extract the file path and size of the code."
    },
    {
        "input_fields": [
            "code_text",
            "repo_name"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-bsd-3-clause",
        "instruction": "Extract the file path from the code snippet."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code_text"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-bsd-2-clause",
        "instruction": "Extract the license used in the code."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "repo_name",
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-bsd-2-clause",
        "instruction": "Extract the repository name and file path of the code."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-bsd-2-clause",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-unlicense",
        "instruction": "Given a code file, extract the size of the file. Answers must be one of 5408, 1860, 4521, 1797, 1284, 1359, 13423."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "code_text"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-unlicense",
        "instruction": "Given a code file, extract the repository name."
    },
    {
        "input_fields": [
            "file_path"
        ],
        "output_field": [
            "code_text"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-unlicense",
        "instruction": "Given a code file, extract the file path."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-mpl-2.0",
        "instruction": "Given a code snippet, identify the repository name."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-mpl-2.0",
        "instruction": "Given a code snippet, identify the file path."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-mpl-2.0",
        "instruction": "Given a code snippet, identify the size of the code."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-isc",
        "instruction": "Extract the file path of the code."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-isc",
        "instruction": "Extract the size of the code. Answers must be one of 2764, 1289, 1511, 5998, 2541, 5851."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "code_text"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-artistic-2.0",
        "instruction": "Identify the license used for the code snippet."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-artistic-2.0",
        "instruction": "Identify the repository name where the code snippet is located."
    },
    {
        "input_fields": [
            "size"
        ],
        "output_field": [
            "code_text"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-artistic-2.0",
        "instruction": "Identify the size of the code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-Matlab-all",
        "instruction": "Calculate the size of the code in bytes."
    },
    {
        "input_fields": [
            "repo_name",
            "file_path",
            "language"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-PHP-mit",
        "instruction": "Given a repository name, file path and language, return the size of the file."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-PHP-bsd-3-clause",
        "instruction": "Given a code file, extract the size of the file."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-PHP-bsd-3-clause",
        "instruction": "Given a code file, extract the repository name."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-PHP-bsd-3-clause",
        "instruction": "Given a code file, extract the file path."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-Python-apache-2.0",
        "instruction": "Given a code file, extract the repository name."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-Python-apache-2.0",
        "instruction": "Given a code file, extract the file path."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-Python-apache-2.0",
        "instruction": "Given a code file, extract the size of the file."
    },
    {
        "input_fields": [
            "repo_name"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-Python-gpl-3.0",
        "instruction": "Given a code file, extract the repository name where the file is located."
    },
    {
        "input_fields": [
            "file_path"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-Python-bsd-3-clause",
        "instruction": "Given a code file, return the size of the file."
    },
    {
        "input_fields": [
            "file_path"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-Python-bsd-3-clause",
        "instruction": "Given a code file, return the name of the repository it belongs to."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-Ruby-all",
        "instruction": "Given a code file, extract the size of the file."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-Ruby-all",
        "instruction": "Given a code file, extract the repository name."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-Ruby-all",
        "instruction": "Given a code file, extract the file path."
    },
    {
        "input_fields": [
            "file_path"
        ],
        "output_field": [
            "code_text"
        ],
        "task_name": "CodedotAI/code_clippy_github-Ruby-all",
        "instruction": "Given a code file, extract the code text."
    },
    {
        "input_fields": [
            "code_text",
            "file_path"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-Ruby-mit",
        "instruction": "Given a code file, extract the size of the file."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-Ruby-mit",
        "instruction": "Given a code file, extract the repository name."
    },
    {
        "input_fields": [
            "file_path"
        ],
        "output_field": [
            "code_text"
        ],
        "task_name": "CodedotAI/code_clippy_github-Ruby-mit",
        "instruction": "Given a code file, extract the code text."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-Rust-all",
        "instruction": "Extract the size of the code."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-Shell-all",
        "instruction": "Extract the file path from the code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-Shell-all",
        "instruction": "Extract the repository name from the code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "license"
        ],
        "task_name": "CodedotAI/code_clippy_github-Shell-all",
        "instruction": "Extract the license type from the code snippet. Answers must be one of gpl-3.0, bsd-3-clause, agpl-3.0, apache-2.0, lgpl-2.1, gpl-2.0, mit, mpl-2.0, cc0-1.0, bsd-2-clause."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-Shell-all",
        "instruction": "Extract the size of the code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-TypeScript-all",
        "instruction": "Given a code file, extract the size of the file."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "repo_name"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-gpl-2.0",
        "instruction": "Identify the repository name where the code is located."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "file_path"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-gpl-2.0",
        "instruction": "Identify the file path of the code snippet."
    },
    {
        "input_fields": [
            "code_text"
        ],
        "output_field": [
            "size"
        ],
        "task_name": "CodedotAI/code_clippy_github-JavaScript-gpl-2.0",
        "instruction": "Identify the size of the code snippet."
    },
    {
        "input_fields": [
            "article",
            "link_text"
        ],
        "output_field": [
            "is_same"
        ],
        "task_name": "Exr0n/wiki-entity-similarity-2018thresh5pairs",
        "instruction": "Given two articles and their corresponding link texts, predict whether they refer to the same entity. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "article",
            "link_text"
        ],
        "output_field": [
            "is_same"
        ],
        "task_name": "Exr0n/wiki-entity-similarity-2018thresh10pairs",
        "instruction": "Given an article and a link text, predict whether they refer to the same entity. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "article",
            "is_same"
        ],
        "output_field": [
            "link_text"
        ],
        "task_name": "Exr0n/wiki-entity-similarity-2018thresh10pairs",
        "instruction": "Given an article and a label indicating whether the link text refers to the same entity, predict the link text."
    },
    {
        "input_fields": [
            "article",
            "link_text"
        ],
        "output_field": [
            "is_same"
        ],
        "task_name": "Exr0n/wiki-entity-similarity-2018thresh20pairs",
        "instruction": "Given two articles and their corresponding link texts, predict whether the link texts refer to the same entity or not. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "article",
            "link_text"
        ],
        "output_field": [
            "is_same"
        ],
        "task_name": "Exr0n/wiki-entity-similarity-2018thresh20pairs",
        "instruction": "Given an article and a link text, predict whether they refer to the same entity or not. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "article"
        ],
        "output_field": [
            "is_same"
        ],
        "task_name": "Exr0n/wiki-entity-similarity-2018thresh20pairs",
        "instruction": "Given two articles, predict whether they are related or not based on their link texts. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "title"
        ],
        "task_name": "eurlex",
        "instruction": "Provide the title of the given regulation."
    },
    {
        "input_fields": [
            "context",
            "bias_type"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "stereoset-intersentence",
        "instruction": "Given a context and a bias type, the task is to identify the target word that is associated with the bias type."
    },
    {
        "input_fields": [
            "target",
            "bias_type"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "stereoset-intersentence",
        "instruction": "Given a target word and a bias type, the task is to generate a context that is associated with the bias type and contains the target word."
    },
    {
        "input_fields": [
            "target",
            "bias_type"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "stereoset-intrasentence",
        "instruction": "Given a profession, fill in the blank in the sentence to complete the sentence."
    },
    {
        "input_fields": [
            "context",
            "bias_type"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "stereoset-intrasentence",
        "instruction": "Given a sentence, fill in the blank with a race to complete the sentence."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "dynabench/qa-dynabench.qa.r1.all",
        "instruction": "Given a context, the task is to generate a question based on the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "dynabench/qa-dynabench.qa.r1.dbert",
        "instruction": "Given a context, the task is to generate a question."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-Python-snippet-level",
        "instruction": "Given a Python code snippet, provide a brief description of what the code does."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-Python-snippet-level",
        "instruction": "Given a brief description of a Python code, write the code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-Python-program-level",
        "instruction": "Given a Python function, write a brief description of what it does."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-Python-program-level",
        "instruction": "Given a Python function, write a test case that will fail if the function is incorrect."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-Python-program-level",
        "instruction": "Given a Python function, write a test case that will pass if the function is correct."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-C-snippet-level",
        "instruction": "Given a C code snippet, provide a brief description of what the code does."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-C-snippet-level",
        "instruction": "Given a C code snippet, identify any syntax errors or logical errors in the code."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-C-snippet-level",
        "instruction": "Given a brief description of a C program, write the corresponding code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-C-program-level",
        "instruction": "Given a C program, write a brief description of what the program does."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-C-program-level",
        "instruction": "Given a brief description of a C program, write the code for the program."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-Java-snippet-level",
        "instruction": "Given a Java code snippet, identify the purpose of the code."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-Java-snippet-level",
        "instruction": "Given a Java code snippet, extract a specific function or method from the code."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-Java-snippet-level",
        "instruction": "Given a description of a Java program, write the corresponding code snippet."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-Javascript-program-level",
        "instruction": "Given a function in Javascript, write a brief explanation of what it does."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-Javascript-program-level",
        "instruction": "Given a function in Javascript, write a test case to verify its functionality."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-Javascript-program-level",
        "instruction": "Given a number, check if it can be represented as the sum of two perfect cubes."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-Javascript-program-level",
        "instruction": "Given two arrays, write a function in Javascript to find the maximum prefix sum possible by merging the two arrays."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-PHP-snippet-level",
        "instruction": "Write a PHP function that checks if a given number in base r is even."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-PHP-snippet-level",
        "instruction": "Write a PHP code snippet that checks if the last digit of a given number in base r is even, if r is even."
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-PHP-snippet-level",
        "instruction": "What is the base of the number in the given PHP code snippet?"
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-PHP-snippet-level",
        "instruction": "What is the purpose of the if statement in the given PHP code snippet?"
    },
    {
        "input_fields": [
            "code"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-PHP-program-level",
        "instruction": "Given a PHP function, write a brief description of what it does."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "codeparrot/xlcost-text-to-code-PHP-program-level",
        "instruction": "Given a PHP function description, write the corresponding code."
    },
    {
        "input_fields": [
            "service_name"
        ],
        "output_field": [
            "description"
        ],
        "task_name": "schema_guided_dstc8-schema",
        "instruction": "Given a service name, provide a description of the service."
    },
    {
        "input_fields": [
            "mention",
            "sentence"
        ],
        "output_field": [
            "polarity"
        ],
        "task_name": "fhamborg/news_sentiment_newsmtsc-mt",
        "instruction": "Given a sentence and the mention of a person, predict the polarity of the sentiment expressed towards the person in the sentence. Answers must be one of -1, 0, 1."
    },
    {
        "input_fields": [
            "mention",
            "sentence"
        ],
        "output_field": [
            "from",
            "to"
        ],
        "task_name": "fhamborg/news_sentiment_newsmtsc-mt",
        "instruction": "Given a sentence and the mention of a person, extract the span of text that refers to the person."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "polarity"
        ],
        "task_name": "fhamborg/news_sentiment_newsmtsc-mt",
        "instruction": "Given a sentence, predict the sentiment polarity expressed in the sentence. Answers must be one of -1, 0, 1."
    },
    {
        "input_fields": [
            "sentence",
            "mention"
        ],
        "output_field": [
            "polarity"
        ],
        "task_name": "fhamborg/news_sentiment_newsmtsc-rw",
        "instruction": "Given a sentence and a mention, predict the polarity of the mention in the sentence. Answers must be one of -1, 0, 1."
    },
    {
        "input_fields": [
            "sentence",
            "mention"
        ],
        "output_field": [
            "from",
            "to"
        ],
        "task_name": "fhamborg/news_sentiment_newsmtsc-rw",
        "instruction": "Given a sentence and a mention, extract the span of the mention in the sentence."
    },
    {
        "input_fields": [
            "sentence",
            "polarity"
        ],
        "output_field": [
            "mention"
        ],
        "task_name": "fhamborg/news_sentiment_newsmtsc-rw",
        "instruction": "Given a polarity and a sentence, generate a mention with the given polarity in the sentence."
    },
    {
        "input_fields": [
            "answerstrings"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "proto_qa-proto_qa",
        "instruction": "Given a list of answers, provide the question that best fits the answers."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "totalcount"
        ],
        "task_name": "proto_qa-proto_qa_cs",
        "instruction": "Given a question, provide the number of possible answers."
    },
    {
        "input_fields": [
            "answerstrings"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "proto_qa-proto_qa_cs",
        "instruction": "Given a list of possible answers, provide a question that could elicit these answers."
    },
    {
        "input_fields": [
            "assessments"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "proto_qa-proto_qa_cs_assessments",
        "instruction": "Given a set of assessments, predict the question."
    },
    {
        "input_fields": [
            "references"
        ],
        "output_field": [
            "title"
        ],
        "task_name": "GEM/wiki_cat_sum-animal",
        "instruction": "Given a region and a list of moth species, identify the species that are found in that region."
    },
    {
        "input_fields": [
            "title"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/wiki_cat_sum-company",
        "instruction": "Given a company name, provide a brief description of the company and its products."
    },
    {
        "input_fields": [
            "paragraphs"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/wiki_cat_sum-company",
        "instruction": "Given a paragraph, extract the key concepts and generate a summary of the paragraph."
    },
    {
        "input_fields": [
            "title"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/wiki_cat_sum-film",
        "instruction": "Given a film title, provide the release date and the production company."
    },
    {
        "input_fields": [
            "title"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/wiki_cat_sum-film",
        "instruction": "Given a film title, provide the list of actors who starred in the film."
    },
    {
        "input_fields": [
            "title",
            "paragraphs"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/wiki_cat_sum-film",
        "instruction": "Given a film title, provide a brief summary of the plot."
    },
    {
        "input_fields": [
            "doc_html_ts"
        ],
        "output_field": [
            "doc_text"
        ],
        "task_name": "multidoc2dial-document_domain",
        "instruction": "Extract the text from the document."
    },
    {
        "input_fields": [
            "doc_html_ts"
        ],
        "output_field": [
            "doc_html_raw"
        ],
        "task_name": "multidoc2dial-document_domain",
        "instruction": "Extract the raw HTML from the document."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "utterance"
        ],
        "task_name": "multidoc2dial-multidoc2dial",
        "instruction": "Given a DMV question, provide an appropriate response."
    },
    {
        "input_fields": [
            "domain"
        ],
        "output_field": [
            "question",
            "utterance"
        ],
        "task_name": "multidoc2dial-multidoc2dial",
        "instruction": "Given a DMV domain, provide a list of common questions and their appropriate responses."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_math_jsonl-titlebody_upvoted_downvoted_answer",
        "instruction": "Given a math problem, provide a step-by-step solution."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "downvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_math_jsonl-titlebody_upvoted_downvoted_answer",
        "instruction": "Given a math problem, provide a different solution."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_math_jsonl-titlebody_upvoted_downvoted_answer",
        "instruction": "Given a set of points in 3D space, determine if they are collinear."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_math_jsonl-titlebody_upvoted_downvoted_answer",
        "instruction": "Given a set of points in 3D space, provide a formula to determine if they are collinear."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_math_jsonl-titlebody_answer",
        "instruction": "Calculate the limit of a given function."
    },
    {
        "input_fields": [
            "title_body"
        ],
        "output_field": [
            "upvoted_answer"
        ],
        "task_name": "flax-sentence-embeddings/stackexchange_math_jsonl-titlebody_answer",
        "instruction": "Prove a theorem related to linear algebra."
    },
    {
        "input_fields": [
            "masked_uri",
            "facts"
        ],
        "output_field": [
            "masked_type"
        ],
        "task_name": "ekinakyurek/ftrace-abstracts",
        "instruction": "Given a masked entity and a set of facts, predict the type of the entity (subject or object). Answers must be one of subject, object."
    },
    {
        "input_fields": [
            "facts",
            "masked_type"
        ],
        "output_field": [
            "masked_uri"
        ],
        "task_name": "ekinakyurek/ftrace-abstracts",
        "instruction": "Given a set of facts and a masked entity, predict the entity."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "masked_uri"
        ],
        "task_name": "ekinakyurek/ftrace-abstracts",
        "instruction": "Given a sentence with a masked entity, predict the entity."
    },
    {
        "input_fields": [
            "inputs_pretokenized"
        ],
        "output_field": [
            "targets_pretokenized"
        ],
        "task_name": "ekinakyurek/ftrace-abstracts",
        "instruction": "Given a sentence with a masked entity, predict the missing word."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "obj_uri"
        ],
        "output_field": [
            "sub_surface"
        ],
        "task_name": "ekinakyurek/ftrace-queries",
        "instruction": "Given a legal term and its corresponding object, provide the subject of the term."
    },
    {
        "input_fields": [
            "inputs_pretokenized",
            "sub_uri"
        ],
        "output_field": [
            "obj_surface"
        ],
        "task_name": "ekinakyurek/ftrace-queries",
        "instruction": "Given a legal term and its corresponding subject, provide the object of the term."
    },
    {
        "input_fields": [
            "sub_surface",
            "obj_surface"
        ],
        "output_field": [
            "inputs_pretokenized"
        ],
        "task_name": "ekinakyurek/ftrace-queries",
        "instruction": "Given a subject and object, provide the corresponding legal term."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "act_tag"
        ],
        "task_name": "swda",
        "instruction": "Given a conversation, predict the act tag of the conversation."
    },
    {
        "input_fields": [
            "from_caller_education"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "swda",
        "instruction": "Given a conversation, predict the education level of the caller."
    },
    {
        "input_fields": [
            "context",
            "hypothesis",
            "probability_word",
            "hypothesis_assertion"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "sileod/probability_words_nli-reasoning_1hop",
        "instruction": "Given a context and a hypothesis, determine whether the probability word used in the context supports or contradicts the hypothesis. Answers must be one of valid, invalid."
    },
    {
        "input_fields": [
            "context",
            "hypothesis",
            "probability_word",
            "distractor"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "sileod/probability_words_nli-reasoning_1hop",
        "instruction": "Given a context and a hypothesis, replace the probability word with a distractor word and determine whether the resulting hypothesis is valid or invalid. Answers must be one of valid, invalid."
    },
    {
        "input_fields": [
            "context",
            "hypothesis",
            "probability_word"
        ],
        "output_field": [
            "probability"
        ],
        "task_name": "sileod/probability_words_nli-reasoning_1hop",
        "instruction": "Given a context and a hypothesis, determine the probability of the hypothesis being true based on the probability word used in the context."
    },
    {
        "input_fields": [
            "context",
            "hypothesis",
            "probability_word"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "sileod/probability_words_nli-reasoning_2hop",
        "instruction": "Given a context and a hypothesis, determine whether the probability word used in the hypothesis is valid or invalid based on the context. Answers must be one of valid, invalid."
    },
    {
        "input_fields": [
            "context",
            "hypothesis",
            "probability_word"
        ],
        "output_field": [
            "distractor"
        ],
        "task_name": "sileod/probability_words_nli-reasoning_2hop",
        "instruction": "Given a context and a hypothesis, provide a distractor word that is opposite in meaning to the probability word used in the hypothesis."
    },
    {
        "input_fields": [
            "context",
            "hypothesis"
        ],
        "output_field": [
            "hypothesis_assertion"
        ],
        "task_name": "sileod/probability_words_nli-reasoning_2hop",
        "instruction": "Given a context and a hypothesis, extract the assertion made in the hypothesis."
    },
    {
        "input_fields": [
            "context",
            "hypothesis",
            "probability_word",
            "probability"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "sileod/probability_words_nli-usnli",
        "instruction": "Given a context and a hypothesis, determine if the probability word used in the hypothesis matches the probability score given in the data. Answers must be one of valid, invalid."
    },
    {
        "input_fields": [
            "context",
            "hypothesis",
            "probability_word"
        ],
        "output_field": [
            "distractor"
        ],
        "task_name": "sileod/probability_words_nli-usnli",
        "instruction": "Given a context and a hypothesis, choose the most appropriate distractor word to replace the probability word in the hypothesis."
    },
    {
        "input_fields": [
            "context",
            "hypothesis",
            "hypothesis_assertion",
            "probability"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "sileod/probability_words_nli-usnli",
        "instruction": "Given a context and a hypothesis, determine if the hypothesis assertion is valid based on the probability score given in the data. Answers must be one of valid, invalid."
    },
    {
        "input_fields": [
            "dialogue"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "deal_or_no_dialog-dialogues",
        "instruction": "Given a dialogue and the output of the negotiation, predict the item that each party received."
    },
    {
        "input_fields": [
            "func1",
            "func2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "code_x_glue_cc_clone_detection_big_clone_bench",
        "instruction": "Given two functions, determine whether they have the same label. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "review_rating"
        ],
        "output_field": [
            "claim_text"
        ],
        "task_name": "datacommons_factcheck-fctchk_politifact_wapo",
        "instruction": "Identify the rating given by the reviewer for the claim."
    },
    {
        "input_fields": [
            "claim_text"
        ],
        "output_field": [
            "claim_author_name"
        ],
        "task_name": "datacommons_factcheck-fctchk_politifact_wapo",
        "instruction": "Identify the author of the claim."
    },
    {
        "input_fields": [
            "review_url"
        ],
        "output_field": [
            "review_date"
        ],
        "task_name": "datacommons_factcheck-fctchk_politifact_wapo",
        "instruction": "Identify the date when the claim was reviewed."
    },
    {
        "input_fields": [
            "claim_text"
        ],
        "output_field": [
            "claim_author_name"
        ],
        "task_name": "datacommons_factcheck-weekly_standard",
        "instruction": "Identify the source of the claim."
    },
    {
        "input_fields": [
            "review_url"
        ],
        "output_field": [
            "review_date"
        ],
        "task_name": "datacommons_factcheck-weekly_standard",
        "instruction": "Provide the date of the review."
    },
    {
        "input_fields": [
            "rel",
            "arg1",
            "arg2"
        ],
        "output_field": [
            "support"
        ],
        "task_name": "ascent_kb-canonical",
        "instruction": "Given a relation and two arguments, predict the support score indicating the likelihood of the relation holding between the two arguments."
    },
    {
        "input_fields": [
            "arg1",
            "rel"
        ],
        "output_field": [
            "arg2"
        ],
        "task_name": "ascent_kb-canonical",
        "instruction": "Given an argument and a relation, predict the other argument that holds the relation with the given argument."
    },
    {
        "input_fields": [
            "subject",
            "predicate"
        ],
        "output_field": [
            "object"
        ],
        "task_name": "ascent_kb-open",
        "instruction": "Given a subject and predicate, provide the object that completes the sentence."
    },
    {
        "input_fields": [
            "subject",
            "object"
        ],
        "output_field": [
            "predicate"
        ],
        "task_name": "ascent_kb-open",
        "instruction": "Given a subject and object, provide the predicate that completes the sentence."
    },
    {
        "input_fields": [
            "subject",
            "object"
        ],
        "output_field": [
            "support"
        ],
        "task_name": "ascent_kb-open",
        "instruction": "Given a subject and object, provide the support for the relationship between them."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "publication_year"
        ],
        "task_name": "eu_regulatory_ir-eu2uk",
        "instruction": "Extract the publication year from the text. Answers must be one of 1981, 1978, 1984, 1983, 1982, 1980, 1977, 1979."
    },
    {
        "input_fields": [
            "permalink"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/ten-million-reddit-answers-posts",
        "instruction": "Identify the score of the post."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "med_hop-masked",
        "instruction": "Fill in the blank with the correct drug name that interacts with DB00773."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "med_hop-masked",
        "instruction": "Fill in the blank with the correct drug name that interacts with DB09079."
    },
    {
        "input_fields": [
            "supports"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "med_hop-masked",
        "instruction": "Provide the drug name that interacts with the given DB ID."
    },
    {
        "input_fields": [
            "system"
        ],
        "output_field": [
            "summary"
        ],
        "task_name": "xsum_factuality-xsum_faithfulness",
        "instruction": "Identify the system used to generate the summary."
    },
    {
        "input_fields": [
            "hallucination_type"
        ],
        "output_field": [
            "summary"
        ],
        "task_name": "xsum_factuality-xsum_faithfulness",
        "instruction": "Determine the type of hallucination in the summary."
    },
    {
        "input_fields": [
            "hallucinated_span_start",
            "hallucinated_span_end"
        ],
        "output_field": [
            "summary"
        ],
        "task_name": "xsum_factuality-xsum_faithfulness",
        "instruction": "Locate the span of text that was hallucinated in the summary."
    },
    {
        "input_fields": [
            "body"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/one-year-of-r-india-comments",
        "instruction": "Identify the score of a comment."
    },
    {
        "input_fields": [
            "permalink"
        ],
        "output_field": [
            "domain"
        ],
        "task_name": "SocialGrep/reddit-wallstreetbets-aug-2021-posts",
        "instruction": "Identify the domain of the post."
    },
    {
        "input_fields": [
            "permalink"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/reddit-wallstreetbets-aug-2021-posts",
        "instruction": "Identify the score of the post."
    },
    {
        "input_fields": [
            "permalink"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/reddit-wallstreetbets-aug-2021-comments",
        "instruction": "Identify the score of the comment."
    },
    {
        "input_fields": [
            "permalink"
        ],
        "output_field": [
            "created_utc"
        ],
        "task_name": "SocialGrep/reddit-wallstreetbets-aug-2021-comments",
        "instruction": "Identify the time the comment was created."
    },
    {
        "input_fields": [
            "permalink"
        ],
        "output_field": [
            "body"
        ],
        "task_name": "SocialGrep/reddit-wallstreetbets-aug-2021-comments",
        "instruction": "Identify the content of the comment."
    },
    {
        "input_fields": [
            "body"
        ],
        "output_field": [
            "permalink"
        ],
        "task_name": "SocialGrep/reddit-wallstreetbets-aug-2021-comments",
        "instruction": "Identify the permalink of the comment."
    },
    {
        "input_fields": [
            "body"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/top-american-universities-on-reddit-comments",
        "instruction": "Identify the score of the comment."
    },
    {
        "input_fields": [
            "bytecode"
        ],
        "output_field": [
            "address"
        ],
        "task_name": "mwritescode/slither-audited-smart-contracts-all-multilabel",
        "instruction": "Given the bytecode of a smart contract, predict the address of the contract on the Ethereum network."
    },
    {
        "input_fields": [
            "bytecode"
        ],
        "output_field": [
            "address"
        ],
        "task_name": "mwritescode/slither-audited-smart-contracts-small-multilabel",
        "instruction": "Given the bytecode of a smart contract, extract the address of the contract."
    },
    {
        "input_fields": [
            "tokens"
        ],
        "output_field": [
            "start_index",
            "end_index"
        ],
        "task_name": "numeric_fused_head-identification",
        "instruction": "Identify the numeric value in the given sentence."
    },
    {
        "input_fields": [
            "tokens",
            "start_index",
            "end_index"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "numeric_fused_head-identification",
        "instruction": "Determine whether the identified numeric value is positive or negative. Answers must be one of pos, neg."
    },
    {
        "input_fields": [
            "bytecode"
        ],
        "output_field": [
            "address"
        ],
        "task_name": "mwritescode/slither-audited-smart-contracts-big-multilabel",
        "instruction": "Given the bytecode of a smart contract, predict the address of the contract on the Ethereum network."
    },
    {
        "input_fields": [
            "facts"
        ],
        "output_field": [
            "casename"
        ],
        "task_name": "lbox/lbox_open-statute_classification",
        "instruction": "Given a set of facts, the task is to predict the case name. Answers must be one of \uac15\uc81c\ucd94\ud589, \uacf5\ubb34\uc9d1\ud589\ubc29\ud574, \uacf5\ubb34\uc9d1\ud589\ubc29\ud574, \uc0c1\ud574."
    },
    {
        "input_fields": [
            "prompt"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/schema_guided_dialog",
        "instruction": "Given a prompt, the task is to ask a follow-up question to get more information from the user."
    },
    {
        "input_fields": [
            "prompt",
            "target"
        ],
        "output_field": [
            "linearized_input"
        ],
        "task_name": "GEM/schema_guided_dialog",
        "instruction": "Given a prompt and a target question, the task is to provide a possible response to the target question based on the prompt."
    },
    {
        "input_fields": [
            "meaning_representation"
        ],
        "output_field": [
            "target",
            "references"
        ],
        "task_name": "GEM/viggo",
        "instruction": "Extract the name, release year, ESRB rating, genres, platforms, and availability information of a video game from its meaning representation."
    },
    {
        "input_fields": [
            "target"
        ],
        "output_field": [
            "meaning_representation"
        ],
        "task_name": "GEM/viggo",
        "instruction": "Given the name, release year, ESRB rating, genres, platforms, and availability information of a video game, generate its meaning representation."
    },
    {
        "input_fields": [
            "title",
            "abstract"
        ],
        "output_field": [
            "date"
        ],
        "task_name": "taln-ls2n/kptimes",
        "instruction": "Given an article, the task is to predict the date of publication."
    },
    {
        "input_fields": [
            "situation",
            "moral_action"
        ],
        "output_field": [
            "moral_consequence"
        ],
        "task_name": "demelin/moral_stories-full",
        "instruction": "Given a situation and a moral action, predict the moral consequence."
    },
    {
        "input_fields": [
            "situation",
            "immoral_action"
        ],
        "output_field": [
            "immoral_consequence"
        ],
        "task_name": "demelin/moral_stories-full",
        "instruction": "Given a situation and an immoral action, predict the immoral consequence."
    },
    {
        "input_fields": [
            "moral_action",
            "immoral_action",
            "situation"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action-lexical_bias",
        "instruction": "Given a moral action and an immoral action, predict which one is more likely to be chosen by a person in a given situation. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action-lexical_bias",
        "instruction": "Given a moral action and its consequence, predict whether the consequence is positive or negative. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "immoral_action",
            "immoral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action-lexical_bias",
        "instruction": "Given an immoral action and its consequence, predict whether the consequence is positive or negative. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "moral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action-norm_distance",
        "instruction": "Predict the label based on the moral action taken by the protagonist. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "immoral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action-norm_distance",
        "instruction": "Predict the label based on the immoral action taken by the protagonist. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "moral_action",
            "immoral_action"
        ],
        "task_name": "demelin/moral_stories-cls-action-norm_distance",
        "instruction": "Given a label, predict the moral or immoral action taken by the protagonist."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context-lexical_bias",
        "instruction": "Given a situation and intention, determine whether the action taken is moral or immoral. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "norm"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context-minimal_pairs",
        "instruction": "Given a situation and a norm, predict whether the person will perform a moral action or not. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "norm"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context-minimal_pairs",
        "instruction": "Given a situation and a norm, predict whether the person will perform an immoral action or not. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "norm"
        ],
        "output_field": [
            "intention"
        ],
        "task_name": "demelin/moral_stories-cls-action+context-minimal_pairs",
        "instruction": "Given a situation and a norm, predict the intention of the person."
    },
    {
        "input_fields": [
            "norm",
            "situation"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context-norm_distance",
        "instruction": "Given a norm and a situation, predict whether the intention of the person is to help their family finances or not. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "norm",
            "intention"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context-norm_distance",
        "instruction": "Given a norm and an intention, predict whether the person takes a moral or immoral action. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context-norm_distance",
        "instruction": "Given a moral action and a moral consequence, predict whether the action is moral or not. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-lexical_bias",
        "instruction": "Given a situation and intention, identify the moral action taken by the protagonist."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "immoral_action"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-lexical_bias",
        "instruction": "Given a situation and intention, identify the immoral action taken by the protagonist."
    },
    {
        "input_fields": [
            "moral_action"
        ],
        "output_field": [
            "moral_consequence"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-lexical_bias",
        "instruction": "Given a moral action, identify the moral consequence."
    },
    {
        "input_fields": [
            "immoral_action"
        ],
        "output_field": [
            "immoral_consequence"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-lexical_bias",
        "instruction": "Given an immoral action, identify the immoral consequence."
    },
    {
        "input_fields": [
            "moral_action",
            "immoral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-lexical_bias",
        "instruction": "Identify whether the given action is moral or immoral. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-minimal_pairs",
        "instruction": "Given a situation and intention, predict whether the moral action will be taken or not. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-minimal_pairs",
        "instruction": "Given a moral action and its consequence, predict the label. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "immoral_action",
            "immoral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-minimal_pairs",
        "instruction": "Given an immoral action and its consequence, predict the label. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-norm_distance",
        "instruction": "Given a situation and an intention, predict whether the action taken is moral or immoral. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-norm_distance",
        "instruction": "Given a moral action and its consequence, predict whether the action is moral or immoral. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "immoral_action",
            "immoral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+context+consequence-norm_distance",
        "instruction": "Given an immoral action and its consequence, predict whether the action is moral or immoral. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "norm",
            "moral_action",
            "immoral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+norm-lexical_bias",
        "instruction": "Given a norm, predict whether the action described is moral or immoral. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "norm",
            "moral_action",
            "moral_consequence",
            "immoral_action",
            "immoral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+norm-lexical_bias",
        "instruction": "Given a norm and an action, predict whether the consequence is moral or immoral. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "moral_action",
            "immoral_action"
        ],
        "task_name": "demelin/moral_stories-cls-action+norm-lexical_bias",
        "instruction": "Given a label, provide the corresponding action."
    },
    {
        "input_fields": [
            "norm",
            "moral_action",
            "immoral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+norm-minimal_pairs",
        "instruction": "Given a norm, predict whether the scenario described is moral or immoral. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "moral_action",
            "immoral_action",
            "label"
        ],
        "output_field": [
            "norm"
        ],
        "task_name": "demelin/moral_stories-cls-action+norm-minimal_pairs",
        "instruction": "Given a scenario and its label, identify the norm that applies."
    },
    {
        "input_fields": [
            "norm",
            "moral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+norm-norm_distance",
        "instruction": "Given a norm, predict whether the moral action taken is good or bad. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "norm",
            "immoral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+norm-norm_distance",
        "instruction": "Given a norm, predict whether the immoral action taken is good or bad. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "moral_action"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-action+norm-norm_distance",
        "instruction": "Given a moral action, predict whether it is good or bad. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action-lexical_bias",
        "instruction": "Given a moral action and its consequence, predict the label (0 or 1) indicating whether the consequence is positive or negative. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "immoral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action-lexical_bias",
        "instruction": "Given an immoral consequence, predict the label (0 or 1) indicating whether the action leading to the consequence is negative or positive. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "moral_action"
        ],
        "output_field": [
            "moral_consequence"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action-lexical_bias",
        "instruction": "Given a moral action, predict the moral consequence."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action-minimal_pairs",
        "instruction": "Given a moral action and its consequence, predict the label (moral or immoral). Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "immoral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action-minimal_pairs",
        "instruction": "Given an immoral consequence, predict the label (moral or immoral). Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "moral_action"
        ],
        "output_field": [
            "moral_consequence"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action-minimal_pairs",
        "instruction": "Given a moral action, predict its consequence."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action-norm_distance",
        "instruction": "Given a moral action and its consequence, predict whether it is a moral or immoral action. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "immoral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action-norm_distance",
        "instruction": "Given an immoral consequence, predict whether it is a result of a moral or immoral action. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action-norm_distance",
        "instruction": "Given a situation and an intention, predict whether the action is moral or immoral. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "moral_action"
        ],
        "output_field": [
            "moral_consequence"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action+context-lexical_bias",
        "instruction": "Given a situation and an action, predict the moral consequence of the action."
    },
    {
        "input_fields": [
            "situation",
            "immoral_action"
        ],
        "output_field": [
            "immoral_consequence"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action+context-lexical_bias",
        "instruction": "Given a situation and an action, predict the immoral consequence of the action."
    },
    {
        "input_fields": [
            "situation",
            "moral_consequence"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action+context-lexical_bias",
        "instruction": "Given a moral consequence, predict the action that led to it."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "moral_action"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action+context-minimal_pairs",
        "instruction": "Given a situation and an intention, identify the moral action taken by the person."
    },
    {
        "input_fields": [
            "situation",
            "intention"
        ],
        "output_field": [
            "immoral_consequence"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action+context-minimal_pairs",
        "instruction": "Given a situation and an intention, identify the immoral consequence of the action taken by the person."
    },
    {
        "input_fields": [
            "moral_action"
        ],
        "output_field": [
            "moral_consequence"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action+context-minimal_pairs",
        "instruction": "Given a moral action, identify the moral consequence."
    },
    {
        "input_fields": [
            "immoral_consequence"
        ],
        "output_field": [
            "immoral_action"
        ],
        "task_name": "demelin/moral_stories-cls-consequence+action+context-minimal_pairs",
        "instruction": "Given an immoral consequence, identify the immoral action."
    },
    {
        "input_fields": [
            "norm",
            "situation"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-gen-action$context-norm_distance",
        "instruction": "Given a norm and a situation, predict whether the intention of the person is to help their family finances or not. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "norm",
            "intention"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-gen-action$context-norm_distance",
        "instruction": "Given a norm and an intention, predict whether the person takes a moral or immoral action. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-gen-action$context-norm_distance",
        "instruction": "Given a moral action and its consequence, predict whether it is a moral or immoral action. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "situation",
            "norm",
            "intention"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-gen-action$context+consequence-norm_distance",
        "instruction": "Given a situation and a moral norm, determine whether the intention of the person is moral or not. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-gen-action$context+consequence-norm_distance",
        "instruction": "Given a moral action and its consequence, determine whether the action is moral or not. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "immoral_action",
            "immoral_consequence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "demelin/moral_stories-gen-action$context+consequence-norm_distance",
        "instruction": "Given an immoral action and its consequence, determine whether the action is immoral or not. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "norm"
        ],
        "output_field": [
            "moral_action",
            "immoral_action"
        ],
        "task_name": "demelin/moral_stories-gen-norm$actions-norm_distance",
        "instruction": "Given a norm, generate a moral or immoral action that follows the norm or violates the norm."
    },
    {
        "input_fields": [
            "norm",
            "situation"
        ],
        "output_field": [
            "moral_action",
            "immoral_action"
        ],
        "task_name": "demelin/moral_stories-gen-norm$actions+context-norm_distance",
        "instruction": "Given a norm and a situation, predict the moral or immoral action taken by the person."
    },
    {
        "input_fields": [
            "moral_action",
            "moral_consequence",
            "immoral_action",
            "immoral_consequence"
        ],
        "output_field": [
            "intention"
        ],
        "task_name": "demelin/moral_stories-gen-norm$actions+context-norm_distance",
        "instruction": "Given a moral or immoral action and its consequence, predict the intention of the person."
    },
    {
        "input_fields": [
            "norm",
            "situation"
        ],
        "output_field": [
            "intention"
        ],
        "task_name": "demelin/moral_stories-gen-norm$actions+context+consequences-norm_distance",
        "instruction": "Given a norm and a situation, generate an intention."
    },
    {
        "input_fields": [
            "intention",
            "moral_consequence",
            "immoral_consequence"
        ],
        "output_field": [
            "moral_action",
            "immoral_action"
        ],
        "task_name": "demelin/moral_stories-gen-norm$actions+context+consequences-norm_distance",
        "instruction": "Given an intention and a consequence, generate a moral or immoral action."
    },
    {
        "input_fields": [
            "instructions"
        ],
        "output_field": [
            "scenario"
        ],
        "task_name": "taskmaster3",
        "instruction": "Fill in the missing turns in the conversation to purchase movie tickets."
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "code"
        ],
        "task_name": "code_x_glue_cc_clone_detection_poj104",
        "instruction": "Given a label (0 or 1), find the corresponding code snippet for clone detection."
    },
    {
        "input_fields": [
            "domain"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "the_pile_stack_exchange",
        "instruction": "Given a domain, provide a list of questions related to that domain."
    },
    {
        "input_fields": [
            "func"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "code_x_glue_cc_defect_detection",
        "instruction": "Given a function, determine if it has any syntax errors. Answers must be one of True, False."
    },
    {
        "input_fields": [
            "summaries"
        ],
        "output_field": [
            "review_sents"
        ],
        "task_name": "opinosis",
        "instruction": "Given a summary, generate a review sentence that matches the summary."
    },
    {
        "input_fields": [
            "respondentWorkerId",
            "initiatorWorkerId"
        ],
        "output_field": [
            "conversationId"
        ],
        "task_name": "re_dial",
        "instruction": "Identify the conversation ID of a given respondent and initiator worker."
    },
    {
        "input_fields": [
            "conversationId"
        ],
        "output_field": [
            "respondentWorkerId"
        ],
        "task_name": "re_dial",
        "instruction": "Identify the respondent worker ID of a given conversation."
    },
    {
        "input_fields": [
            "conversationId"
        ],
        "output_field": [
            "initiatorWorkerId"
        ],
        "task_name": "re_dial",
        "instruction": "Identify the initiator worker ID of a given conversation."
    },
    {
        "input_fields": [
            "story",
            "selected_sentence"
        ],
        "output_field": [
            "1_specificStructured"
        ],
        "task_name": "glucose",
        "instruction": "Given a story and a specific sentence, identify the corresponding structured representation."
    },
    {
        "input_fields": [
            "1_specificStructured"
        ],
        "output_field": [
            "1_specificNL"
        ],
        "task_name": "glucose",
        "instruction": "Given a structured representation, generate a natural language sentence."
    },
    {
        "input_fields": [
            "story",
            "selected_sentence"
        ],
        "output_field": [
            "worker_quality_assessment"
        ],
        "task_name": "glucose",
        "instruction": "Given a story and a specific sentence, identify the corresponding worker quality assessment. Answers must be one of 2, 3, 1."
    },
    {
        "input_fields": [
            "title",
            "context"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/squad_v2",
        "instruction": "Given a country name and its official name, construct a sentence that includes both."
    },
    {
        "input_fields": [
            "target"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "GEM/squad_v2",
        "instruction": "Given a sentence that includes a country name and its official name, extract the official name."
    },
    {
        "input_fields": [
            "target"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "GEM/squad_v2",
        "instruction": "Given a sentence that includes a country name and its location, extract the location."
    },
    {
        "input_fields": [
            "title",
            "context"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/squad_v2",
        "instruction": "Given a sentence that includes a country name and its location, construct a sentence that includes both."
    },
    {
        "input_fields": [
            "disfluent question"
        ],
        "output_field": [
            "original question"
        ],
        "task_name": "disfl_qa",
        "instruction": "Given a disfluent question, generate the original question."
    },
    {
        "input_fields": [
            "context",
            "original question"
        ],
        "output_field": [
            "title"
        ],
        "task_name": "disfl_qa",
        "instruction": "Given a context and a question, extract the relevant title."
    },
    {
        "input_fields": [
            "license"
        ],
        "output_field": [
            "filepath"
        ],
        "task_name": "eth_py150_open",
        "instruction": "Identify the file path of a given license type."
    },
    {
        "input_fields": [
            "claim",
            "num_hops"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "hover",
        "instruction": "Given a claim and the number of hops, predict the label (SUPPORTED or REFUTED) for the claim. Answers must be one of SUPPORTED, NOT_SUPPORTED."
    },
    {
        "input_fields": [
            "claim",
            "label"
        ],
        "output_field": [
            "num_hops"
        ],
        "task_name": "hover",
        "instruction": "Given a claim and the label, predict the number of hops required to support or refute the claim. Answers must be one of 2, 3, 4."
    },
    {
        "input_fields": [
            "QueryBody"
        ],
        "output_field": [
            "Title",
            "Description"
        ],
        "task_name": "sede",
        "instruction": "Find the most controversial posts on the site based on the ratio of upvotes to downvotes."
    },
    {
        "input_fields": [
            "QuerySetId"
        ],
        "output_field": [
            "CreationDate"
        ],
        "task_name": "sede",
        "instruction": "Find the creation date of a query set."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "multi_nli_mismatch",
        "instruction": "Given a premise and a hypothesis, determine whether the hypothesis is entailed by the premise, contradicted by the premise, or neither. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "multi_nli_mismatch",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise if the label is \"entailment\", contradicted by the premise if the label is \"contradiction\", or neither if the label is \"neutral\"."
    },
    {
        "input_fields": [
            "source"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/cochrane-simplification",
        "instruction": "Summarize the findings of the study and the quality of the evidence."
    },
    {
        "input_fields": [
            "paper_title",
            "paper_abstract"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "GEM/SciDuet",
        "instruction": "Given a paper title and abstract, generate a coherent target sentence that summarizes the main idea of the paper."
    },
    {
        "input_fields": [
            "name"
        ],
        "output_field": [
            "origin"
        ],
        "task_name": "TRoboto/names",
        "instruction": "Given a name, provide its origin."
    },
    {
        "input_fields": [
            "name"
        ],
        "output_field": [
            "description"
        ],
        "task_name": "TRoboto/names",
        "instruction": "Given a name, provide its description."
    },
    {
        "input_fields": [
            "origin"
        ],
        "output_field": [
            "name"
        ],
        "task_name": "TRoboto/names",
        "instruction": "Given an origin, provide a list of names with that origin."
    },
    {
        "input_fields": [
            "claim"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "ctu-aic/csfever",
        "instruction": "Given a claim, predict the label (SUPPORTS, REFUTES, NOT ENOUGH INFO) for the claim. Answers must be one of SUPPORTS, NOT ENOUGH INFO, REFUTES."
    },
    {
        "input_fields": [
            "sequence"
        ],
        "output_field": [
            "FPV",
            "IDV",
            "NFV",
            "SQV"
        ],
        "task_name": "damlab/HIV_PI",
        "instruction": "Given a sequence, predict whether it is resistant to FPV (Fosamprenavir), IDV (Indinavir), NFV (Nelfinavir), or SQV (Saquinavir). Answers must be one of True, False."
    },
    {
        "input_fields": [
            "sequence"
        ],
        "output_field": [
            "fold"
        ],
        "task_name": "damlab/HIV_PI",
        "instruction": "Given a sequence, predict the fold it belongs to. Answers must be one of 0, 3, 4, 2, 1."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "ngram"
        ],
        "task_name": "corypaik/coda",
        "instruction": "Fill in the blank with the correct word to complete the sentence."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "display_name"
        ],
        "task_name": "corypaik/coda",
        "instruction": "Provide the display name for the given text."
    },
    {
        "input_fields": [
            "phrase1",
            "phrase2",
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "PiC/phrase_similarity",
        "instruction": "Determine if two phrases are similar or dissimilar based on their context in a sentence. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "phrase1",
            "sentence1"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "PiC/phrase_similarity",
        "instruction": "Given a sentence and a phrase, determine if the phrase appears in the sentence. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "phrase1",
            "phrase2",
            "sentence1"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "PiC/phrase_similarity",
        "instruction": "Given a sentence, determine which of two phrases appears in the sentence. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "difficulty"
        ],
        "output_field": [
            "name"
        ],
        "task_name": "deepmind/code_contests",
        "instruction": "Given a difficulty level, return all problems with that difficulty level."
    },
    {
        "input_fields": [
            "cf_tags"
        ],
        "output_field": [
            "name"
        ],
        "task_name": "deepmind/code_contests",
        "instruction": "Given a set of tags, return all problems with those tags."
    },
    {
        "input_fields": [
            "output"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "tau/sled-summ_screen_fd",
        "instruction": "Given a plot summary of an episode, can you name the episode based on the title of a song?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "tau/sled-summ_screen_fd",
        "instruction": "Given a transcript of a TV show, can you summarize the plot of the episode?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "tau/sled-qasper",
        "instruction": "Given a sentence, predict the polarity of the affective event represented by the sentence."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "input_prefix"
        ],
        "task_name": "tau/sled-qasper",
        "instruction": "Given a sentence, provide a prefix question that can be used to extract a specific piece of information from the sentence."
    },
    {
        "input_fields": [
            "output"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "tau/sled-qasper",
        "instruction": "Given a seed lexicon, predict the polarity score of an affective event."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "tau/sled-qmsum",
        "instruction": "Summarize the given text."
    },
    {
        "input_fields": [
            "input_prefix",
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "tau/sled-qmsum",
        "instruction": "Given a question, answer it based on the given text."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "tau/sled-narrative_qa",
        "instruction": "Given a chapter title, provide a brief summary of the chapter."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "tau/sled-narrative_qa",
        "instruction": "Given a chapter title, provide the name of the main character(s) in the chapter."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "input_prefix"
        ],
        "task_name": "tau/sled-narrative_qa",
        "instruction": "Given a chapter title, provide a question that can be answered by reading the chapter."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "tau/sled-gov_report",
        "instruction": "Given a report on a political event, extract the key players and their actions."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "tau/sled-gov_report",
        "instruction": "Given a report on renewable energy, extract the number of people employed in the utilities sector."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "tau/sled-gov_report",
        "instruction": "Given a report on renewable energy, extract the policy objectives for the development of \"green jobs\"."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "input_prefix",
            "output"
        ],
        "task_name": "tau/sled-quality",
        "instruction": "Given a text, generate a multiple-choice question based on the text."
    },
    {
        "input_fields": [
            "input_prefix"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "tau/sled-quality",
        "instruction": "Given a multiple-choice question, extract the correct answer."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "tau/sled-quality",
        "instruction": "Given a text, extract the name of the spaceman who retired."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "tau/sled-squad",
        "instruction": "Given a description of a place, identify the name of the place."
    },
    {
        "input_fields": [
            "input",
            "input_prefix"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "tau/sled-squad",
        "instruction": "Given a description of a place, identify a specific object or feature within the place."
    },
    {
        "input_fields": [
            "input",
            "input_prefix"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "tau/sled-squad_shuffled_distractors",
        "instruction": "Given a sentence and an input prefix, generate a question that can be answered by the sentence."
    },
    {
        "input_fields": [
            "input",
            "input_prefix"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "tau/sled-squad_ordered_distractors",
        "instruction": "Given a text and a question prefix, the task is to extract the answer from the text."
    },
    {
        "input_fields": [
            "input",
            "input_prefix"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "tau/sled-squad_ordered_distractors",
        "instruction": "Given a text, the task is to extract a specific piece of information from the text."
    },
    {
        "input_fields": [
            "input_prefix",
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "tau/sled-squad_ordered_distractors",
        "instruction": "Given a text, the task is to identify the location of a specific object or statue."
    },
    {
        "input_fields": [
            "sent",
            "event"
        ],
        "output_field": [
            "gen_sent",
            "gen_event"
        ],
        "task_name": "lara-martin/Scifi_TV_Shows",
        "instruction": "Given a sentence with events, generate a sentence with the same meaning but with the events replaced by their corresponding tags."
    },
    {
        "input_fields": [
            "text",
            "beer_ABV",
            "beer_style",
            "review_appearance",
            "review_palette",
            "review_taste",
            "review_aroma"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "arize-ai/beer_reviews_label_drift_neg",
        "instruction": "Predict the label of a beer review based on the text and other features. Answers must be one of neutral, negative, positive."
    },
    {
        "input_fields": [
            "beer_name"
        ],
        "output_field": [
            "beer_style"
        ],
        "task_name": "arize-ai/beer_reviews_label_drift_neg",
        "instruction": "Given a beer name, predict its style."
    },
    {
        "input_fields": [
            "beer_style"
        ],
        "output_field": [
            "beer_ABV"
        ],
        "task_name": "arize-ai/beer_reviews_label_drift_neg",
        "instruction": "Given a beer style, predict the average ABV."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/gad-gad_fold0_source",
        "instruction": "Given a sentence, predict the label of the sentence. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/gad-gad_fold1_source",
        "instruction": "Predict the label of the sentence (0 or 1). Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/gad-gad_fold2_source",
        "instruction": "Given a sentence, predict whether the sentence indicates a genetic vulnerability for a disease or not. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/gad-gad_fold3_source",
        "instruction": "Given a sentence containing a disease and a gene, predict whether there is a significant association between them. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/gad-gad_fold4_source",
        "instruction": "Given a sentence, predict whether it suggests further testing is needed for an interaction between a gene and a disease. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/gad-gad_fold5_source",
        "instruction": "Given a sentence containing a gene and a disease, predict whether the gene is associated with the disease. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/gad-gad_fold6_source",
        "instruction": "Given a sentence, predict the label (0 or 1) for the presence of mutations in NLGN3 and NLGN4 genes in autism cases. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/gad-gad_fold7_source",
        "instruction": "Given a sentence containing a gene and a disease, predict whether the gene is a major genetic risk factor for the disease. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/gad-gad_fold7_source",
        "instruction": "Given a sentence containing TGF-beta 1 and a gene, predict whether these genes are loci influencing disease susceptibility in a certain population. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/gad-gad_fold8_source",
        "instruction": "Given a sentence, predict the label (0 or 1) for the presence of a specific disease. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/gad-gad_fold9_source",
        "instruction": "Given a sentence containing a gene and a disease, predict whether it is relevant to prognosis or not. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/gad-gad_fold9_source",
        "instruction": "Given a sentence containing a gene and a level of IGFBP-3 protein, predict whether it is associated with an increased risk of breast cancer or not. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-iid",
        "instruction": "Given a math problem with multiple choices, select the biggest value among them."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program"
        ],
        "task_name": "allenai/lila-iid",
        "instruction": "Given a math problem, write a Python program to solve it."
    },
    {
        "input_fields": [
            "output_program",
            "output_answer"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "allenai/lila-ood",
        "instruction": "Given the input and output program, can you generate the input for the program?"
    },
    {
        "input_fields": [
            "output_program"
        ],
        "output_field": [
            "input",
            "output_answer"
        ],
        "task_name": "allenai/lila-ood",
        "instruction": "Given the input and output program, can you generate the input and output answer?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-addsub",
        "instruction": "Given a subtraction problem, provide the answer."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-addsub",
        "instruction": "Given an addition problem, provide the answer."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program"
        ],
        "task_name": "allenai/lila-addsub",
        "instruction": "Given a word problem, generate the corresponding Python program."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-amps_algebra",
        "instruction": "Solve the system of two equations."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-amps_algebra",
        "instruction": "Find the norm and argument of a complex number."
    },
    {
        "input_fields": [
            "output_program"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "allenai/lila-amps_calculus",
        "instruction": "Given the input and output program, can you design a problem that fits the program?"
    },
    {
        "input_fields": [
            "output_program"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-amps_calculus",
        "instruction": "Given the input and output program, can you design a problem that fits the answer?"
    },
    {
        "input_fields": [
            "output_answer"
        ],
        "output_field": [
            "output_program"
        ],
        "task_name": "allenai/lila-amps_calculus",
        "instruction": "Given the input and output answer, can you design a program that fits the answer?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program",
            "output_answer"
        ],
        "task_name": "allenai/lila-amps_counting_and_stats",
        "instruction": "Given a list of numbers, compute the maximum and minimum values."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program",
            "output_answer"
        ],
        "task_name": "allenai/lila-amps_counting_and_stats",
        "instruction": "Given a list of numbers, compute the range of the values."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-amps_geometry",
        "instruction": "Estimate the surface area, volume, and solid angle of a polyhedron given its vertices coordinates."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-amps_geometry",
        "instruction": "Estimate the interior angles, area, and perimeter of a polygon given its vertices coordinates. Classify the polygon as \"Simple\" or \"Convex\"."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-amps_linear_algebra",
        "instruction": "Find the distance between two given vectors."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-amps_linear_algebra",
        "instruction": "Given a point and a plane, find the distance between them."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-amps_number_theory",
        "instruction": "Find the smallest integer m such that a^m is congruent to 1 modulo b."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-amps_number_theory",
        "instruction": "Determine whether a number is divisible by another number."
    },
    {
        "input_fields": [
            "output_program"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "allenai/lila-APPS_structured",
        "instruction": "Given a program, can you write a description of what the program does?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program",
            "output_answer"
        ],
        "task_name": "allenai/lila-APPS_structured",
        "instruction": "Given a problem description, can you write a program to solve it?"
    },
    {
        "input_fields": [
            "output_program"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "allenai/lila-asdiv",
        "instruction": "Given an input and output program, the task is to generate a question that can be answered by the program."
    },
    {
        "input_fields": [
            "input",
            "output_program"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-asdiv",
        "instruction": "Given an input and output program, the task is to generate the correct output answer."
    },
    {
        "input_fields": [
            "input",
            "output_answer"
        ],
        "output_field": [
            "output_program"
        ],
        "task_name": "allenai/lila-asdiv",
        "instruction": "Given an input and output answer, the task is to generate the correct output program."
    },
    {
        "input_fields": [
            "input",
            "output_program"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-conala_structured",
        "instruction": "Given an input string and an output program, generate a natural language description of what the program does."
    },
    {
        "input_fields": [
            "output_program"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "allenai/lila-conala_structured",
        "instruction": "Given an input string and an output program, generate a natural language description of the input."
    },
    {
        "input_fields": [
            "input",
            "output_program"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-conala_structured",
        "instruction": "Given an input string and an output program, generate a natural language description of the output."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-deepmind_mathematics_algebra",
        "instruction": "Solve the given algebraic equation for a specific variable."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program"
        ],
        "task_name": "allenai/lila-deepmind_mathematics_algebra",
        "instruction": "Write a program to solve the given algebraic equation."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-deepmind_mathematics_algebra",
        "instruction": "Determine if a certain number is a factor of another number in the given algebraic equation."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-deepmind_mathematics_calculus",
        "instruction": "Given a mathematical expression, find the second derivative with respect to a given variable."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-deepmind_mathematics_calculus",
        "instruction": "Given a mathematical expression, find the third derivative with respect to a given variable."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-deepmind_mathematics_muldiv",
        "instruction": "Evaluate the given mathematical expression."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program"
        ],
        "task_name": "allenai/lila-deepmind_mathematics_muldiv",
        "instruction": "Write a Python program to evaluate the given mathematical expression."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-deepmind_mathematics_numbertheory",
        "instruction": "Given an integer, find the hundred thousands digit of the integer."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-deepmind_mathematics_numbertheory",
        "instruction": "Given two equations, solve for the prime factors of z."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-dolphin_t2_final",
        "instruction": "Solve a system of linear equations given in the input and output the answer."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program"
        ],
        "task_name": "allenai/lila-dolphin_t2_final",
        "instruction": "Write a program to solve a system of linear equations given in the input and output the program."
    },
    {
        "input_fields": [
            "output_program",
            "output_answer"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "allenai/lila-GSM8k_structured",
        "instruction": "Given an input and output program, write a question that can be solved by the program."
    },
    {
        "input_fields": [
            "input",
            "output_answer"
        ],
        "output_field": [
            "output_program"
        ],
        "task_name": "allenai/lila-GSM8k_structured",
        "instruction": "Given an input and output program, write a program that can solve the same problem."
    },
    {
        "input_fields": [
            "input",
            "output_program"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-GSM8k_structured",
        "instruction": "Given an input and output program, what is the expected output for a different input?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-MATH_algebra_crowdsourced",
        "instruction": "Solve the algebraic equation for a given value of x."
    },
    {
        "input_fields": [
            "output_program"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "allenai/lila-MATH_algebra_crowdsourced",
        "instruction": "Provide the algebraic equation given the output program."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-MATH_intermediate_algebra_crowdsourced",
        "instruction": "Solve the equation system and calculate the product of the solutions."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-MATH_intermediate_algebra_crowdsourced",
        "instruction": "Calculate the length of a complex number."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program"
        ],
        "task_name": "allenai/lila-MATH_intermediate_algebra_crowdsourced",
        "instruction": "Extract the variables used in the equation system."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program",
            "output_answer"
        ],
        "task_name": "allenai/lila-mathqa_gain",
        "instruction": "Given the input, write a program to calculate the present worth of the sum due."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program",
            "output_answer"
        ],
        "task_name": "allenai/lila-mathqa_gain",
        "instruction": "Given the input, write a program to calculate the percentage."
    },
    {
        "input_fields": [
            "input",
            "output_program"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-mathqa_general",
        "instruction": "Given a math problem and its solution program, can you identify the correct answer?"
    },
    {
        "input_fields": [
            "output_program",
            "output_answer"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "allenai/lila-mathqa_geometry",
        "instruction": "Given the input and output program, can you generate the input for the program?"
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-mathqa_other",
        "instruction": "Given a math problem with a ratio question, provide the ratio of the given quantities."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program"
        ],
        "task_name": "allenai/lila-mathqa_other",
        "instruction": "Given a math problem with a ratio question, provide the program to solve the problem."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-mathqa_probability",
        "instruction": "Calculate the probability of selecting a specific set of items from a larger set."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program"
        ],
        "task_name": "allenai/lila-mathqa_probability",
        "instruction": "Write a program to calculate the probability of selecting a specific set of items from a larger set."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program"
        ],
        "task_name": "allenai/lila-multiarith",
        "instruction": "Given an arithmetic word problem, write a program to solve it."
    },
    {
        "input_fields": [
            "input",
            "output_program"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-multiarith",
        "instruction": "Given an arithmetic word problem and its program, compute the answer."
    },
    {
        "input_fields": [
            "input",
            "output_answer"
        ],
        "output_field": [
            "output_program"
        ],
        "task_name": "allenai/lila-multiarith",
        "instruction": "Given an arithmetic word problem and its answer, write a program to solve it."
    },
    {
        "input_fields": [
            "input",
            "output_program"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-NumGLUE_Type_1_crowdsourced",
        "instruction": "Given a problem and a solution program, the task is to identify the output answer."
    },
    {
        "input_fields": [
            "input",
            "output_answer"
        ],
        "output_field": [
            "output_program"
        ],
        "task_name": "allenai/lila-NumGLUE_Type_1_crowdsourced",
        "instruction": "Given a problem and the output answer, the task is to generate a solution program."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program",
            "output_answer"
        ],
        "task_name": "allenai/lila-NumGLUE_Type_2_crowdsourced",
        "instruction": "Given a chemical equation, write a program to calculate the mass percentage of a given element in a given compound."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program",
            "output_answer"
        ],
        "task_name": "allenai/lila-NumGLUE_Type_2_crowdsourced",
        "instruction": "Given a chemical equation, write a program to calculate the amount of a given product formed from a given amount of reactants."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-NumGLUE_Type_3_crowdsourced",
        "instruction": "Given the speed of an agate rolling in different environments, determine which environment will cause the agate to roll a greater distance. Answers must be one of Option 1, Option 2."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-NumGLUE_Type_3_crowdsourced",
        "instruction": "Given the time it takes for a ball to stop rolling on grass and sand, determine which surface has more friction. Answers must be one of Option 1, Option 2."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program"
        ],
        "task_name": "allenai/lila-NumGLUE_Type_6_crowdsourced",
        "instruction": "Given a passage describing a football game, extract the names of the players who scored touchdowns and print them in a list."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program"
        ],
        "task_name": "allenai/lila-NumGLUE_Type_6_crowdsourced",
        "instruction": "Given a passage describing a county, extract the ancestral groups that make up at least 10% of the population and print them in a list."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-NumGLUE_Type_6_crowdsourced",
        "instruction": "Given a passage describing a football game, extract the name of the player who scored the most touchdowns and print it as a string."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-NumGLUE_Type_7_crowdsourced",
        "instruction": "Determine if the two statements are contradictory or not. Answers must be one of neutral, Entailment, contradiction."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program",
            "output_answer"
        ],
        "task_name": "allenai/lila-NumGLUE_Type_7_crowdsourced",
        "instruction": "Extract the number of books Rahim bought from the statement."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program",
            "output_answer"
        ],
        "task_name": "allenai/lila-NumGLUE_Type_7_crowdsourced",
        "instruction": "Extract the number of days Sakshi can do a piece of work from the statement."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-NumGLUE_Type_8_crowdsourced",
        "instruction": "Given a total amount and number of packs, calculate the price per can of a soft drink."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-NumGLUE_Type_8_crowdsourced",
        "instruction": "Given the number of books to put away, the number of books added, and the number of books that can fit on a shelf, calculate the number of shelves needed."
    },
    {
        "input_fields": [
            "input",
            "output_answer"
        ],
        "output_field": [
            "output_program"
        ],
        "task_name": "allenai/lila-simuleq",
        "instruction": "Solve the equation system given the input and output answer."
    },
    {
        "input_fields": [
            "input",
            "output_program"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-simuleq",
        "instruction": "Given the input and output program, find the answer."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-singleop",
        "instruction": "Given a word problem, write a program to solve it and output the answer."
    },
    {
        "input_fields": [
            "input",
            "output_program"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-singleop",
        "instruction": "Given a word problem and its solution program, extract the input values."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output_program",
            "output_answer"
        ],
        "task_name": "allenai/lila-singleq",
        "instruction": "Given a word problem, write a program to solve it."
    },
    {
        "input_fields": [
            "input",
            "output_program"
        ],
        "output_field": [
            "output_answer"
        ],
        "task_name": "allenai/lila-singleq",
        "instruction": "Given a program and its input, predict the output."
    },
    {
        "input_fields": [
            "output_program",
            "output_answer"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "allenai/lila-singleq",
        "instruction": "Given a program and its output, predict the input."
    },
    {
        "input_fields": [
            "PUBCHEM_OPENEYE_CAN_SMILES"
        ],
        "output_field": [
            "CAN_SELFIES"
        ],
        "task_name": "zpn/pubchem_selfies",
        "instruction": "Given a SMILES string, convert it to a SELFIES string."
    },
    {
        "input_fields": [
            "CAN_SELFIES"
        ],
        "output_field": [
            "PUBCHEM_OPENEYE_CAN_SMILES"
        ],
        "task_name": "zpn/pubchem_selfies",
        "instruction": "Given a SELFIES string, convert it to a SMILES string."
    },
    {
        "input_fields": [
            "PUBCHEM_COMPOUND_CID"
        ],
        "output_field": [
            "PUBCHEM_OPENEYE_CAN_SMILES"
        ],
        "task_name": "zpn/pubchem_selfies",
        "instruction": "Given a CID, return the corresponding SMILES string."
    },
    {
        "input_fields": [
            "PUBCHEM_COMPOUND_CID"
        ],
        "output_field": [
            "CAN_SELFIES"
        ],
        "task_name": "zpn/pubchem_selfies",
        "instruction": "Given a CID, return the corresponding SELFIES string."
    },
    {
        "input_fields": [
            "source_text"
        ],
        "output_field": [
            "reply_text"
        ],
        "task_name": "strombergnlp/rumoureval_2019",
        "instruction": "Given a source text and a reply text, generate a new reply text that is not a comment."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "resolve_type"
        ],
        "task_name": "launch/open_question_type",
        "instruction": "Provide a definition for the given concept. Answers must be one of comparison, cause, extent, procedural, verification, judgmental, concept, example, disjunction."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "resolve_type"
        ],
        "task_name": "launch/open_question_type",
        "instruction": "Verify the truthfulness of the given statement. Answers must be one of comparison, cause, extent, procedural, verification, judgmental, concept, example, disjunction."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "EMBO/BLURB-BIOSSES",
        "instruction": "Given two sentences, determine if they are synthetically lethal or not."
    },
    {
        "input_fields": [
            "filingDate"
        ],
        "output_field": [
            "sentenceCount"
        ],
        "task_name": "JanosAudran/financial-reports-sec-large_lite",
        "instruction": "Given a filing date, predict the number of sentences in the document."
    },
    {
        "input_fields": [
            "sub_sentence",
            "table_content"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "kasnerz/hitab",
        "instruction": "Given a sub-sentence and a table, the task is to extract the relevant information from the table and fill in the blank in the sub-sentence."
    },
    {
        "input_fields": [
            "question",
            "table_content"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "kasnerz/hitab",
        "instruction": "Given a question and a table, the task is to extract the relevant information from the table and answer the question."
    },
    {
        "input_fields": [
            "table_content"
        ],
        "output_field": [
            "aggregation"
        ],
        "task_name": "kasnerz/hitab",
        "instruction": "Given a table, the task is to identify the aggregation method used in the table."
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "PolyAI/banking77",
        "instruction": "Given a label, generate a question about a banking issue."
    },
    {
        "input_fields": [
            "nl_statement"
        ],
        "output_field": [
            "formal_statement"
        ],
        "task_name": "hoskinson-center/proofnet",
        "instruction": "Given a natural language statement, provide the corresponding formal statement."
    },
    {
        "input_fields": [
            "formal_statement"
        ],
        "output_field": [
            "nl_statement"
        ],
        "task_name": "hoskinson-center/proofnet",
        "instruction": "Given a formal statement, provide a natural language statement."
    },
    {
        "input_fields": [
            "nl_statement"
        ],
        "output_field": [
            "nl_proof"
        ],
        "task_name": "hoskinson-center/proofnet",
        "instruction": "Given a natural language statement, provide a proof."
    },
    {
        "input_fields": [
            "formal_statement"
        ],
        "output_field": [
            "nl_proof"
        ],
        "task_name": "hoskinson-center/proofnet",
        "instruction": "Given a formal statement, provide a proof."
    },
    {
        "input_fields": [
            "ent_wikipedia_external_ref"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "DFKI-SLT/wikitext_linked-wikitext2",
        "instruction": "Extract the external references from the named entities."
    },
    {
        "input_fields": [
            "document"
        ],
        "output_field": [
            "summary"
        ],
        "task_name": "launch/gov_report-plain_text",
        "instruction": "Given a document, summarize it in a few sentences."
    },
    {
        "input_fields": [
            "summary"
        ],
        "output_field": [
            "document"
        ],
        "task_name": "launch/gov_report-plain_text",
        "instruction": "Given a summary, generate a document that could have produced it."
    },
    {
        "input_fields": [
            "document"
        ],
        "output_field": [
            "summary"
        ],
        "task_name": "launch/gov_report-plain_text_with_recommendations",
        "instruction": "Given a document, summarize it in a few sentences."
    },
    {
        "input_fields": [
            "summary"
        ],
        "output_field": [
            "document"
        ],
        "task_name": "launch/gov_report-plain_text_with_recommendations",
        "instruction": "Given a summary, generate a document that summarizes the original document."
    },
    {
        "input_fields": [
            "question",
            "distractor1",
            "distractor2",
            "distractor3"
        ],
        "output_field": [
            "correct_answer"
        ],
        "task_name": "bigbio/sciq-sciq_source",
        "instruction": "Given a question and its answer choices, choose the correct answer."
    },
    {
        "input_fields": [
            "question",
            "distractor1",
            "distractor2",
            "distractor3",
            "correct_answer"
        ],
        "output_field": [
            "support"
        ],
        "task_name": "bigbio/sciq-sciq_source",
        "instruction": "Given a question and its answer choices, provide a supporting statement for the correct answer."
    },
    {
        "input_fields": [
            "support"
        ],
        "output_field": [
            "correct_answer"
        ],
        "task_name": "bigbio/sciq-sciq_source",
        "instruction": "Given a supporting statement, identify the correct answer to the question."
    },
    {
        "input_fields": [
            "support"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "bigbio/sciq-sciq_source",
        "instruction": "Given a supporting statement, provide the question that it supports."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "bigbio/sciq-sciq_bigbio_qa",
        "instruction": "Provide the context for the given question."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question",
            "choices",
            "answer"
        ],
        "task_name": "bigbio/sciq-sciq_bigbio_qa",
        "instruction": "Can you design a question based on the context?"
    },
    {
        "input_fields": [
            "title",
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lhoestq/custom_squad",
        "instruction": "Given a question and title, the task is to extract the relevant context."
    },
    {
        "input_fields": [
            "title",
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lhoestq/custom_squad",
        "instruction": "Given a context and title, the task is to answer the question."
    },
    {
        "input_fields": [
            "judges"
        ],
        "output_field": [
            "conclusion"
        ],
        "task_name": "jonathanli/echr-non-anon",
        "instruction": "Identify the judges who made the decision."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "conclusion"
        ],
        "task_name": "jonathanli/echr-anon",
        "instruction": "Given the text of a legal case, predict the conclusion of the case (e.g. violation, inadmissible, etc.)."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "gokuls/glue_augmented_mrpc",
        "instruction": "Determine if two sentences are equivalent or not. Answers must be one of not_equivalent, equivalent."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "gokuls/glue_augmented_mnli",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis is supported by the premise. Answers must be one of neutral, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "gokuls/glue_augmented_mnli",
        "instruction": "Given a premise and a label, generate a hypothesis that is supported by the premise."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "gokuls/glue_augmented_mnli",
        "instruction": "Given a hypothesis and a label, generate a premise that supports the hypothesis."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-sst2",
        "instruction": "Given a sentence, predict its sentiment label. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-sst2",
        "instruction": "Given a set of sentences, predict their sentiment labels. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-sst2",
        "instruction": "Given a sentence, predict whether it contains humor or not. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-mrpc",
        "instruction": "Determine if two sentences are equivalent or not. Answers must be one of not_equivalent, equivalent."
    },
    {
        "input_fields": [
            "question1",
            "question2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-qqp",
        "instruction": "Given two questions, determine if they are duplicates or not. Answers must be one of duplicate, not_duplicate."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-stsb",
        "instruction": "Given two sentences, determine if they have the same meaning."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-stsb",
        "instruction": "Given a sentence pair and a label, predict which sentence has a higher degree of similarity."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-mnli",
        "instruction": "Given a premise and a hypothesis, predict whether the hypothesis is entailed by the premise or not. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "evaluate/glue-ci-mnli",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "evaluate/glue-ci-mnli",
        "instruction": "Given a hypothesis and a label, generate a premise that entails the hypothesis."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-mnli_mismatched",
        "instruction": "Given a premise and a hypothesis, predict whether the hypothesis contradicts the premise or not. Answers must be one of entailment, neutral, contradiction."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "evaluate/glue-ci-mnli_mismatched",
        "instruction": "Given a premise and a label, generate a hypothesis that contradicts the premise."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "evaluate/glue-ci-mnli_mismatched",
        "instruction": "Given a hypothesis and a label, generate a premise that contradicts the hypothesis."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-mnli_matched",
        "instruction": "Given a premise and a hypothesis, predict the label (entailment, neutral, or contradiction). Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "evaluate/glue-ci-mnli_matched",
        "instruction": "Given a premise and a label, generate a hypothesis that is consistent with the premise and label."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "evaluate/glue-ci-mnli_matched",
        "instruction": "Given a hypothesis and a label, generate a premise that is consistent with the hypothesis and label."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-rte",
        "instruction": "Determine if two sentences are entailed by each other or not. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-wnli",
        "instruction": "Given two sentences, determine whether they entail each other or not. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "evaluate/glue-ci-wnli",
        "instruction": "Given a sentence pair, predict whether the first sentence entails the second sentence or vice versa. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence1",
            "label"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "evaluate/glue-ci-wnli",
        "instruction": "Given a sentence and a label, predict whether the sentence entails the label or not."
    },
    {
        "input_fields": [
            "question1",
            "question2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "gokuls/glue_augmented_qqp",
        "instruction": "Determine if two questions are duplicates or not. Answers must be one of duplicate, not_duplicate."
    },
    {
        "input_fields": [
            "question1"
        ],
        "output_field": [
            "question2"
        ],
        "task_name": "gokuls/glue_augmented_qqp",
        "instruction": "Given a question, provide a personal experience related to the topic."
    },
    {
        "input_fields": [
            "question2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "gokuls/glue_augmented_qqp",
        "instruction": "Given a question, determine the level of preparation needed for a specific exam. Answers must be one of duplicate, not_duplicate."
    },
    {
        "input_fields": [
            "pixel_values"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "Xpitfire/cmp_facade",
        "instruction": "Given the pixel values of an image, predict the corresponding label."
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "pixel_values"
        ],
        "task_name": "Xpitfire/cmp_facade",
        "instruction": "Given the label of an image, generate the corresponding pixel values."
    },
    {
        "input_fields": [
            "document"
        ],
        "output_field": [
            "summary"
        ],
        "task_name": "polinaeterna/xsum",
        "instruction": "Given a document, summarize it in one sentence."
    },
    {
        "input_fields": [
            "summary"
        ],
        "output_field": [
            "document"
        ],
        "task_name": "polinaeterna/xsum",
        "instruction": "Given a summary, generate a document that could have produced it."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "sagnikrayc/snli-bt",
        "instruction": "Given a premise and a hypothesis, predict whether the hypothesis is entailed by the premise or not. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "sagnikrayc/snli-bt",
        "instruction": "Given a premise, generate a hypothesis that is entailed by the premise."
    },
    {
        "input_fields": [
            "fen",
            "move"
        ],
        "output_field": [
            "result"
        ],
        "task_name": "jrahn/yolochess_deepblue",
        "instruction": "Given a FEN notation and a move, predict the result of the game. Answers must be one of 1-0, 1/2-1/2."
    },
    {
        "input_fields": [
            "fen",
            "move"
        ],
        "output_field": [
            "eco"
        ],
        "task_name": "jrahn/yolochess_deepblue",
        "instruction": "Given a FEN notation and a move, predict the opening classification (ECO code) of the game. Answers must be one of A00, B17, B12, A07, C93."
    },
    {
        "input_fields": [
            "fen"
        ],
        "output_field": [
            "move"
        ],
        "task_name": "jrahn/yolochess_deepblue",
        "instruction": "Given a FEN notation, predict the next best move."
    },
    {
        "input_fields": [
            "word",
            "countrycode"
        ],
        "output_field": [
            "timestamp"
        ],
        "task_name": "quickdraw-raw",
        "instruction": "Given a word and a country code, determine the timestamp when it was drawn."
    },
    {
        "input_fields": [
            "timestamp"
        ],
        "output_field": [
            "countrycode"
        ],
        "task_name": "quickdraw-preprocessed_simplified_drawings",
        "instruction": "Given a timestamp, determine the country code of the drawing."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "variant"
        ],
        "task_name": "gsarti/magpie",
        "instruction": "Identify the variant of a given phrase in the sentence."
    },
    {
        "input_fields": [
            "eurovoc_concepts"
        ],
        "output_field": [
            "title",
            "text"
        ],
        "task_name": "jonathanli/eurlex",
        "instruction": "Given a eurovoc concept, find all the documents that contain this concept."
    },
    {
        "input_fields": [
            "instruction"
        ],
        "output_field": [
            "demonstration"
        ],
        "task_name": "HuggingFaceH4/helpful-anthropic-raw",
        "instruction": "Provide a brief introduction to a topic or activity."
    },
    {
        "input_fields": [
            "demonstration"
        ],
        "output_field": [
            "instruction"
        ],
        "task_name": "HuggingFaceH4/helpful-anthropic-raw",
        "instruction": "Provide a demonstration or explanation of a topic or activity."
    },
    {
        "input_fields": [
            "smiles"
        ],
        "output_field": [
            "selfies"
        ],
        "task_name": "zpn/clintox",
        "instruction": "Convert the SMILES representation of the molecule to SELFIES."
    },
    {
        "input_fields": [
            "selfies"
        ],
        "output_field": [
            "smiles"
        ],
        "task_name": "zpn/clintox",
        "instruction": "Convert the SELFIES representation of the molecule to SMILES."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "questions"
        ],
        "task_name": "derek-thomas/squad-v1.1-t5-question-generation",
        "instruction": "Given a context, generate a question about a specific detail mentioned in the context."
    },
    {
        "input_fields": [
            "questions"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "derek-thomas/squad-v1.1-t5-question-generation",
        "instruction": "Given a question, extract the specific detail mentioned in the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "questions"
        ],
        "task_name": "derek-thomas/squad-v1.1-t5-question-generation",
        "instruction": "Given a context, generate a question that requires inference or interpretation of the information provided."
    },
    {
        "input_fields": [
            "title",
            "abstract"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "allenai/cochrane_sparse_max",
        "instruction": "Extract the key findings from the article."
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "gokuls/glue_augmented_sst2",
        "instruction": "Can you generate a new sentence with the same sentiment label?"
    },
    {
        "input_fields": [
            "comment"
        ],
        "output_field": [
            "number"
        ],
        "task_name": "usc-isi/WikiConvert",
        "instruction": "Extract the number mentioned in the comment."
    },
    {
        "input_fields": [
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "gokuls/glue_augmented_wnli",
        "instruction": "Given a sentence, generate a paraphrase of the sentence."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "StatsGary/socialmedia-abuse",
        "instruction": "Given a text, predict whether it contains abusive language or not. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "gokuls/glue_augmented_rte",
        "instruction": "Determine if the two sentences entail each other or not. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence1",
            "label"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "gokuls/glue_augmented_rte",
        "instruction": "Given a sentence and a label, generate a sentence that does not entail the original sentence."
    },
    {
        "input_fields": [
            "text_1",
            "text_2"
        ],
        "output_field": [
            "mean_score",
            "std_score"
        ],
        "task_name": "bigbio/umnsrs-umnsrs_similarity_mod_source",
        "instruction": "Given two medical terms, predict the similarity score between them."
    },
    {
        "input_fields": [
            "text_1",
            "text_2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/umnsrs-umnsrs_similarity_mod_bigbio_pairs",
        "instruction": "Given two texts, predict the similarity score between them."
    },
    {
        "input_fields": [
            "text_1",
            "text_2"
        ],
        "output_field": [
            "mean_score",
            "std_score"
        ],
        "task_name": "bigbio/umnsrs-umnsrs_similarity_source",
        "instruction": "Given two medical terms, predict the similarity score between them."
    },
    {
        "input_fields": [
            "text_1",
            "text_2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/umnsrs-umnsrs_similarity_bigbio_pairs",
        "instruction": "Given two texts, predict the similarity score between them."
    },
    {
        "input_fields": [
            "text_1"
        ],
        "output_field": [
            "text_2",
            "label"
        ],
        "task_name": "bigbio/umnsrs-umnsrs_similarity_bigbio_pairs",
        "instruction": "Given a text, predict the most similar text from a set of texts."
    },
    {
        "input_fields": [
            "text_1",
            "text_2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/umnsrs-umnsrs_similarity_bigbio_pairs",
        "instruction": "Given a similarity score, predict whether the two texts are similar or dissimilar."
    },
    {
        "input_fields": [
            "text_1",
            "text_2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/umnsrs-umnsrs_relatedness_mod_bigbio_pairs",
        "instruction": "Given two texts, predict the relatedness score between them."
    },
    {
        "input_fields": [
            "text_1",
            "text_2"
        ],
        "output_field": [
            "mean_score",
            "std_score"
        ],
        "task_name": "bigbio/umnsrs-umnsrs_relatedness_source",
        "instruction": "Given two medical terms, predict the relatedness score between them."
    },
    {
        "input_fields": [
            "text_1"
        ],
        "output_field": [
            "code_1"
        ],
        "task_name": "bigbio/umnsrs-umnsrs_relatedness_source",
        "instruction": "Given a medical term, predict the code associated with it."
    },
    {
        "input_fields": [
            "code_2"
        ],
        "output_field": [
            "text_2"
        ],
        "task_name": "bigbio/umnsrs-umnsrs_relatedness_source",
        "instruction": "Given a code, predict the medical term associated with it."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "gokuls/glue_augmented_stsb",
        "instruction": "Given two sentences, determine if they have the same meaning."
    },
    {
        "input_fields": [
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "gokuls/glue_augmented_stsb",
        "instruction": "Given a sentence, generate a similar sentence with a different wording."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "gokuls/glue_augmented_stsb",
        "instruction": "Given a sentence pair, determine the degree of similarity between them."
    },
    {
        "input_fields": [
            "snomed_label_1",
            "snomed_label_2",
            "rater_A",
            "rater_B",
            "rater_C",
            "rater_D",
            "rater_E"
        ],
        "output_field": [
            "mean_rating"
        ],
        "task_name": "bigbio/ehr_rel-ehr_rel_source",
        "instruction": "Predict the mean rating given the two SNOMED labels and the ratings from five raters."
    },
    {
        "input_fields": [
            "CUI_1"
        ],
        "output_field": [
            "snomed_label_1"
        ],
        "task_name": "bigbio/ehr_rel-ehr_rel_source",
        "instruction": "Identify the SNOMED label given the CUI."
    },
    {
        "input_fields": [
            "snomed_label_2"
        ],
        "output_field": [
            "CUI_2"
        ],
        "task_name": "bigbio/ehr_rel-ehr_rel_source",
        "instruction": "Identify the CUI given the SNOMED label."
    },
    {
        "input_fields": [
            "snomed_label_1",
            "snomed_label_2"
        ],
        "output_field": [
            "mean_rating"
        ],
        "task_name": "bigbio/ehr_rel-ehr_rel_a_source",
        "instruction": "Predict the mean rating given the two SNOMED labels."
    },
    {
        "input_fields": [
            "snomed_label_1"
        ],
        "output_field": [
            "CUI_1"
        ],
        "task_name": "bigbio/ehr_rel-ehr_rel_a_source",
        "instruction": "Identify the CUI associated with a given SNOMED label."
    },
    {
        "input_fields": [
            "CUI_2"
        ],
        "output_field": [
            "snomed_label_2"
        ],
        "task_name": "bigbio/ehr_rel-ehr_rel_a_source",
        "instruction": "Identify the SNOMED label associated with a given CUI."
    },
    {
        "input_fields": [
            "snomed_label_1"
        ],
        "output_field": [
            "CUI_1"
        ],
        "task_name": "bigbio/ehr_rel-ehr_rel_b_source",
        "instruction": "Given a snomed label, predict the corresponding CUI."
    },
    {
        "input_fields": [
            "CUI_2"
        ],
        "output_field": [
            "snomed_label_2"
        ],
        "task_name": "bigbio/ehr_rel-ehr_rel_b_source",
        "instruction": "Given a CUI, predict the corresponding snomed label."
    },
    {
        "input_fields": [
            "rater_A",
            "rater_B",
            "rater_C",
            "rater_D",
            "rater_E"
        ],
        "output_field": [
            "mean_rating"
        ],
        "task_name": "bigbio/ehr_rel-ehr_rel_b_source",
        "instruction": "Given the ratings of raters A, B, C, D, and E, predict the mean rating."
    },
    {
        "input_fields": [
            "text_1",
            "text_2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/ehr_rel-ehr_rel_bigbio_pairs",
        "instruction": "Given two medical terms, predict the likelihood of a relationship between them."
    },
    {
        "input_fields": [
            "text_1"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/ehr_rel-ehr_rel_bigbio_pairs",
        "instruction": "Given a medical term, predict the likelihood of it being associated with abdominal pain."
    },
    {
        "input_fields": [
            "text_1"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/ehr_rel-ehr_rel_bigbio_pairs",
        "instruction": "Given a medical term, predict the likelihood of it being associated with dysuria."
    },
    {
        "input_fields": [
            "demonstration"
        ],
        "output_field": [
            "instruction"
        ],
        "task_name": "HuggingFaceH4/helpful-self-instruct-raw",
        "instruction": "Given a list of study tips, identify the corresponding number for each tip."
    },
    {
        "input_fields": [
            "instruction"
        ],
        "output_field": [
            "demonstration"
        ],
        "task_name": "HuggingFaceH4/helpful-self-instruct-raw",
        "instruction": "Given a sentence, identify the key topic."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "arize-ai/ecommerce_reviews_with_language_drift",
        "instruction": "Given a review text, predict the sentiment of the review (positive/negative). Answers must be one of neutral, negative, positive."
    },
    {
        "input_fields": [
            "product_category"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "arize-ai/ecommerce_reviews_with_language_drift",
        "instruction": "Given a product category, predict the sentiment of the reviews for that category. Answers must be one of neutral, negative, positive."
    },
    {
        "input_fields": [
            "reviewer_age",
            "reviewer_gender"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "arize-ai/ecommerce_reviews_with_language_drift",
        "instruction": "Given a reviewer age and gender, predict the sentiment of their reviews. Answers must be one of neutral, negative, positive."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "reviewer_age",
            "reviewer_gender"
        ],
        "task_name": "arize-ai/ecommerce_reviews_with_language_drift",
        "instruction": "Given a review text, predict the age and gender of the reviewer."
    },
    {
        "input_fields": [
            "sentence",
            "answer"
        ],
        "output_field": [
            "sentence_answer"
        ],
        "task_name": "lmqg/qg_squadshifts-all",
        "instruction": "Given a sentence and its answer, highlight the answer in the sentence."
    },
    {
        "input_fields": [
            "paragraph",
            "answer"
        ],
        "output_field": [
            "paragraph_answer"
        ],
        "task_name": "lmqg/qg_squadshifts-all",
        "instruction": "Given a paragraph and its answer, highlight the answer in the paragraph."
    },
    {
        "input_fields": [
            "paragraph",
            "sentence"
        ],
        "output_field": [
            "paragraph_sentence"
        ],
        "task_name": "lmqg/qg_squadshifts-all",
        "instruction": "Given a paragraph and a sentence, highlight the sentence in the paragraph."
    },
    {
        "input_fields": [
            "sentence",
            "question"
        ],
        "output_field": [
            "sentence_answer"
        ],
        "task_name": "lmqg/qg_squadshifts-nyt",
        "instruction": "Given a sentence and a question, the task is to identify the answer."
    },
    {
        "input_fields": [
            "paragraph",
            "question"
        ],
        "output_field": [
            "paragraph_answer"
        ],
        "task_name": "lmqg/qg_squadshifts-nyt",
        "instruction": "Given a paragraph and a question, the task is to identify the answer."
    },
    {
        "input_fields": [
            "paragraph",
            "sentence"
        ],
        "output_field": [
            "paragraph_sentence"
        ],
        "task_name": "lmqg/qg_squadshifts-nyt",
        "instruction": "Given a paragraph and a sentence, the task is to identify the sentence that contains the answer."
    },
    {
        "input_fields": [
            "paragraph_question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "lmqg/qg_squadshifts-reddit",
        "instruction": "Given a question and context, identify the main reason for wanting to hack."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "sentence_answer"
        ],
        "task_name": "lmqg/qg_squadshifts-reddit",
        "instruction": "Given a sentence, highlight the subreddit mentioned."
    },
    {
        "input_fields": [
            "paragraph"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qg_squadshifts-reddit",
        "instruction": "Given a paragraph, extract the question being asked."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "paragraph"
        ],
        "task_name": "lmqg/qg_squadshifts-reddit",
        "instruction": "Given a sentence, extract the context."
    },
    {
        "input_fields": [
            "paragraph"
        ],
        "output_field": [
            "paragraph_answer"
        ],
        "task_name": "lmqg/qg_squadshifts-reddit",
        "instruction": "Given a paragraph, extract the answer."
    },
    {
        "input_fields": [
            "paragraph"
        ],
        "output_field": [
            "paragraph_sentence"
        ],
        "task_name": "lmqg/qg_squadshifts-reddit",
        "instruction": "Given a paragraph, extract the sentence with the highlighted answer."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "IDEA-CCNL/AFQMC",
        "instruction": "Determine whether two sentences are semantically similar or not. Answers must be one of similar, not similar."
    },
    {
        "input_fields": [
            "sentence1"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "IDEA-CCNL/AFQMC",
        "instruction": "Given a sentence, predict whether it is asking about increasing the credit limit of a payment method or not. Answers must be one of similar, not similar."
    },
    {
        "input_fields": [
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "IDEA-CCNL/AFQMC",
        "instruction": "Given a sentence, predict whether it is asking about the payment method support of a specific service or not. Answers must be one of similar, not similar."
    },
    {
        "input_fields": [
            "Abstract"
        ],
        "output_field": [
            "Keywords"
        ],
        "task_name": "Adapting/abstract-keyphrases",
        "instruction": "Given an abstract, predict the keywords that best describe the content."
    },
    {
        "input_fields": [
            "Keywords"
        ],
        "output_field": [
            "Abstract"
        ],
        "task_name": "Adapting/abstract-keyphrases",
        "instruction": "Given a set of keywords, generate an abstract that best describes the content."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "text_type"
        ],
        "task_name": "bigbio/chia-chia_fixed_source",
        "instruction": "Identify the exclusion criteria for a clinical trial based on the text. Answers must be one of exclusion, inclusion."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "text_type"
        ],
        "task_name": "bigbio/chia-chia_fixed_source",
        "instruction": "Identify the inclusion criteria for a clinical trial based on the text. Answers must be one of exclusion, inclusion."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "text_type"
        ],
        "task_name": "bigbio/chia-chia_fixed_source",
        "instruction": "Extract the laboratory parameters for renal and hepatic function from the text. Answers must be one of exclusion, inclusion."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "bigbio/scitail-scitail_source",
        "instruction": "Given a premise and a label, generate a hypothesis that is either neutral, entailment, or contradiction."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "bigbio/scitail-scitail_bigbio_te",
        "instruction": "Given a premise and a label, generate a hypothesis that is entailed by the premise or is neutral to the premise."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "bigbio/scitail-scitail_bigbio_te",
        "instruction": "Given a hypothesis and a label, generate a premise that entails the hypothesis or is neutral to the hypothesis."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-sst2",
        "instruction": "Determine the sentiment of the given sentence. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "mariosasko/glue-sst2",
        "instruction": "Generate a sentence with the given sentiment."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-mrpc",
        "instruction": "Determine if two sentences are equivalent or not. Answers must be one of not_equivalent, equivalent."
    },
    {
        "input_fields": [
            "question1",
            "question2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-qqp",
        "instruction": "Determine if two questions are duplicates or not. Answers must be one of duplicate, not_duplicate."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-stsb",
        "instruction": "Given two sentences, predict the similarity score between them."
    },
    {
        "input_fields": [
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "mariosasko/glue-stsb",
        "instruction": "Given a sentence, generate a similar sentence with a higher similarity score."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-mnli",
        "instruction": "Given a premise and a hypothesis, predict whether the hypothesis is entailed by the premise or not. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-mnli_mismatched",
        "instruction": "Given a premise and a hypothesis, determine whether they are in agreement or disagreement. Answers must be one of entailment, neutral, contradiction."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "mariosasko/glue-mnli_mismatched",
        "instruction": "Given a premise and a label, generate a hypothesis that is in agreement with the premise."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "mariosasko/glue-mnli_mismatched",
        "instruction": "Given a hypothesis and a label, generate a premise that is in agreement with the hypothesis."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-mnli_matched",
        "instruction": "Given a premise and a hypothesis, predict the relationship between them. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "mariosasko/glue-mnli_matched",
        "instruction": "Given a premise and a label, generate a hypothesis that is consistent with the premise and label."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "mariosasko/glue-mnli_matched",
        "instruction": "Given a hypothesis and a label, generate a premise that is consistent with the hypothesis and label."
    },
    {
        "input_fields": [
            "question",
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-qnli",
        "instruction": "Determine whether the given sentence entails the answer to the question or not. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "question",
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-qnli",
        "instruction": "Provide the answer to the given question based on the information in the sentence. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-rte",
        "instruction": "Determine whether the two sentences are entailed or not entailed. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence1",
            "label"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "mariosasko/glue-rte",
        "instruction": "Given a sentence and a label, generate a new sentence that is entailed by the original sentence."
    },
    {
        "input_fields": [
            "sentence1",
            "label"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "mariosasko/glue-rte",
        "instruction": "Given a sentence and a label, generate a new sentence that is not entailed by the original sentence."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "mariosasko/glue-wnli",
        "instruction": "Determine if the two sentences are entailed by each other. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence1",
            "label"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "mariosasko/glue-wnli",
        "instruction": "Given a sentence and a label, generate a new sentence that entails the original sentence."
    },
    {
        "input_fields": [
            "sentence1",
            "label"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "mariosasko/glue-wnli",
        "instruction": "Given a sentence and a label, generate a new sentence that contradicts the original sentence."
    },
    {
        "input_fields": [
            "img_bytes"
        ],
        "output_field": [
            "facial_hair"
        ],
        "task_name": "cgarciae/cartoonset-10k+features",
        "instruction": "Given the image bytes, predict the facial hair category."
    },
    {
        "input_fields": [
            "img_bytes"
        ],
        "output_field": [
            "hair_color"
        ],
        "task_name": "cgarciae/cartoonset-10k+features",
        "instruction": "Given the image bytes, predict the hair color category. Answers must be one of 9, 7, 8, 0, 3, 6, 4, 5, 2, 1."
    },
    {
        "input_fields": [
            "eyebrow_shape",
            "eyebrow_thickness"
        ],
        "output_field": [
            "eyebrow_weight"
        ],
        "task_name": "cgarciae/cartoonset-10k+features",
        "instruction": "Given the eyebrow shape and eyebrow thickness, predict the eyebrow weight category. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "img_bytes"
        ],
        "output_field": [
            "facial_hair"
        ],
        "task_name": "cgarciae/cartoonset-100k+features",
        "instruction": "Given the image bytes, predict the facial hair category."
    },
    {
        "input_fields": [
            "img_bytes"
        ],
        "output_field": [
            "hair_color"
        ],
        "task_name": "cgarciae/cartoonset-100k+features",
        "instruction": "Given the image bytes, predict the hair color category. Answers must be one of 9, 8, 7, 0, 3, 6, 4, 5, 2, 1."
    },
    {
        "input_fields": [
            "img_bytes"
        ],
        "output_field": [
            "glasses"
        ],
        "task_name": "cgarciae/cartoonset-100k+features",
        "instruction": "Given the image bytes, predict the glasses category."
    },
    {
        "input_fields": [
            "text_1"
        ],
        "output_field": [
            "code_1"
        ],
        "task_name": "bigbio/minimayosrs-minimayosrs_source",
        "instruction": "Given a medical term or phrase, predict the corresponding code."
    },
    {
        "input_fields": [
            "code_1"
        ],
        "output_field": [
            "text_1"
        ],
        "task_name": "bigbio/minimayosrs-minimayosrs_source",
        "instruction": "Given a medical code, predict the corresponding term or phrase."
    },
    {
        "input_fields": [
            "text_1",
            "text_2"
        ],
        "output_field": [
            "label_physicians",
            "label_coders"
        ],
        "task_name": "bigbio/minimayosrs-minimayosrs_source",
        "instruction": "Given a pair of medical terms or phrases, predict whether they correspond to the same code or not. Answers must be one of 2.0, 3.0, 1.3, 4.0, 2.3, 3.3, 1.7, 2.7, 1.0."
    },
    {
        "input_fields": [
            "code_1",
            "code_2"
        ],
        "output_field": [
            "label_physicians",
            "label_coders"
        ],
        "task_name": "bigbio/minimayosrs-minimayosrs_source",
        "instruction": "Given a pair of medical codes, predict whether they correspond to the same term or phrase or not. Answers must be one of 2.0, 3.0, 1.3, 4.0, 2.3, 3.3, 1.7, 2.7, 1.0."
    },
    {
        "input_fields": [
            "text_1",
            "text_2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/minimayosrs-minimayosrs_bigbio_pairs",
        "instruction": "Given two medical terms, predict the similarity score between them."
    },
    {
        "input_fields": [
            "text_1"
        ],
        "output_field": [
            "text_2"
        ],
        "task_name": "bigbio/minimayosrs-minimayosrs_bigbio_pairs",
        "instruction": "Given a medical term, generate a list of related terms."
    },
    {
        "input_fields": [
            "text_1",
            "text_2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/minimayosrs-minimayosrs_bigbio_pairs",
        "instruction": "Given a similarity score, determine if two medical terms are related or not."
    },
    {
        "input_fields": [
            "code_1",
            "code_2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/mayosrs-mayosrs_source",
        "instruction": "Given a pair of medical codes, predict the likelihood of the two symptoms being related."
    },
    {
        "input_fields": [
            "text_1"
        ],
        "output_field": [
            "code_1"
        ],
        "task_name": "bigbio/mayosrs-mayosrs_source",
        "instruction": "Given a symptom, predict the corresponding medical code."
    },
    {
        "input_fields": [
            "code_2"
        ],
        "output_field": [
            "text_2"
        ],
        "task_name": "bigbio/mayosrs-mayosrs_source",
        "instruction": "Given a medical code, predict the corresponding symptom."
    },
    {
        "input_fields": [
            "text_1",
            "text_2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/mayosrs-mayosrs_bigbio_pairs",
        "instruction": "Given two medical terms, predict the similarity score between them."
    },
    {
        "input_fields": [
            "text_1"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/mayosrs-mayosrs_bigbio_pairs",
        "instruction": "Given a medical term, predict the presence of a specific symptom or condition."
    },
    {
        "input_fields": [
            "text_1",
            "text_2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "bigbio/mayosrs-mayosrs_bigbio_pairs",
        "instruction": "Given a pair of medical terms, predict which one is more likely to be associated with a certain symptom or condition."
    },
    {
        "input_fields": [
            "prompt"
        ],
        "output_field": [
            "solution"
        ],
        "task_name": "nuprl/MultiPL-E-synthetic-solutions",
        "instruction": "Given a function and a threshold, determine if there are any two numbers in the given vector that are closer to each other than the threshold."
    },
    {
        "input_fields": [
            "prompt"
        ],
        "output_field": [
            "solution"
        ],
        "task_name": "nuprl/MultiPL-E-synthetic-solutions",
        "instruction": "Given a positive integer n, create a pile of n levels of stones. The first level has n stones. The number of stones in the next level is the next odd number if n is odd, and the next even number if n is even. Return the number of stones in each level in a vector, where element at index i represents the number of stones in the level (i+1)."
    },
    {
        "input_fields": [
            "claim"
        ],
        "output_field": [
            "labels"
        ],
        "task_name": "mwong/climate-claim-related",
        "instruction": "Given a climate-related claim, determine whether it is related to global sea level rise or not. Answers must be one of related, not_related."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "outlet"
        ],
        "task_name": "mediabiasgroup/BABE",
        "instruction": "Identify the news outlet that published the article. Answers must be one of Breitbart, Fox News, Alternet, Federalist, Daily Beast, MSNBC, Reuters, USA Today."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "topic"
        ],
        "task_name": "mediabiasgroup/BABE",
        "instruction": "Determine the topic of the article."
    },
    {
        "input_fields": [
            "outlet"
        ],
        "output_field": [
            "type"
        ],
        "task_name": "mediabiasgroup/BABE",
        "instruction": "Classify the article as left-leaning or right-leaning. Answers must be one of nan, right, left, center."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "biased_words"
        ],
        "task_name": "mediabiasgroup/BABE",
        "instruction": "Identify the biased words used in the article."
    },
    {
        "input_fields": [
            "mesh_terms"
        ],
        "output_field": [
            "year"
        ],
        "task_name": "taln-ls2n/kpbiomed-large",
        "instruction": "Given a set of mesh terms, the task is to predict the year of publication of the article."
    },
    {
        "input_fields": [
            "mesh_terms"
        ],
        "output_field": [
            "year"
        ],
        "task_name": "taln-ls2n/kpbiomed-small",
        "instruction": "Given a set of mesh terms, the task is to predict the year of publication of the article."
    },
    {
        "input_fields": [
            "icon"
        ],
        "output_field": [
            "word"
        ],
        "task_name": "niceblueman/icons_dataset",
        "instruction": "Given an icon, can you identify the word that best describes it?"
    },
    {
        "input_fields": [
            "word"
        ],
        "output_field": [
            "icon"
        ],
        "task_name": "niceblueman/icons_dataset",
        "instruction": "Given a word, can you find the icon that best represents it?"
    },
    {
        "input_fields": [
            "Roi.X1",
            "Roi.Y1",
            "Roi.X2",
            "Roi.Y2"
        ],
        "output_field": [
            "Height"
        ],
        "task_name": "bazyl/GTSRB",
        "instruction": "Given the coordinates of a bounding box, predict the height of the traffic sign."
    },
    {
        "input_fields": [
            "Height",
            "ClassId"
        ],
        "output_field": [
            "Roi.X1",
            "Roi.Y1",
            "Roi.X2",
            "Roi.Y2"
        ],
        "task_name": "bazyl/GTSRB",
        "instruction": "Given the height and class ID of a traffic sign, predict the coordinates of the bounding box. Answers must be one of 9, 8, 7, 6, 5, 10, 12, 15."
    },
    {
        "input_fields": [
            "smiles"
        ],
        "output_field": [
            "selfies"
        ],
        "task_name": "zpn/zinc20",
        "instruction": "Convert the SMILES notation to SELFIES notation."
    },
    {
        "input_fields": [
            "selfies"
        ],
        "output_field": [
            "smiles"
        ],
        "task_name": "zpn/zinc20",
        "instruction": "Convert the SELFIES notation to SMILES notation."
    },
    {
        "input_fields": [
            "color"
        ],
        "output_field": [
            "normal"
        ],
        "task_name": "dream-textures/textures-color-normal-1k",
        "instruction": "Given a color image, generate a normal map image."
    },
    {
        "input_fields": [
            "normal"
        ],
        "output_field": [
            "color"
        ],
        "task_name": "dream-textures/textures-color-normal-1k",
        "instruction": "Given a normal map image, generate a color image."
    },
    {
        "input_fields": [
            "body"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/reddit-r-bitcoin-data-for-jun-2022-comments",
        "instruction": "Identify the score of the comment."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "sagnikrayc/snli-cf-kaushik",
        "instruction": "Given a premise and a hypothesis, predict whether the hypothesis is entailed by the premise or not. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "sagnikrayc/snli-cf-kaushik",
        "instruction": "Given a premise and a hypothesis, generate a new hypothesis that is entailed by the premise."
    },
    {
        "input_fields": [
            "hypothesis"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "sagnikrayc/snli-cf-kaushik",
        "instruction": "Given a premise and a hypothesis, generate a new premise that entails the hypothesis."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/blimp_classification-semantics",
        "instruction": "Classify the given sentence as acceptable or unacceptable. Answers must be one of acceptable, unacceptable."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/blimp_classification-syntax",
        "instruction": "Classify the sentence as acceptable or unacceptable. Answers must be one of acceptable, unacceptable."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/blimp_classification-morphology",
        "instruction": "Classify the given sentence as acceptable or unacceptable. Answers must be one of acceptable, unacceptable."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/blimp_classification-syntax+semantics",
        "instruction": "Classify the given sentence as acceptable or unacceptable. Answers must be one of acceptable, unacceptable."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "metaeval/blimp_classification-syntax_semantics",
        "instruction": "Determine whether a given sentence is acceptable or unacceptable. Answers must be one of acceptable, unacceptable."
    },
    {
        "input_fields": [
            "claim"
        ],
        "output_field": [
            "labels"
        ],
        "task_name": "mwong/climate-evidence-related",
        "instruction": "Determine whether the claim is supported by evidence or not. Answers must be one of related, not_related."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_phonearena-com",
        "instruction": "Compare the hardware of two phones and determine which phone has a better system chip."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_phonearena-com",
        "instruction": "Compare the hardware of two phones and determine which phone has a better processor."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_wkdu-org",
        "instruction": "Given the album and title, predict the artist."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_w3-org",
        "instruction": "Given a policy expression, extract the output column name."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster05",
        "instruction": "Given a privacy policy statement, determine if Capitol Federal shares information for everyday business purposes. Answers must be one of We don\u2019t share, We Don't Share, We don't share, No, Yes, WE DON\u2019T SHARE, NO."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster05",
        "instruction": "Given a privacy policy statement, determine if Capitol Federal shares information for marketing purposes. Answers must be one of We don\u2019t share, We Don't Share, We don't share, No, Yes, WE DON\u2019T SHARE, NO."
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "MicPie/unpredictable_cluster05",
        "instruction": "Extract the privacy policy statement from the webpage."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_rated-low",
        "instruction": "Given the duration and output, what is the song?"
    },
    {
        "input_fields": [
            "string",
            "text"
        ],
        "output_field": [
            "correct_description"
        ],
        "task_name": "mnemlaghi/widdd",
        "instruction": "Given a string and a text, determine if the correct description matches the string in the text."
    },
    {
        "input_fields": [
            "string",
            "text"
        ],
        "output_field": [
            "wrong_description"
        ],
        "task_name": "mnemlaghi/widdd",
        "instruction": "Given a string and a text, determine if the wrong description matches the string in the text."
    },
    {
        "input_fields": [
            "correct_description"
        ],
        "output_field": [
            "string"
        ],
        "task_name": "mnemlaghi/widdd",
        "instruction": "Given a correct description, find the string that matches it. Answers must be one of corfu, bible, knight, captain marvel, hera, pausanias, george h. smith, gold."
    },
    {
        "input_fields": [
            "wrong_description"
        ],
        "output_field": [
            "string"
        ],
        "task_name": "mnemlaghi/widdd",
        "instruction": "Given a wrong description, find the string that matches it. Answers must be one of corfu, bible, knight, captain marvel, hera, pausanias, george h. smith, gold."
    },
    {
        "input_fields": [
            "task",
            "transcriptions"
        ],
        "output_field": [
            "gt"
        ],
        "task_name": "toloka/CrowdSpeech",
        "instruction": "Given a task and multiple transcriptions, the task is to select the correct transcription."
    },
    {
        "input_fields": [
            "task",
            "transcriptions"
        ],
        "output_field": [
            "performers"
        ],
        "task_name": "toloka/CrowdSpeech",
        "instruction": "Given a task and multiple transcriptions, the task is to identify the performer who provided the correct transcription."
    },
    {
        "input_fields": [
            "transcriptions",
            "gt"
        ],
        "output_field": [
            "performers"
        ],
        "task_name": "toloka/CrowdSpeech",
        "instruction": "Given a transcription and the correct transcription, the task is to identify the performer who provided the correct transcription."
    },
    {
        "input_fields": [
            "hashtag"
        ],
        "output_field": [
            "segmentation"
        ],
        "task_name": "ruanchaves/stan_large",
        "instruction": "Given a hashtag, provide the corresponding segmentation."
    },
    {
        "input_fields": [
            "segmentation"
        ],
        "output_field": [
            "hashtag"
        ],
        "task_name": "ruanchaves/stan_large",
        "instruction": "Given a segmentation, provide the corresponding hashtag."
    },
    {
        "input_fields": [
            "index"
        ],
        "output_field": [
            "hashtag",
            "segmentation"
        ],
        "task_name": "ruanchaves/stan_large",
        "instruction": "Given an index, provide the corresponding hashtag and segmentation."
    },
    {
        "input_fields": [
            "permalink"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/the-antiwork-subreddit-dataset-posts",
        "instruction": "Identify the score of the post."
    },
    {
        "input_fields": [
            "permalink"
        ],
        "output_field": [
            "title"
        ],
        "task_name": "SocialGrep/the-antiwork-subreddit-dataset-posts",
        "instruction": "Identify the title of the post."
    },
    {
        "input_fields": [
            "permalink"
        ],
        "output_field": [
            "selftext"
        ],
        "task_name": "SocialGrep/the-antiwork-subreddit-dataset-posts",
        "instruction": "Identify the selftext of the post."
    },
    {
        "input_fields": [
            "permalink"
        ],
        "output_field": [
            "created_utc"
        ],
        "task_name": "SocialGrep/the-antiwork-subreddit-dataset-posts",
        "instruction": "Identify the created_utc of the post."
    },
    {
        "input_fields": [
            "Date"
        ],
        "output_field": [
            "Day"
        ],
        "task_name": "Mulin/sg-holiday",
        "instruction": "Determine the day of the week for a given date. Answers must be one of Tuesday, Sunday, Friday, Thursday, Wednesday, Saturday, Monday."
    },
    {
        "input_fields": [
            "hiv_protein_product",
            "human_protein_product"
        ],
        "output_field": [
            "interaction_type"
        ],
        "task_name": "damlab/human_hiv_ppi",
        "instruction": "Given a HIV protein product and a human protein product, predict the interaction type between them."
    },
    {
        "input_fields": [
            "hiv_protein_product",
            "human_protein_product"
        ],
        "output_field": [
            "reference_list"
        ],
        "task_name": "damlab/human_hiv_ppi",
        "instruction": "Given a HIV protein product and a human protein product, predict the reference list for the interaction between them."
    },
    {
        "input_fields": [
            "hiv_protein_product",
            "human_protein_product"
        ],
        "output_field": [
            "description"
        ],
        "task_name": "damlab/human_hiv_ppi",
        "instruction": "Given a HIV protein product and a human protein product, predict the description of the interaction between them."
    },
    {
        "input_fields": [
            "body"
        ],
        "output_field": [
            "created_utc"
        ],
        "task_name": "SocialGrep/the-reddit-dataset-dataset-comments",
        "instruction": "Given a comment, predict the time it was created in UTC."
    },
    {
        "input_fields": [
            "permalink"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/the-reddit-place-dataset-comments",
        "instruction": "Identify the score of the comment."
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "domain"
        ],
        "task_name": "SocialGrep/the-reddit-irl-dataset-posts",
        "instruction": "Identify the domain of the post. Answers must be one of imgur.com, i.redd.it, youtu.be, i.imgur.com."
    },
    {
        "input_fields": [
            "permalink"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/the-reddit-irl-dataset-posts",
        "instruction": "Identify the score of the post."
    },
    {
        "input_fields": [
            "permalink"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/the-reddit-irl-dataset-comments",
        "instruction": "Identify the score of the comment."
    },
    {
        "input_fields": [
            "permalink"
        ],
        "output_field": [
            "created_utc"
        ],
        "task_name": "SocialGrep/the-reddit-irl-dataset-comments",
        "instruction": "Identify the time the comment was created."
    },
    {
        "input_fields": [
            "Product",
            "Sub-product"
        ],
        "output_field": [
            "Consumer Complaint"
        ],
        "task_name": "milesbutler/consumer_complaints",
        "instruction": "Identify the product and sub-product of the complaint."
    },
    {
        "input_fields": [
            "Consumer Complaint"
        ],
        "output_field": [
            "State",
            "ZIP code"
        ],
        "task_name": "milesbutler/consumer_complaints",
        "instruction": "Identify the state and zip code of the complaint."
    },
    {
        "input_fields": [
            "Company Response to Consumer"
        ],
        "output_field": [
            "Consumer Complaint"
        ],
        "task_name": "milesbutler/consumer_complaints",
        "instruction": "Identify the company response to the consumer."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "paragraph"
        ],
        "task_name": "lmqg/qg_subjqa-all",
        "instruction": "Given a sentence, identify the book it is referring to."
    },
    {
        "input_fields": [
            "paragraph"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "lmqg/qg_subjqa-all",
        "instruction": "Given a book review, extract the rating given by the reviewer."
    },
    {
        "input_fields": [
            "paragraph"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "lmqg/qg_subjqa-all",
        "instruction": "Given a book review, extract the main complaint of the reviewer."
    },
    {
        "input_fields": [
            "answer"
        ],
        "output_field": [
            "paragraph_question"
        ],
        "task_name": "lmqg/qg_subjqa-books",
        "instruction": "Given a book review, predict the rating given by the reviewer."
    },
    {
        "input_fields": [
            "paragraph"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qg_subjqa-books",
        "instruction": "Given a book review, extract the main topic of the review."
    },
    {
        "input_fields": [
            "paragraph"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "lmqg/qg_subjqa-books",
        "instruction": "Given a book review, extract the key sentence that summarizes the reviewer's opinion."
    },
    {
        "input_fields": [
            "domain"
        ],
        "output_field": [
            "paragraph"
        ],
        "task_name": "lmqg/qg_subjqa-books",
        "instruction": "Given a book review, extract the domain of the book being reviewed."
    },
    {
        "input_fields": [
            "question_subj_level"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qg_subjqa-books",
        "instruction": "Given a book review, extract the level of subjectivity of the question."
    },
    {
        "input_fields": [
            "answer_subj_level"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "lmqg/qg_subjqa-books",
        "instruction": "Given a book review, extract the level of subjectivity of the answer."
    },
    {
        "input_fields": [
            "sentence",
            "paragraph"
        ],
        "output_field": [
            "sentence_answer",
            "paragraph_answer"
        ],
        "task_name": "lmqg/qg_subjqa-electronics",
        "instruction": "Given a sentence or paragraph, highlight the part that describes a negative aspect of a product."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "lmqg/qg_subjqa-electronics",
        "instruction": "Given a question, provide the corresponding answer."
    },
    {
        "input_fields": [
            "paragraph"
        ],
        "output_field": [
            "sentence_answer"
        ],
        "task_name": "lmqg/qg_subjqa-grocery",
        "instruction": "Given a coffee review, identify the coffee maker used."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "lmqg/qg_subjqa-grocery",
        "instruction": "Given a coffee review, identify the coffee flavor preference."
    },
    {
        "input_fields": [
            "paragraph_sentence"
        ],
        "output_field": [
            "sentence_answer"
        ],
        "task_name": "lmqg/qg_subjqa-grocery",
        "instruction": "Given a coffee review, identify the coffee brewing method used."
    },
    {
        "input_fields": [
            "question",
            "paragraph"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "lmqg/qg_subjqa-movies",
        "instruction": "Given a question and a paragraph, the task is to predict the answer."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "lmqg/qg_subjqa-movies",
        "instruction": "Given a sentence, the task is to predict the answer."
    },
    {
        "input_fields": [
            "paragraph",
            "question"
        ],
        "output_field": [
            "sentence_answer"
        ],
        "task_name": "lmqg/qg_subjqa-movies",
        "instruction": "Given a paragraph and a question, the task is to predict the sentence that answers the question."
    },
    {
        "input_fields": [
            "paragraph"
        ],
        "output_field": [
            "paragraph_sentence"
        ],
        "task_name": "lmqg/qg_subjqa-movies",
        "instruction": "Given a paragraph, the task is to predict the sentence that summarizes the paragraph."
    },
    {
        "input_fields": [
            "question",
            "paragraph_question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "lmqg/qg_subjqa-restaurants",
        "instruction": "Given a question and its context, predict the answer."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "pub_date"
        ],
        "task_name": "Livingwithmachines/hmd-erwt-training",
        "instruction": "Extract the publication date from the given text."
    },
    {
        "input_fields": [
            "answer"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "launch/reddit_qg",
        "instruction": "Given an answer, generate a question that could have led to that answer."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "launch/reddit_qg",
        "instruction": "Given a question, generate an answer that could be a plausible response."
    },
    {
        "input_fields": [
            "question",
            "answer"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "launch/reddit_qg",
        "instruction": "Identify the score of a given question-answer pair."
    },
    {
        "input_fields": [
            "split_text"
        ],
        "output_field": [
            "prediction_ts"
        ],
        "task_name": "arize-ai/xtreme_en_language_drift_es",
        "instruction": "Given a sentence in English, predict the year it was released based on the text."
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_full",
        "instruction": "Given a task and input, provide the output."
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "MicPie/unpredictable_full",
        "instruction": "Given a task and input, provide the webpage title."
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_full",
        "instruction": "Given a task and input, provide the output column name."
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "MicPie/unpredictable_full",
        "instruction": "Given a task and input, provide the URL."
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "wdcFile"
        ],
        "task_name": "MicPie/unpredictable_full",
        "instruction": "Given a task and input, provide the WDC file."
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "task",
            "input",
            "output",
            "options",
            "pageTitle",
            "outputColName",
            "wdcFile"
        ],
        "task_name": "MicPie/unpredictable_baseball-fantasysports-yahoo-com",
        "instruction": "Given a URL, extract the task, input, output, options, pageTitle, outputColName and wdcFile."
    },
    {
        "input_fields": [
            "input",
            "outputColName"
        ],
        "output_field": [
            "task"
        ],
        "task_name": "MicPie/unpredictable_dividend-com",
        "instruction": "Given the input and output columns, what is the task?"
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_dividend-com",
        "instruction": "Given the task and input columns, what is the output column? Answers must be one of Metric, Notes."
    },
    {
        "input_fields": [
            "task",
            "outputColName"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "MicPie/unpredictable_dividend-com",
        "instruction": "Given the task and output column, what is the input column?"
    },
    {
        "input_fields": [
            "pageTitle"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "MicPie/unpredictable_gamefaqs-com",
        "instruction": "Given a webpage title, provide the corresponding URL."
    },
    {
        "input_fields": [
            "input",
            "outputColName"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_sittercity-com",
        "instruction": "Given the input schedule, determine if the care provider is available during a specific time period. Answers must be one of Not Available, Available."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_sittercity-com",
        "instruction": "Extract the time period during which the care provider is not available. Answers must be one of Not Available, Available."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cappex-com",
        "instruction": "Given a comment about campus safety, predict the category it belongs to."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cappex-com",
        "instruction": "Given a comment about Greek life, predict the category it belongs to."
    },
    {
        "input_fields": [
            "output"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "MicPie/unpredictable_cappex-com",
        "instruction": "Given a category, provide a comment about it."
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cram-com",
        "instruction": "Given a task and input, provide the correct output."
    },
    {
        "input_fields": [
            "task"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "MicPie/unpredictable_cram-com",
        "instruction": "Given a task and input, provide the webpage title."
    },
    {
        "input_fields": [
            "task"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_cram-com",
        "instruction": "Given a task and input, provide the output column name."
    },
    {
        "input_fields": [
            "task"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "MicPie/unpredictable_cram-com",
        "instruction": "Given a task and input, provide the URL."
    },
    {
        "input_fields": [
            "task"
        ],
        "output_field": [
            "wdcFile"
        ],
        "task_name": "MicPie/unpredictable_cram-com",
        "instruction": "Given a task and input, provide the WDC file."
    },
    {
        "input_fields": [
            "task",
            "output"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_sporcle-com",
        "instruction": "Given the input and output of a quiz question, can you identify the column name in the original dataset?"
    },
    {
        "input_fields": [
            "task",
            "output"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "MicPie/unpredictable_sporcle-com",
        "instruction": "Given the input and output of a quiz question, can you identify the webpage where the quiz was taken?"
    },
    {
        "input_fields": [
            "output"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "MicPie/unpredictable_5k",
        "instruction": "Identify the source of the comment."
    },
    {
        "input_fields": [
            "output"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "MicPie/unpredictable_5k",
        "instruction": "Identify the topic of the article based on the comments."
    },
    {
        "input_fields": [
            "permalink"
        ],
        "output_field": [
            "score"
        ],
        "task_name": "SocialGrep/one-year-of-tsla-on-reddit-comments",
        "instruction": "Find the score of the comment."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster-noise",
        "instruction": "Given a set of inputs and outputs, can you predict the availability of a babysitter at a certain time?"
    },
    {
        "input_fields": [
            "url",
            "outputColName"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster00",
        "instruction": "Given a webpage and a specific field, extract the corresponding value. Answers must be one of Yes, YES, No, NO."
    },
    {
        "input_fields": [
            "url",
            "output"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_cluster00",
        "instruction": "Given a webpage and a specific field, extract the corresponding column name."
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "MicPie/unpredictable_cluster00",
        "instruction": "Given a webpage and a specific field, extract the corresponding page title."
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "MicPie/unpredictable_cluster12",
        "instruction": "Find the source of the given text."
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_cluster15",
        "instruction": "Given a webpage URL, extract the output column name."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster16",
        "instruction": "Given a tourist attraction category, type, venue name, price range, rating, and value rating, predict the hours of operation."
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "MicPie/unpredictable_cluster16",
        "instruction": "Given a webpage URL, extract the tourist attraction category, type, venue name, price range, rating, and value rating."
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "pageTitle",
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_cluster16",
        "instruction": "Given a webpage URL, extract the page title and output column name."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster17",
        "instruction": "Given a course code, what is the corresponding course name?"
    },
    {
        "input_fields": [
            "output"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "MicPie/unpredictable_cluster18",
        "instruction": "Given a skincare product, provide the corresponding skin type, what it does, and expected results."
    },
    {
        "input_fields": [
            "output"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "MicPie/unpredictable_cluster18",
        "instruction": "Given a skincare product, provide the corresponding webpage title."
    },
    {
        "input_fields": [
            "output"
        ],
        "output_field": [
            "url"
        ],
        "task_name": "MicPie/unpredictable_cluster18",
        "instruction": "Given a skincare product, provide the corresponding URL."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster19",
        "instruction": "Extract the type of event or job from the given input."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster21",
        "instruction": "Given a list of activities and their approximate costs, recommend the most cost-effective activity."
    },
    {
        "input_fields": [
            "output"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "MicPie/unpredictable_cluster21",
        "instruction": "Given a list of activities and their descriptions, recommend the most interesting activity."
    },
    {
        "input_fields": [
            "output",
            "pageTitle"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_cluster21",
        "instruction": "Given a list of activities and their descriptions, recommend the activity that is most relevant to the trip."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster24",
        "instruction": "Given a list of guild names, predict the name of the next guild."
    },
    {
        "input_fields": [
            "output"
        ],
        "output_field": [
            "pageTitle",
            "outputColName",
            "url",
            "wdcFile"
        ],
        "task_name": "MicPie/unpredictable_cluster24",
        "instruction": "Given a guild name, predict the server it belongs to."
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_cluster27",
        "instruction": "Given the task and input, what is the output column name?"
    },
    {
        "input_fields": [
            "task",
            "outputColName"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "MicPie/unpredictable_cluster27",
        "instruction": "Given the task and output column name, what is the input?"
    },
    {
        "input_fields": [
            "task",
            "url"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster27",
        "instruction": "Given the task and URL, what is the output?"
    },
    {
        "input_fields": [
            "task",
            "input"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_cluster28",
        "instruction": "Given the task and input, extract the output column name."
    },
    {
        "input_fields": [
            "task",
            "outputColName"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "MicPie/unpredictable_cluster28",
        "instruction": "Given the task and output column name, extract the input."
    },
    {
        "input_fields": [
            "task",
            "outputColName"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster28",
        "instruction": "Given the task and output column name, extract the output."
    },
    {
        "input_fields": [
            "url",
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster03",
        "instruction": "Given a webpage URL and a cookie name, extract the description of the cookie."
    },
    {
        "input_fields": [
            "url",
            "input"
        ],
        "output_field": [
            "wdcFile"
        ],
        "task_name": "MicPie/unpredictable_cluster03",
        "instruction": "Given a webpage URL and a cookie name, extract the file name where the cookie description is stored."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster04",
        "instruction": "Given a privacy policy statement and a type of sharing, determine whether the sharing can be limited. Answers must be one of We don\u2019t share, We Don't Share, We don't share, No, Yes, YES, NO."
    },
    {
        "input_fields": [
            "pageTitle",
            "url"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "MicPie/unpredictable_cluster04",
        "instruction": "Extract the privacy policy statement from the webpage."
    },
    {
        "input_fields": [
            "pageTitle",
            "url"
        ],
        "output_field": [
            "input",
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster04",
        "instruction": "Extract the privacy policy statement and the type of sharing from the webpage."
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "pageTitle"
        ],
        "task_name": "MicPie/unpredictable_cluster06",
        "instruction": "Given a webpage URL, extract the page title."
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_cluster06",
        "instruction": "Given a webpage URL, extract the output column name."
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "task"
        ],
        "task_name": "MicPie/unpredictable_cluster06",
        "instruction": "Given a webpage URL, extract the task."
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "input"
        ],
        "task_name": "MicPie/unpredictable_cluster06",
        "instruction": "Given a webpage URL, extract the input field."
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster06",
        "instruction": "Given a webpage URL, extract the output field."
    },
    {
        "input_fields": [
            "url"
        ],
        "output_field": [
            "wdcFile"
        ],
        "task_name": "MicPie/unpredictable_cluster06",
        "instruction": "Given a webpage URL, extract the wdcFile."
    },
    {
        "input_fields": [
            "url",
            "outputColName"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster06",
        "instruction": "Given a webpage URL, determine if MSUFCU shares information for a given purpose."
    },
    {
        "input_fields": [
            "input"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster08",
        "instruction": "Given a question, provide the corresponding answer from the Antiquities and Monuments Office FAQ."
    },
    {
        "input_fields": [
            "task",
            "input",
            "pageTitle",
            "url",
            "wdcFile"
        ],
        "output_field": [
            "outputColName"
        ],
        "task_name": "MicPie/unpredictable_cluster09",
        "instruction": "Given a set of inputs, predict the output column name."
    },
    {
        "input_fields": [
            "task",
            "input",
            "pageTitle",
            "url",
            "wdcFile"
        ],
        "output_field": [
            "output"
        ],
        "task_name": "MicPie/unpredictable_cluster09",
        "instruction": "Given a set of inputs, predict the output value."
    },
    {
        "input_fields": [
            "sentence",
            "target"
        ],
        "output_field": [
            "animacy"
        ],
        "task_name": "biglam/atypical_animacy",
        "instruction": "Given a sentence and a target word, determine the animacy of the target word. Answers must be one of 1.0, 0.0."
    },
    {
        "input_fields": [
            "sentence",
            "target"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "biglam/atypical_animacy",
        "instruction": "Given a sentence and a target word, provide the context of the target word."
    },
    {
        "input_fields": [
            "utterance",
            "response"
        ],
        "output_field": [
            "implicature"
        ],
        "task_name": "UCL-DARK/ludwig-0-shot",
        "instruction": "Given an utterance and a response, determine whether the response implies a negation of the utterance. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "utterance",
            "response"
        ],
        "output_field": [
            "incoherent_implicature"
        ],
        "task_name": "UCL-DARK/ludwig-0-shot",
        "instruction": "Given an utterance and a response, determine whether the response implies a negation of the utterance incoherently. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "response"
        ],
        "output_field": [
            "utterance"
        ],
        "task_name": "UCL-DARK/ludwig-0-shot",
        "instruction": "Given a response, generate a plausible utterance that could have elicited the response."
    },
    {
        "input_fields": [
            "utterance",
            "response"
        ],
        "output_field": [
            "implicature"
        ],
        "task_name": "UCL-DARK/ludwig-1-shot",
        "instruction": "Given an utterance and a response, predict whether the response implies a negative answer to the utterance. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "utterance",
            "response"
        ],
        "output_field": [
            "incoherent_implicature"
        ],
        "task_name": "UCL-DARK/ludwig-1-shot",
        "instruction": "Given an utterance and a response, predict whether the response implies a positive answer to the utterance. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "utterance",
            "response"
        ],
        "output_field": [
            "implicature"
        ],
        "task_name": "UCL-DARK/ludwig-5-shot",
        "instruction": "Given an utterance and a response, determine whether the response implies a negation of the utterance. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "utterance",
            "response"
        ],
        "output_field": [
            "incoherent_implicature"
        ],
        "task_name": "UCL-DARK/ludwig-5-shot",
        "instruction": "Given an utterance and a response, determine whether the response implies a contradiction of the utterance. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "utterance",
            "response"
        ],
        "output_field": [
            "implicature"
        ],
        "task_name": "UCL-DARK/ludwig-10-shot",
        "instruction": "Given an utterance and a response, determine whether the response implies a negative answer to the utterance. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "utterance",
            "response"
        ],
        "output_field": [
            "incoherent_implicature"
        ],
        "task_name": "UCL-DARK/ludwig-10-shot",
        "instruction": "Given an utterance and a response, determine whether the response implies a positive answer to the utterance. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "utterance",
            "response"
        ],
        "output_field": [
            "implicature"
        ],
        "task_name": "UCL-DARK/ludwig-15-shot",
        "instruction": "Given an utterance and a response, determine whether the response implies a negative answer to the utterance. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "utterance",
            "response"
        ],
        "output_field": [
            "incoherent_implicature"
        ],
        "task_name": "UCL-DARK/ludwig-15-shot",
        "instruction": "Given an utterance and a response, determine whether the response implies a positive answer to the utterance. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "utterance",
            "response"
        ],
        "output_field": [
            "implicature"
        ],
        "task_name": "UCL-DARK/ludwig-30-shot",
        "instruction": "Given an utterance and a response, determine whether the response implies a negative answer to the utterance. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "utterance",
            "response"
        ],
        "output_field": [
            "incoherent_implicature"
        ],
        "task_name": "UCL-DARK/ludwig-30-shot",
        "instruction": "Given an utterance and a response, determine whether the response implies a positive answer to the utterance. Answers must be one of yes, no."
    },
    {
        "input_fields": [
            "title"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "allenai/ms2_sparse_oracle",
        "instruction": "Given a set of titles, the task is to predict the corresponding target."
    },
    {
        "input_fields": [
            "title"
        ],
        "output_field": [
            "background"
        ],
        "task_name": "allenai/ms2_sparse_oracle",
        "instruction": "Given a set of titles, the task is to predict the corresponding background."
    },
    {
        "input_fields": [
            "abstract"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "allenai/ms2_sparse_oracle",
        "instruction": "Given a set of abstracts, the task is to predict the corresponding target."
    },
    {
        "input_fields": [
            "abstract"
        ],
        "output_field": [
            "background"
        ],
        "task_name": "allenai/ms2_sparse_oracle",
        "instruction": "Given a set of abstracts, the task is to predict the corresponding background."
    },
    {
        "input_fields": [
            "target"
        ],
        "output_field": [
            "background"
        ],
        "task_name": "allenai/ms2_sparse_oracle",
        "instruction": "Given a set of targets, the task is to predict the corresponding background."
    },
    {
        "input_fields": [
            "title",
            "abstract"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "allenai/ms2_sparse_max",
        "instruction": "Given a set of titles and abstracts, the task is to predict the corresponding target."
    },
    {
        "input_fields": [
            "title",
            "background"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "allenai/ms2_sparse_max",
        "instruction": "Given a set of titles and backgrounds, the task is to predict the corresponding target."
    },
    {
        "input_fields": [
            "title",
            "background"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "allenai/ms2_sparse_mean",
        "instruction": "Given a title and background, predict the target of the study."
    },
    {
        "input_fields": [
            "title",
            "target"
        ],
        "output_field": [
            "background"
        ],
        "task_name": "allenai/ms2_sparse_mean",
        "instruction": "Given a title and target, predict the background of the study."
    },
    {
        "input_fields": [
            "permalink"
        ],
        "output_field": [
            "title",
            "domain"
        ],
        "task_name": "SocialGrep/the-reddit-climate-change-dataset-posts",
        "instruction": "Extract the title and the domain of the post."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "title"
        ],
        "task_name": "chenghao/cuad_qa",
        "instruction": "Given a document, the task is to extract the title of the document."
    },
    {
        "input_fields": [
            "title",
            "abstract"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "allenai/cochrane_sparse_mean",
        "instruction": "Given a title and abstract, predict the target of the article."
    },
    {
        "input_fields": [
            "title",
            "abstract"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "allenai/cochrane_sparse_oracle",
        "instruction": "Given a title and abstract, predict the target of the article."
    },
    {
        "input_fields": [
            "title",
            "date"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "Tidrael/tsl_news",
        "instruction": "Given the title and date of a news article, predict the label (positive/negative) of the news. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "title",
            "pct_change"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "Tidrael/tsl_news",
        "instruction": "Given the title and percentage change of a news article, predict the label (positive/negative) of the news. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "close",
            "pct_change"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "Tidrael/tsl_news",
        "instruction": "Given the closing price and percentage change of a news article, predict the label (positive/negative) of the news. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "title",
            "abstract"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "allenai/cochrane_dense_mean",
        "instruction": "Given a set of titles and abstracts, predict the target of the article."
    },
    {
        "input_fields": [
            "title",
            "abstract"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "allenai/cochrane_dense_max",
        "instruction": "Given a set of titles and abstracts, predict the target of the article."
    },
    {
        "input_fields": [
            "title",
            "abstract"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "allenai/ms2_dense_mean",
        "instruction": "Given a title and abstract, predict the target variable (NEC incidence reduction or glucomannan impact)."
    },
    {
        "input_fields": [
            "title",
            "background"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "allenai/ms2_dense_mean",
        "instruction": "Given a title and background, predict the target variable (NEC incidence reduction or glucomannan impact)."
    },
    {
        "input_fields": [
            "title",
            "abstract"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "allenai/ms2_dense_oracle",
        "instruction": "Given a title and abstract, predict the target variable."
    },
    {
        "input_fields": [
            "title",
            "background"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "allenai/ms2_dense_oracle",
        "instruction": "Given a title and background, predict the target variable."
    },
    {
        "input_fields": [
            "title",
            "target"
        ],
        "output_field": [
            "background"
        ],
        "task_name": "allenai/ms2_dense_oracle",
        "instruction": "Given a title and target, predict the background."
    },
    {
        "input_fields": [
            "title",
            "abstract"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "allenai/ms2_dense_max",
        "instruction": "Given a title and abstract, predict the target of the study (e.g. the effect of a certain intervention on a certain disease)."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "level"
        ],
        "task_name": "tiagoblima/nilc-school-books",
        "instruction": "Given a text, identify the level of education it is intended for."
    },
    {
        "input_fields": [
            "level"
        ],
        "output_field": [
            "text"
        ],
        "task_name": "tiagoblima/nilc-school-books",
        "instruction": "Given a level of education, provide a text that is appropriate for that level."
    },
    {
        "input_fields": [
            "text",
            "beer_ABV",
            "beer_style",
            "review_appearance",
            "review_palette",
            "review_taste",
            "review_aroma"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "arize-ai/beer_reviews_label_drift_neutral",
        "instruction": "Predict the label of a beer review based on the review text and other features. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "classification"
        ],
        "task_name": "niurl/eraser_esnli",
        "instruction": "Given a premise and a hypothesis, determine if the hypothesis contradicts the premise. Answers must be one of entailment, neutral, contradiction."
    },
    {
        "input_fields": [
            "question",
            "choices"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "drt/kqa_pro-train_val",
        "instruction": "Given a question and a set of choices, select the correct answer."
    },
    {
        "input_fields": [
            "question",
            "answer"
        ],
        "output_field": [
            "sparql"
        ],
        "task_name": "drt/kqa_pro-train_val",
        "instruction": "Given a question and a correct answer, generate a SPARQL query to retrieve the answer."
    },
    {
        "input_fields": [
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-sst2",
        "instruction": "Given a sentence, predict the sentiment label. Answers must be one of negative, positive."
    },
    {
        "input_fields": [
            "label"
        ],
        "output_field": [
            "sentence"
        ],
        "task_name": "severo/glue-sst2",
        "instruction": "Given a set of sentiment labels, generate a sentence that fits the label."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-mrpc",
        "instruction": "Determine if two sentences are equivalent or not. Answers must be one of not_equivalent, equivalent."
    },
    {
        "input_fields": [
            "question1",
            "question2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-qqp",
        "instruction": "Determine if two questions are duplicates or not. Answers must be one of duplicate, not_duplicate."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-stsb",
        "instruction": "Given two sentences, predict the similarity score between them."
    },
    {
        "input_fields": [
            "sentence1"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "severo/glue-stsb",
        "instruction": "Given a sentence, generate a similar sentence with a higher similarity score."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-mnli",
        "instruction": "Given a premise and a hypothesis, predict the label (entailment, contradiction, or neutral) of the hypothesis based on the premise. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "severo/glue-mnli",
        "instruction": "Given a premise and a label, generate a hypothesis that is consistent with the premise and has the given label."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-mnli_mismatched",
        "instruction": "Given a premise and a hypothesis, determine whether they are in agreement or disagreement. Answers must be one of entailment, neutral, contradiction."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "severo/glue-mnli_mismatched",
        "instruction": "Given a premise and a label, generate a hypothesis that is in agreement with the premise."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "severo/glue-mnli_mismatched",
        "instruction": "Given a hypothesis and a label, generate a premise that is in disagreement with the hypothesis."
    },
    {
        "input_fields": [
            "premise",
            "hypothesis"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-mnli_matched",
        "instruction": "Given a premise and a hypothesis, predict the relationship between them. Answers must be one of neutral, contradiction, entailment."
    },
    {
        "input_fields": [
            "premise",
            "label"
        ],
        "output_field": [
            "hypothesis"
        ],
        "task_name": "severo/glue-mnli_matched",
        "instruction": "Given a premise and a label, generate a hypothesis that is consistent with the premise and label."
    },
    {
        "input_fields": [
            "hypothesis",
            "label"
        ],
        "output_field": [
            "premise"
        ],
        "task_name": "severo/glue-mnli_matched",
        "instruction": "Given a hypothesis and a label, generate a premise that is consistent with the hypothesis and label."
    },
    {
        "input_fields": [
            "question",
            "sentence"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-qnli",
        "instruction": "Determine whether the given sentence entails the given question. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence",
            "label"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "severo/glue-qnli",
        "instruction": "Given a sentence and its label, generate a question that can be answered by the sentence."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-rte",
        "instruction": "Determine whether the two sentences are entailed or not entailed. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence1",
            "label"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "severo/glue-rte",
        "instruction": "Given a sentence and a label, generate a new sentence that is entailed by the original sentence."
    },
    {
        "input_fields": [
            "sentence1",
            "sentence2"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-wnli",
        "instruction": "Determine whether the two sentences entail each other or not. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence1"
        ],
        "output_field": [
            "label"
        ],
        "task_name": "severo/glue-wnli",
        "instruction": "Given a sentence and its entailment label, predict the entailment label for a new sentence. Answers must be one of entailment, not_entailment."
    },
    {
        "input_fields": [
            "sentence1",
            "label"
        ],
        "output_field": [
            "sentence2"
        ],
        "task_name": "severo/glue-wnli",
        "instruction": "Given a sentence and its entailment label, generate a new sentence that entails the original sentence."
    },
    {
        "input_fields": [
            "steps"
        ],
        "output_field": [
            "entity"
        ],
        "task_name": "abhinavk/openpi_v2-Task 4",
        "instruction": "Given a set of steps, the task is to identify the fighting position."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts-new_wiki",
        "instruction": "Given a context, the task is to generate a question that can be answered by the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_harvesting_from_wikipedia_pseudo-t5-base-squad",
        "instruction": "Given a context, the task is to generate a question that can be answered by the context."
    },
    {
        "input_fields": [
            "title",
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_harvesting_from_wikipedia_pseudo-bart-base-squad",
        "instruction": "Given a title and a question, the task is to predict the context of the article that the question is from."
    },
    {
        "input_fields": [
            "title",
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_harvesting_from_wikipedia_pseudo-bart-large-squad",
        "instruction": "Given a title and context, the task is to answer a specific question."
    },
    {
        "input_fields": [
            "title",
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_harvesting_from_wikipedia_pseudo-bart-large-squad",
        "instruction": "Given a title and question, the task is to provide the relevant context."
    },
    {
        "input_fields": [
            "questions_answers"
        ],
        "output_field": [
            "paragraph"
        ],
        "task_name": "lmqg/qag_tweetqa",
        "instruction": "Given a question and its answer, the task is to generate a coherent paragraph that includes the answer."
    },
    {
        "input_fields": [
            "smiles"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "zpn/bace_classification",
        "instruction": "Given a SMILES string, predict the target value. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "selfies"
        ],
        "output_field": [
            "target"
        ],
        "task_name": "zpn/bace_classification",
        "instruction": "Given a SELFIES string, predict the target value. Answers must be one of 0, 1."
    },
    {
        "input_fields": [
            "smiles"
        ],
        "output_field": [
            "selfies"
        ],
        "task_name": "zpn/bace_classification",
        "instruction": "Given a SMILES string, predict the SELFIES string."
    },
    {
        "input_fields": [
            "selfies"
        ],
        "output_field": [
            "smiles"
        ],
        "task_name": "zpn/bace_classification",
        "instruction": "Given a SELFIES string, predict the SMILES string."
    },
    {
        "input_fields": [
            "target"
        ],
        "output_field": [
            "smiles"
        ],
        "task_name": "zpn/pcba_686978",
        "instruction": "Given a target value, predict the SMILES string."
    },
    {
        "input_fields": [
            "target"
        ],
        "output_field": [
            "selfies"
        ],
        "task_name": "zpn/pcba_686978",
        "instruction": "Given a target value, predict the SELFIES string."
    },
    {
        "input_fields": [
            "target"
        ],
        "output_field": [
            "smiles"
        ],
        "task_name": "zpn/bbbp",
        "instruction": "Given a target value, predict the corresponding SMILES string."
    },
    {
        "input_fields": [
            "target"
        ],
        "output_field": [
            "selfies"
        ],
        "task_name": "zpn/bbbp",
        "instruction": "Given a target value, predict the corresponding SELFIES string."
    },
    {
        "input_fields": [
            "paragraph",
            "question"
        ],
        "output_field": [
            "answer"
        ],
        "task_name": "lmqg/qg_tweetqa",
        "instruction": "Given a paragraph and a question, the task is to predict the answer."
    },
    {
        "input_fields": [
            "paragraph",
            "answer"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qg_tweetqa",
        "instruction": "Given a paragraph and an answer, the task is to generate a question that could be answered by the given answer."
    },
    {
        "input_fields": [
            "question",
            "answer"
        ],
        "output_field": [
            "paragraph"
        ],
        "task_name": "lmqg/qg_tweetqa",
        "instruction": "Given a question and an answer, the task is to generate a paragraph that could answer the given question."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.end2end.amazon",
        "instruction": "Given a context, the task is to generate a question that can be answered based on the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.end2end.new_wiki",
        "instruction": "Given a context, generate a question that can be answered by the context."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.end2end.reddit",
        "instruction": "Given a question, the task is to extract the relevant context from the text."
    },
    {
        "input_fields": [
            "title",
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.multitask.new_wiki",
        "instruction": "Given a question and title, predict the context of the article."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.multitask.reddit",
        "instruction": "Given a question, the task is to extract the relevant context from the text."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.pipeline.amazon",
        "instruction": "Given a context, the task is to generate a question that can be answered based on the context."
    },
    {
        "input_fields": [
            "title",
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.pipeline.new_wiki",
        "instruction": "Given a title and context, predict the question."
    },
    {
        "input_fields": [
            "context",
            "title"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.pipeline.nyt",
        "instruction": "Given a context and a title, the task is to predict the question."
    },
    {
        "input_fields": [
            "title",
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-base-squad.qg_reference.reddit",
        "instruction": "Given a question and title, predict the context of the post."
    },
    {
        "input_fields": [
            "title",
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.end2end.reddit",
        "instruction": "Given a title and context, the task is to generate a question that can be answered based on the context."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.end2end.reddit",
        "instruction": "Given a question, the task is to provide the context where the answer can be found."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.multitask.amazon",
        "instruction": "Given a context, generate a question about the wok."
    },
    {
        "input_fields": [
            "title",
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.multitask.new_wiki",
        "instruction": "Given a question and title, predict the context of the article."
    },
    {
        "input_fields": [
            "title",
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.multitask.reddit",
        "instruction": "Given a youtube channel, can you find the link to the google+ profile of someone you have been subscribed to?"
    },
    {
        "input_fields": [
            "title",
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.multitask.reddit",
        "instruction": "Given a youtube channel, what does the top right corner look like?"
    },
    {
        "input_fields": [
            "title",
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.multitask.reddit",
        "instruction": "Given a youtube channel and a link to a google+ profile, can you remove the \"also subscribed\" feature?"
    },
    {
        "input_fields": [
            "title",
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.pipeline.new_wiki",
        "instruction": "Given a title and context, predict the question."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.pipeline.nyt",
        "instruction": "Given the context, the task is to generate a question that can be answered by the context."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-bart-large-squad.pipeline.nyt",
        "instruction": "Given a question, the task is to generate a context that can answer the question."
    },
    {
        "input_fields": [
            "title",
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.end2end.new_wiki",
        "instruction": "Given a question and title, predict the context of the article."
    },
    {
        "input_fields": [
            "context",
            "title"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.end2end.nyt",
        "instruction": "Given a context and a title, the task is to generate a question that can be answered by the context."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.end2end.reddit",
        "instruction": "Given a question, the task is to extract the relevant context from the text."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.end2end.reddit",
        "instruction": "Given a context, the task is to generate a question that can be answered by the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.multitask.new_wiki",
        "instruction": "Given a context, the task is to generate a question that can be answered based on the information in the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.pipeline.amazon",
        "instruction": "Given a context and a question, generate a new question based on the context."
    },
    {
        "input_fields": [
            "title",
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.pipeline.new_wiki",
        "instruction": "Given a question and title, predict the context of the article."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.pipeline.reddit",
        "instruction": "Given a question, the task is to extract the relevant context from the text."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.pipeline.reddit",
        "instruction": "Given a context, the task is to generate a question that can be answered by the context."
    },
    {
        "input_fields": [
            "title"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.qg_reference.new_wiki",
        "instruction": "Given a company name, provide its history and services."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-base-squad.qg_reference.new_wiki",
        "instruction": "Given a description of a measure of magnifying power, provide the name of the measure."
    },
    {
        "input_fields": [
            "context",
            "title"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.end2end.nyt",
        "instruction": "Given a context and a title, the task is to generate a question that can be answered by the context."
    },
    {
        "input_fields": [
            "title",
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.end2end.reddit",
        "instruction": "Given a title and context, generate a question that summarizes the content."
    },
    {
        "input_fields": [
            "title",
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.multitask.new_wiki",
        "instruction": "Given a question and title, predict the context of the article."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.multitask.nyt",
        "instruction": "Given a question, the task is to generate a context that answers the question."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.multitask.reddit",
        "instruction": "Given a text, the task is to extract the question."
    },
    {
        "input_fields": [
            "title",
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.pipeline.new_wiki",
        "instruction": "Given a question and title, predict the context of the article."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.pipeline.nyt",
        "instruction": "Given a context and a question, the task is to generate a new question based on the information provided in the context."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.pipeline.reddit",
        "instruction": "Given a question, the task is to extract the relevant context from a given text."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.pipeline.reddit",
        "instruction": "Given a context, the task is to generate a question that can be answered by the context."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-large-squad.qg_reference.reddit",
        "instruction": "Given a context and a question, generate a new question that can be answered using the context."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.end2end.reddit",
        "instruction": "Given a question, the task is to extract the relevant context from a given text."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.end2end.reddit",
        "instruction": "Given a text, the task is to extract the question from the text."
    },
    {
        "input_fields": [
            "context",
            "title"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.multitask.nyt",
        "instruction": "Given a context and a title, the task is to predict the question."
    },
    {
        "input_fields": [
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.multitask.reddit",
        "instruction": "Given a question, the task is to extract the relevant context from the text."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.multitask.reddit",
        "instruction": "Given a context, the task is to generate a question that can be answered by the context."
    },
    {
        "input_fields": [
            "title",
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.pipeline.reddit",
        "instruction": "Given a title and context, generate a question that captures the essence of the content."
    },
    {
        "input_fields": [
            "title",
            "question"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.pipeline.reddit",
        "instruction": "Given a question and title, generate a concise summary of the content."
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.qg_reference.amazon",
        "instruction": "What is the job of a specific tool in a given context?"
    },
    {
        "input_fields": [
            "context"
        ],
        "output_field": [
            "question"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.qg_reference.amazon",
        "instruction": "What is the best type of coffee that can be brewed with a specific coffee maker?"
    },
    {
        "input_fields": [
            "title"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.qg_reference.amazon",
        "instruction": "What is the main purpose of a specific product?"
    },
    {
        "input_fields": [
            "title"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.qg_reference.amazon",
        "instruction": "What is the best way to use a specific product?"
    },
    {
        "input_fields": [
            "title"
        ],
        "output_field": [
            "context"
        ],
        "task_name": "lmqg/qa_squadshifts_synthetic-t5-small-squad.qg_reference.new_wiki",
        "instruction": "Given a company name, provide its history and services."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "title"
        ],
        "task_name": "castorini/odqa-wiki-corpora-wiki-text-100w-karpukhin",
        "instruction": "Extract the name of the prophet mentioned in the text."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "title"
        ],
        "task_name": "castorini/odqa-wiki-corpora-wiki-text-100w-tamber",
        "instruction": "Extract the title of the article from the given text."
    },
    {
        "input_fields": [
            "target"
        ],
        "output_field": [
            "smiles"
        ],
        "task_name": "zpn/tox21_srp53",
        "instruction": "Given a target value, predict the corresponding SMILES string."
    },
    {
        "input_fields": [
            "target"
        ],
        "output_field": [
            "selfies"
        ],
        "task_name": "zpn/tox21_srp53",
        "instruction": "Given a target value, predict the corresponding SELFIES string."
    },
    {
        "input_fields": [
            "texts"
        ],
        "output_field": [
            "avg_score"
        ],
        "task_name": "tomekkorbak/pile-detoxify",
        "instruction": "Given a text, predict the average score of the text."
    },
    {
        "input_fields": [
            "texts"
        ],
        "output_field": [
            "num_sents"
        ],
        "task_name": "tomekkorbak/pile-detoxify",
        "instruction": "Given a text, predict the number of sentences in the text."
    },
    {
        "input_fields": [
            "text"
        ],
        "output_field": [
            "__index_level_0__"
        ],
        "task_name": "civility-lab/incivility-arizona-daily-star-comments",
        "instruction": "Identify the index of the comment in the dataset."
    },
    {
        "input_fields": [
            "METADATA"
        ],
        "output_field": [
            "RESPONSE"
        ],
        "task_name": "sedthh/ubuntu_dialogue_qa",
        "instruction": "Given a user question, provide an appropriate response."
    },
    {
        "input_fields": [
            "METADATA"
        ],
        "output_field": [
            "INSTRUCTION"
        ],
        "task_name": "sedthh/ubuntu_dialogue_qa",
        "instruction": "Given a user question, extract the metadata associated with the question."
    }
]